#!/usr/bin/env python3
"""
ArXiv API æ ¸å¿ƒæ¨¡å—
ç»Ÿä¸€ç®¡ç†ArXivæ•°æ®äº¤äº’å’Œè®ºæ–‡æ™ºèƒ½æ’åºåŠŸèƒ½
ä½¿ç”¨å®˜æ–¹arxivåº“è¿›è¡Œè¯·æ±‚ï¼Œæ”¯æŒPDFä¸‹è½½
"""

import arxiv
import re
import json
import os
from datetime import datetime, timedelta
from typing import Dict, List, Any, Tuple, Optional
from collections import defaultdict
import math
import requests
from pathlib import Path
import hashlib

# ä¼˜å…ˆä½¿ç”¨ rapidfuzzï¼Œå¦‚æœä¸å¯ç”¨åˆ™å›é€€åˆ° difflib
try:
    from rapidfuzz import fuzz, process

    RAPIDFUZZ_AVAILABLE = True
except ImportError:
    import difflib

    RAPIDFUZZ_AVAILABLE = False
    print("âš ï¸  rapidfuzz ä¸å¯ç”¨ï¼Œä½¿ç”¨ difflib ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ")


class ArxivAPI:
    """ArXiv API äº¤äº’ç±» - ä½¿ç”¨å®˜æ–¹arxivåº“"""

    def __init__(self, timeout: int = 30, download_dir: str = "downloads"):
        """åˆå§‹åŒ–ArXiv APIå®¢æˆ·ç«¯"""
        self.timeout = timeout
        self.download_dir = Path(download_dir)
        self.download_dir.mkdir(exist_ok=True)

        # é…ç½®arxivå®¢æˆ·ç«¯ï¼ˆä½¿ç”¨æ­£ç¡®çš„æ–¹å¼ï¼‰
        self.client = arxiv.Client(page_size=100, delay_seconds=3.0, num_retries=3)

    def search_papers(
        self,
        query: str = None,
        categories: List[str] = None,
        max_results: int = 100,
        sort_by: arxiv.SortCriterion = arxiv.SortCriterion.SubmittedDate,
        sort_order: arxiv.SortOrder = arxiv.SortOrder.Descending,
        date_from: datetime = None,
        date_to: datetime = None,
    ) -> List[Dict[str, Any]]:
        """
        æœç´¢ArXivè®ºæ–‡

        Args:
            query: æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²
            categories: åˆ†ç±»åˆ—è¡¨ï¼Œå¦‚ ['cs.AI', 'cs.RO']
            max_results: æœ€å¤§ç»“æœæ•°
            sort_by: æ’åºå­—æ®µ
            sort_order: æ’åºé¡ºåº
            date_from: å¼€å§‹æ—¥æœŸ
            date_to: ç»“æŸæ—¥æœŸ

        Returns:
            è®ºæ–‡ä¿¡æ¯åˆ—è¡¨
        """
        try:
            # æ„å»ºæœç´¢æŸ¥è¯¢
            search_query = self._build_search_query(query, categories, date_from, date_to)

            print(f"ğŸ” æœç´¢æŸ¥è¯¢: {search_query}")

            # åˆ›å»ºæœç´¢å¯¹è±¡
            search = arxiv.Search(query=search_query, max_results=max_results, sort_by=sort_by, sort_order=sort_order)

            papers = []
            for result in self.client.results(search):
                paper_info = self._parse_arxiv_result(result)
                if paper_info:
                    papers.append(paper_info)

            print(f"âœ… æˆåŠŸè·å– {len(papers)} ç¯‡è®ºæ–‡")
            return papers

        except Exception as e:
            print(f"âŒ æœç´¢è®ºæ–‡æ—¶å‡ºé”™: {e}")
            return []

    def get_recent_papers(
        self, days: int = 7, max_results: int = 300, categories: List[str] = None, field_type: str = "all"
    ) -> List[Dict[str, Any]]:
        """
        è·å–æœ€è¿‘Nå¤©çš„è®ºæ–‡

        Args:
            days: å¤©æ•°èŒƒå›´
            max_results: æœ€å¤§ç»“æœæ•°
            categories: åˆ†ç±»åˆ—è¡¨
            field_type: é¢†åŸŸç±»å‹ (all, ai, robotics, cv, nlp)

        Returns:
            è®ºæ–‡ä¿¡æ¯åˆ—è¡¨
        """
        # æ ¹æ®field_typeè®¾ç½®é»˜è®¤åˆ†ç±»
        if categories is None:
            categories = self._get_field_categories(field_type)

        # è®¡ç®—æ—¥æœŸèŒƒå›´
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days)

        return self.search_papers(
            categories=categories, max_results=max_results, date_from=start_date, date_to=end_date
        )

    def download_pdf(
        self, paper: Dict[str, Any], force_download: bool = False, create_metadata: bool = True
    ) -> Tuple[bool, str]:
        """
        ä¸‹è½½è®ºæ–‡PDF

        Args:
            paper: è®ºæ–‡ä¿¡æ¯å­—å…¸
            force_download: æ˜¯å¦å¼ºåˆ¶é‡æ–°ä¸‹è½½
            create_metadata: æ˜¯å¦åˆ›å»ºå…ƒæ•°æ®æ–‡ä»¶

        Returns:
            tuple: (æ˜¯å¦æˆåŠŸ, æ–‡ä»¶è·¯å¾„æˆ–é”™è¯¯ä¿¡æ¯)
        """
        try:
            arxiv_id = paper.get("arxiv_id", "")
            if not arxiv_id:
                return False, "è®ºæ–‡IDæ— æ•ˆ"

            # ç”Ÿæˆæ–‡ä»¶å
            safe_title = self._sanitize_filename(paper.get("title", ""))[:100]
            pdf_filename = f"{arxiv_id}_{safe_title}.pdf"
            pdf_path = self.download_dir / pdf_filename

            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            if pdf_path.exists() and not force_download:
                return True, str(pdf_path)

            print(f"ğŸ“¥ ä¸‹è½½è®ºæ–‡: {paper.get('title', '')[:50]}...")

            # è·å–arxivç»“æœå¯¹è±¡
            search = arxiv.Search(id_list=[arxiv_id])
            result = next(self.client.results(search), None)

            if not result:
                return False, "æ— æ³•æ‰¾åˆ°è®ºæ–‡"

            # ä¸‹è½½PDF
            result.download_pdf(dirpath=str(self.download_dir), filename=pdf_filename)

            # åˆ›å»ºå…ƒæ•°æ®æ–‡ä»¶
            if create_metadata:
                self._create_paper_metadata(paper, pdf_path.with_suffix(".md"))

            print(f"âœ… ä¸‹è½½å®Œæˆ: {pdf_filename}")
            return True, str(pdf_path)

        except Exception as e:
            error_msg = f"ä¸‹è½½å¤±è´¥: {e}"
            print(f"âŒ {error_msg}")
            return False, error_msg

    def batch_download_pdfs(
        self, papers: List[Dict[str, Any]], max_downloads: int = 10, create_index: bool = True
    ) -> Dict[str, Any]:
        """
        æ‰¹é‡ä¸‹è½½PDF

        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            max_downloads: æœ€å¤§ä¸‹è½½æ•°é‡
            create_index: æ˜¯å¦åˆ›å»ºç´¢å¼•æ–‡ä»¶

        Returns:
            ä¸‹è½½ç»Ÿè®¡ä¿¡æ¯
        """
        stats = {"total": len(papers), "downloaded": 0, "skipped": 0, "failed": 0, "failed_papers": []}

        download_count = 0
        downloaded_papers = []

        for paper in papers:
            if download_count >= max_downloads:
                break

            success, result = self.download_pdf(paper)
            if success:
                stats["downloaded"] += 1
                downloaded_papers.append({**paper, "pdf_path": result})
            else:
                if "å·²å­˜åœ¨" in result:
                    stats["skipped"] += 1
                else:
                    stats["failed"] += 1
                    stats["failed_papers"].append({"title": paper.get("title", ""), "error": result})

            download_count += 1

        # åˆ›å»ºä¸‹è½½ç´¢å¼•
        if create_index and downloaded_papers:
            self._create_download_index(downloaded_papers)

        return stats

    def _build_search_query(
        self, query: str = None, categories: List[str] = None, date_from: datetime = None, date_to: datetime = None
    ) -> str:
        """æ„å»ºæœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²"""
        query_parts = []

        # æ·»åŠ è‡ªå®šä¹‰æŸ¥è¯¢
        if query:
            query_parts.append(f"({query})")

        # æ·»åŠ åˆ†ç±»æŸ¥è¯¢
        if categories:
            category_query = " OR ".join([f"cat:{cat}" for cat in categories])
            query_parts.append(f"({category_query})")

        # æ·»åŠ æ—¥æœŸèŒƒå›´æŸ¥è¯¢
        if date_from or date_to:
            date_parts = []

            if date_from:
                # å°†datetimeè½¬æ¢ä¸ºYYYYMMDDæ ¼å¼
                date_from_str = date_from.strftime("%Y%m%d")
                date_parts.append(f"submittedDate:[{date_from_str}0000")

            if date_to:
                # å°†datetimeè½¬æ¢ä¸ºYYYYMMDDæ ¼å¼
                date_to_str = date_to.strftime("%Y%m%d")
                if not date_from:
                    date_parts.append(f"submittedDate:[19910801 TO {date_to_str}2359]")
                else:
                    # é—­åˆä¹‹å‰çš„èŒƒå›´
                    date_parts[0] = f"submittedDate:[{date_from_str}0000 TO {date_to_str}2359]"
            elif date_from:
                # åªæœ‰å¼€å§‹æ—¥æœŸï¼Œåˆ°å½“å‰æ—¥æœŸ
                current_date = datetime.now().strftime("%Y%m%d")
                date_parts[0] = f"submittedDate:[{date_from_str}0000 TO {current_date}2359]"

            if date_parts:
                query_parts.extend(date_parts)

        # åˆå¹¶æŸ¥è¯¢éƒ¨åˆ†
        search_query = " AND ".join(query_parts) if query_parts else "all:*"

        return search_query

    def _parse_arxiv_result(self, result: arxiv.Result) -> Optional[Dict[str, Any]]:
        """è§£æarxiv.Resultå¯¹è±¡ä¸ºè®ºæ–‡ä¿¡æ¯å­—å…¸"""
        try:
            return {
                "title": result.title.strip(),
                "authors": [author.name for author in result.authors],
                "authors_str": ", ".join([author.name for author in result.authors]),
                "summary": result.summary.strip(),
                "published_date": result.published,
                "updated_date": result.updated if result.updated else result.published,
                "paper_url": result.entry_id,
                "pdf_url": result.pdf_url,
                "categories": [cat for cat in result.categories],
                "categories_str": ", ".join(result.categories),
                "arxiv_id": result.entry_id.split("/")[-1],
                "primary_category": result.primary_category,
                "comment": result.comment if result.comment else "",
                "journal_ref": result.journal_ref if result.journal_ref else "",
                "doi": result.doi if result.doi else "",
            }
        except Exception as e:
            print(f"è§£æè®ºæ–‡ä¿¡æ¯æ—¶å‡ºé”™: {e}")
            return None

    def _get_field_categories(self, field_type) -> List[str]:
        """
        æ ¹æ®é¢†åŸŸç±»å‹è·å–åˆ†ç±»åˆ—è¡¨ï¼Œæ”¯æŒäº¤é›†/å¹¶é›†æ“ä½œ

        æ”¯æŒçš„æ ¼å¼ï¼š
        - å•ä¸ªå­—ç¬¦ä¸²: "ai", "robotics"
        - å¹¶é›†æ“ä½œ: "ai+robotics" æˆ– ["ai", "robotics"]
        - äº¤é›†æ“ä½œ: "ai&robotics" (ç›®å‰ArXiv APIä¸ç›´æ¥æ”¯æŒï¼Œè¿”å›å¹¶é›†)
        - ç›´æ¥åˆ†ç±»: "cs.AI" æˆ– ["cs.AI", "cs.LG"]
        """
        # åŸºç¡€é¢†åŸŸæ˜ å°„
        field_mappings = {
            "ai": ["cs.AI", "cs.LG", "stat.ML"],
            "robotics": ["cs.RO"],
            "cv": ["cs.CV", "eess.IV"],
            "nlp": ["cs.CL"],
            "physics": ["physics.comp-ph", "cond-mat", "quant-ph"],
            "math": ["math.OC", "math.ST", "math.NA"],
            "stat": ["stat.ML", "stat.ME", "stat.AP"],
            "eess": ["eess.IV", "eess.SP", "eess.AS"],
            "q-bio": ["q-bio.QM", "q-bio.GN", "q-bio.MN"],
            "all": ["cs.AI", "cs.LG", "cs.RO", "cs.CV", "cs.CL", "cs.CR", "cs.DC", "cs.DS", "cs.HC", "cs.IR"],
        }

        # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œå¤„ç†åˆ—è¡¨ä¸­çš„æ¯ä¸ªå…ƒç´ 
        if isinstance(field_type, list):
            all_categories = []
            for field in field_type:
                all_categories.extend(self._parse_single_field(field, field_mappings))
            return list(set(all_categories))  # å»é‡

        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œæ£€æŸ¥æ˜¯å¦åŒ…å«è¿ç®—ç¬¦
        if isinstance(field_type, str):
            return self._parse_field_string(field_type, field_mappings)

        # é»˜è®¤è¿”å›all
        return field_mappings["all"]

    def _parse_field_string(self, field_str: str, field_mappings: dict) -> List[str]:
        """è§£æå­—æ®µå­—ç¬¦ä¸²ï¼Œæ”¯æŒè¿ç®—ç¬¦"""
        field_str = field_str.strip()

        # æ£€æŸ¥å¹¶é›†è¿ç®—ç¬¦ (+, |, or)
        if "+" in field_str or "|" in field_str or " or " in field_str.lower():
            # åˆ†å‰²å¹¶å¤„ç†å„ä¸ªéƒ¨åˆ†
            separators = ["+", "|", " or ", " OR "]
            parts = [field_str]

            for sep in separators:
                new_parts = []
                for part in parts:
                    new_parts.extend(part.split(sep))
                parts = new_parts

            all_categories = []
            for part in parts:
                part = part.strip()
                if part:
                    all_categories.extend(self._parse_single_field(part, field_mappings))

            return list(set(all_categories))  # å»é‡

        # æ£€æŸ¥äº¤é›†è¿ç®—ç¬¦ (&, and) - æ³¨æ„ï¼šArXiv APIä¸ç›´æ¥æ”¯æŒäº¤é›†
        elif "&" in field_str or " and " in field_str.lower():
            print("âš ï¸  æ³¨æ„ï¼šArXiv APIä¸ç›´æ¥æ”¯æŒåˆ†ç±»äº¤é›†æŸ¥è¯¢ï¼Œå°†è½¬æ¢ä¸ºå¹¶é›†æŸ¥è¯¢")
            # å°†äº¤é›†è½¬æ¢ä¸ºå¹¶é›†å¤„ç†
            separators = ["&", " and ", " AND "]
            parts = [field_str]

            for sep in separators:
                new_parts = []
                for part in parts:
                    new_parts.extend(part.split(sep))
                parts = new_parts

            all_categories = []
            for part in parts:
                part = part.strip()
                if part:
                    all_categories.extend(self._parse_single_field(part, field_mappings))

            return list(set(all_categories))

        # å•ä¸ªå­—æ®µ
        else:
            return self._parse_single_field(field_str, field_mappings)

    def _parse_single_field(self, field: str, field_mappings: dict) -> List[str]:
        """è§£æå•ä¸ªå­—æ®µ"""
        field = field.strip()

        # ç›´æ¥æ˜¯ArXivåˆ†ç±»
        if field.startswith(("cs.", "stat.", "math.", "physics.", "eess.", "q-bio.", "quant-ph", "cond-mat")):
            return [field]

        # æŸ¥æ‰¾é¢„å®šä¹‰æ˜ å°„
        if field.lower() in field_mappings:
            return field_mappings[field.lower()]

        # å¦‚æœä¸åœ¨æ˜ å°„ä¸­ï¼Œå‡è®¾æ˜¯è‡ªå®šä¹‰åˆ†ç±»
        print(f"âš ï¸  æœªè¯†åˆ«çš„é¢†åŸŸç±»å‹: {field}ï¼Œå°†å°è¯•ä½œä¸ºArXivåˆ†ç±»ä½¿ç”¨")
        return [field]

    def _sanitize_filename(self, filename: str) -> str:
        """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤ä¸åˆæ³•å­—ç¬¦"""
        # ç§»é™¤æˆ–æ›¿æ¢ä¸åˆæ³•å­—ç¬¦
        invalid_chars = r'[<>:"/\\|?*]'
        sanitized = re.sub(invalid_chars, "_", filename)
        # ç§»é™¤å¤šä½™ç©ºæ ¼å’Œæ ‡ç‚¹
        sanitized = re.sub(r"\s+", "_", sanitized)
        sanitized = re.sub(r"[^\w\-_.]", "", sanitized)
        return sanitized.strip("_")

    def _create_paper_metadata(self, paper: Dict[str, Any], md_path: Path) -> None:
        """åˆ›å»ºè®ºæ–‡å…ƒæ•°æ®Markdownæ–‡ä»¶"""
        try:
            content = f"""# {paper.get('title', 'Unknown Title')}

## åŸºæœ¬ä¿¡æ¯
- **ArXiv ID**: {paper.get('arxiv_id', 'N/A')}
- **å‘å¸ƒæ—¥æœŸ**: {paper.get('published_date', 'N/A')}
- **ä¸»åˆ†ç±»**: {paper.get('primary_category', 'N/A')}
- **æ‰€æœ‰åˆ†ç±»**: {paper.get('categories_str', 'N/A')}

## ä½œè€…
{paper.get('authors_str', 'Unknown Authors')}

## æ‘˜è¦
{paper.get('summary', 'No summary available')}

## é“¾æ¥
- **è®ºæ–‡é¡µé¢**: {paper.get('paper_url', 'N/A')}
- **PDFä¸‹è½½**: {paper.get('pdf_url', 'N/A')}

## å…¶ä»–ä¿¡æ¯
- **æœŸåˆŠå¼•ç”¨**: {paper.get('journal_ref', 'N/A')}
- **DOI**: {paper.get('doi', 'N/A')}
- **å¤‡æ³¨**: {paper.get('comment', 'N/A')}

---
*ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""

            with open(md_path, "w", encoding="utf-8") as f:
                f.write(content)

        except Exception as e:
            print(f"åˆ›å»ºå…ƒæ•°æ®æ–‡ä»¶å¤±è´¥: {e}")

    def _create_download_index(self, papers: List[Dict[str, Any]]) -> None:
        """åˆ›å»ºä¸‹è½½ç´¢å¼•æ–‡ä»¶"""
        try:
            index_path = self.download_dir / "README.md"

            content = f"""# ArXiv è®ºæ–‡ä¸‹è½½ç´¢å¼•

> ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
> æ€»è®¡è®ºæ–‡æ•°: {len(papers)}

## è®ºæ–‡åˆ—è¡¨

"""

            for i, paper in enumerate(papers, 1):
                arxiv_id = paper.get("arxiv_id", "N/A")
                title = paper.get("title", "Unknown Title")
                authors = paper.get("authors_str", "Unknown Authors")
                categories = paper.get("categories_str", "N/A")
                published = paper.get("published_date", "N/A")

                content += f"""### {i}. {title}

- **ArXiv ID**: {arxiv_id}
- **ä½œè€…**: {authors}
- **åˆ†ç±»**: {categories}
- **å‘å¸ƒ**: {published}
- **é“¾æ¥**: [{arxiv_id}]({paper.get('paper_url', '#')}) | [PDF]({paper.get('pdf_url', '#')})

---

"""

            with open(index_path, "w", encoding="utf-8") as f:
                f.write(content)

            print(f"ğŸ“ åˆ›å»ºä¸‹è½½ç´¢å¼•: {index_path}")

        except Exception as e:
            print(f"åˆ›å»ºç´¢å¼•æ–‡ä»¶å¤±è´¥: {e}")
