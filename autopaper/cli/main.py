#!/usr/bin/env python3
"""
AutoPaper CLI - åŸºäº Hydra çš„ ArXiv è®ºæ–‡é‡‡é›†å·¥å…·
é‡æ„è‡ª arxiv_hydra.pyï¼Œæ”¯æŒçµæ´»çš„é…ç½®ç®¡ç†å’Œä¸“ä¸šé¢†åŸŸå…³é”®è¯
"""

import os
import sys
import glob
import yaml
import traceback
import hydra
from datetime import datetime
from hydra.core.hydra_config import HydraConfig
from omegaconf import DictConfig, OmegaConf
from pathlib import Path
from typing import List, Dict, Any, Optional

# æ·»åŠ é¡¹ç›®è·¯å¾„åˆ°sys.path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥é¡¹ç›®æ ¸å¿ƒæ¨¡å—
try:
    from autopaper import ArxivAPI, PaperRanker, PaperDisplayer

    print("âœ… æ ¸å¿ƒæ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ æ ¸å¿ƒæ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

# å¯¼å…¥autopaperæ¨¡å—
try:
    from autopaper import FeishuConfig, SyncManager

    AUTOPAPER_AVAILABLE = True
    print("âœ… AutoPaperæ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    AUTOPAPER_AVAILABLE = False
    print(f"âš ï¸ AutoPaperæ¨¡å—å¯¼å…¥å¤±è´¥: {e}")


def find_sync_configs() -> List[str]:
    """æŸ¥æ‰¾æ‰€æœ‰ä»¥syncå¼€å¤´çš„é…ç½®æ–‡ä»¶"""
    config_dir = Path(__file__).parent.parent / "config"
    pattern = str(config_dir / "sync*.yaml")

    sync_configs = glob.glob(pattern)
    config_names = [Path(config).stem for config in sync_configs]

    print(f"ğŸ” å‘ç° {len(config_names)} ä¸ªåŒæ­¥é…ç½®æ–‡ä»¶:")
    for config_name in config_names:
        print(f"   - {config_name}")

    return config_names


def process_single_config(config_name: str, enable_notification: bool = True) -> Dict[str, Any]:
    """å¤„ç†å•ä¸ªé…ç½®æ–‡ä»¶å¹¶è¿”å›ç»“æœ

    Args:
        config_name: é…ç½®æ–‡ä»¶å
        enable_notification: æ˜¯å¦å¯ç”¨é€šçŸ¥ï¼Œé»˜è®¤ä¸ºTrue
    """
    try:
        print(f"\nğŸš€ å¼€å§‹å¤„ç†é…ç½®: {config_name}")
        print("=" * 60)

        # åŠ è½½é…ç½®æ–‡ä»¶
        config_dir = Path(__file__).parent.parent / "config"
        config_path = config_dir / f"{config_name}.yaml"

        if not config_path.exists():
            return {
                "config_name": config_name,
                "success": False,
                "error": f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}",
                "new_papers": 0,
                "total_papers": 0,
                "research_area": "",
                "table_name": "",
                "ranked_papers": [],
            }

        # ç›´æ¥åŠ è½½YAMLé…ç½®æ–‡ä»¶
        with open(config_path, "r", encoding="utf-8") as f:
            cfg_dict = yaml.safe_load(f)

        cfg = OmegaConf.create(cfg_dict)

        # æ£€æŸ¥æ˜¯å¦æ˜¯æ‰©å±•é…ç½®ç»“æ„
        if hasattr(cfg, "search_config") or hasattr(cfg, "user_profile"):
            # åˆ›å»ºåŸºç¡€é…ç½®ç»“æ„
            base_cfg = OmegaConf.create(
                {
                    "search": {"days": 7, "max_results": 300, "max_display": 10, "min_score": 0.1, "field": "all"},
                    "download": {"enabled": False, "max_downloads": 10, "download_dir": "downloads"},
                    "intelligent_matching": {"enabled": False, "score_weights": {"base": 1.0, "semantic": 0.3}},
                    "display": {"show_ranking": True, "show_scores": True, "show_breakdown": False, "stats": True},
                    "output": {"save": True, "save_keywords": False, "include_scores": True, "format": "markdown"},
                }
            )
            final_cfg = merge_configs(base_cfg, cfg)
        else:
            final_cfg = cfg

        # åˆå§‹åŒ–ç»„ä»¶
        download_dir = final_cfg.get("download", {}).get("download_dir", "downloads")
        arxiv_api = ArxivAPI(download_dir=download_dir)
        paper_ranker = PaperRanker()

        # åŠ è½½å…³é”®è¯
        interest_keywords, exclude_keywords, raw_interest_keywords, required_keywords_config = (
            load_keywords_from_config(final_cfg)
        )

        # è·å–è®ºæ–‡
        search_cfg = final_cfg.get("search", {})
        papers = arxiv_api.get_recent_papers(
            days=search_cfg.get("days", 7),
            max_results=search_cfg.get("max_results", 300),
            field_type=search_cfg.get("field", "all"),
        )

        if not papers:
            return {
                "config_name": config_name,
                "success": True,
                "new_papers": 0,
                "total_papers": 0,
                "research_area": final_cfg.get("user_profile", {}).get(
                    "research_area", config_name.replace("sync_", "")
                ),
                "table_name": final_cfg.get("user_profile", {}).get("name", "").replace("ç ”ç©¶å‘˜", "") + "è®ºæ–‡è¡¨",
                "ranked_papers": [],
            }

        # æ™ºèƒ½æ’åºå¤„ç†
        ranked_papers = []
        synced_count = 0
        if interest_keywords or exclude_keywords:
            intelligent_cfg = final_cfg.get("intelligent_matching", {})
            use_intelligent = intelligent_cfg.get("enabled", False)
            score_weights = dict(intelligent_cfg.get("score_weights", {})) if use_intelligent else None

            ranked_papers, excluded_papers, score_stats = paper_ranker.filter_and_rank_papers(
                papers,
                interest_keywords,
                exclude_keywords,
                search_cfg.get("min_score", 0.1),
                use_advanced_scoring=use_intelligent,
                score_weights=score_weights,
                raw_interest_keywords=raw_interest_keywords,
                required_keywords_config=required_keywords_config,
            )

            # åŒæ­¥åˆ°é£ä¹¦å¤šç»´è¡¨æ ¼
            if ranked_papers and AUTOPAPER_AVAILABLE:
                synced_count = sync_to_autopaper(ranked_papers, final_cfg, enable_notification)

            return {
                "config_name": config_name,
                "success": True,
                "new_papers": max(synced_count, 0),
                "total_papers": len(ranked_papers),
                "research_area": final_cfg.get("user_profile", {}).get(
                    "research_area", config_name.replace("sync_", "")
                ),
                "table_name": final_cfg.get("user_profile", {}).get("name", "").replace("ç ”ç©¶å‘˜", "").strip()
                + "è®ºæ–‡è¡¨",
                "ranked_papers": ranked_papers if ranked_papers else [],
            }

        return {
            "config_name": config_name,
            "success": True,
            "new_papers": 0,
            "total_papers": len(papers),
            "research_area": final_cfg.get("user_profile", {}).get("research_area", config_name.replace("sync_", "")),
            "table_name": final_cfg.get("user_profile", {}).get("name", "").replace("ç ”ç©¶å‘˜", "").strip() + "è®ºæ–‡è¡¨",
            "ranked_papers": [],
        }

    except Exception as e:
        print(f"âŒ é…ç½® {config_name} å¤„ç†å¤±è´¥: {e}")
        traceback.print_exc()
        return {
            "config_name": config_name,
            "success": False,
            "error": str(e),
            "new_papers": 0,
            "total_papers": 0,
            "research_area": config_name.replace("sync_", ""),
            "table_name": f'{config_name.replace("sync_", "")}è®ºæ–‡è¡¨',
            "ranked_papers": [],
        }


def process_all_configs() -> bool:
    """å¤„ç†æ‰€æœ‰syncé…ç½®æ–‡ä»¶å¹¶å‘é€æ±‡æ€»é€šçŸ¥"""
    try:
        print("ğŸš€ ArXivè®ºæ–‡æ‰¹é‡åŒæ­¥æ¨¡å¼")
        print("=" * 70)

        # æŸ¥æ‰¾æ‰€æœ‰syncé…ç½®
        sync_configs = find_sync_configs()
        if not sync_configs:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°åŒæ­¥é…ç½®æ–‡ä»¶")
            return False

        print(f"\nğŸ¯ å¼€å§‹å¤„ç† {len(sync_configs)} ä¸ªé…ç½®...")

        # å¤„ç†æ¯ä¸ªé…ç½®
        all_results = []
        total_new_papers = 0
        successful_configs = 0

        for config_name in sync_configs:
            # åœ¨æ‰¹é‡å¤„ç†æ¨¡å¼ä¸‹ç¦ç”¨å•ä¸ªé…ç½®çš„é€šçŸ¥
            result = process_single_config(config_name, enable_notification=False)
            all_results.append(result)

            if result["success"]:
                successful_configs += 1
                total_new_papers += result["new_papers"]
                print(f"âœ… {config_name}: {result['new_papers']} ç¯‡æ–°è®ºæ–‡")
            else:
                print(f"âŒ {config_name}: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

        print(f"\nğŸ“Š æ‰¹é‡å¤„ç†å®Œæˆ!")
        print(f"âœ… æˆåŠŸ: {successful_configs} ä¸ª")
        print(f"âŒ å¤±è´¥: {len(sync_configs) - successful_configs} ä¸ª")
        print(f"ğŸ“š æ€»æ–°å¢è®ºæ–‡: {total_new_papers} ç¯‡")

        # å‘é€æ±‡æ€»é€šçŸ¥
        if total_new_papers > 0:
            print("\nğŸ“¢ å‘é€æ±‡æ€»é€šçŸ¥...")
            return send_batch_summary_notification(all_results)
        else:
            print("\nâ„¹ï¸ æ²¡æœ‰æ–°è®ºæ–‡ï¼Œè·³è¿‡é€šçŸ¥å‘é€")
            return True

    except Exception as e:
        print(f"âŒ æ‰¹é‡å¤„ç†å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def send_batch_summary_notification(results: List[Dict[str, Any]]) -> bool:
    """å‘é€æ‰¹é‡å¤„ç†çš„æ±‡æ€»é€šçŸ¥"""
    try:
        if not AUTOPAPER_AVAILABLE:
            print("âš ï¸ AutoPaperæ¨¡å—ä¸å¯ç”¨ï¼Œè·³è¿‡é€šçŸ¥")
            return False

        # åŠ è½½é»˜è®¤é…ç½®ç”¨äºé€šçŸ¥
        config_dir = Path(__file__).parent.parent / "config"
        default_config_path = config_dir / "default.yaml"

        with open(default_config_path, "r", encoding="utf-8") as f:
            default_cfg_dict = yaml.safe_load(f)
        default_cfg = OmegaConf.create(default_cfg_dict)

        # ä½¿ç”¨autopaperè¿›è¡Œé€šçŸ¥
        try:
            from dotenv import load_dotenv

            load_dotenv()

            config = FeishuConfig.from_env()
            sync_manager = SyncManager(config)

            # æ„å»ºæ±‡æ€»æ•°æ®
            update_stats = {}
            papers_by_field = {}
            table_links = {}

            for result in results:
                if result["success"] and result["new_papers"] > 0:
                    field_name = result["research_area"]
                    update_stats[field_name] = {
                        "new_count": result["new_papers"],
                        "total_count": result["total_papers"],
                        "table_name": result["table_name"],
                    }
                    papers_by_field[field_name] = result["ranked_papers"]

                    # è·å–å®é™…çš„è¡¨æ ¼é“¾æ¥
                    try:
                        # é€šè¿‡è¡¨æ ¼åç§°æŸ¥æ‰¾è¡¨æ ¼IDï¼Œç„¶åæ„å»ºé“¾æ¥
                        table_id = sync_manager.bitable_manager.find_table_by_name(result["table_name"])
                        if table_id:
                            table_links[field_name] = f"https://feishu.cn/base/{config.app_token}?table={table_id}"
                        else:
                            table_links[field_name] = f"https://feishu.cn/base/{config.app_token}"
                    except Exception as e:
                        print(f"âš ï¸ è·å–è¡¨æ ¼é“¾æ¥å¤±è´¥: {e}")
                        table_links[field_name] = f"https://feishu.cn/base/{config.app_token}"

            if not update_stats:
                print("â„¹ï¸ æ²¡æœ‰éœ€è¦é€šçŸ¥çš„æ›´æ–°")
                return True

            # å‘é€é€šçŸ¥
            success = sync_manager.chat_notifier.notify_paper_updates(
                update_stats=update_stats, papers_by_field=papers_by_field, table_links=table_links
            )

            if success:
                print("âœ… æ±‡æ€»é€šçŸ¥å‘é€æˆåŠŸ")
            else:
                print("âŒ æ±‡æ€»é€šçŸ¥å‘é€å¤±è´¥")

            return success

        except Exception as e:
            print(f"âŒ ä½¿ç”¨autopaperå‘é€é€šçŸ¥å¤±è´¥: {e}")
            return False

    except Exception as e:
        print(f"âŒ å‘é€æ±‡æ€»é€šçŸ¥å¤±è´¥: {e}")
        traceback.print_exc()
        return False


def sync_to_autopaper(papers, cfg: DictConfig, enable_notification: bool = True) -> int:
    """ä½¿ç”¨autopaperåŒæ­¥è®ºæ–‡åˆ°é£ä¹¦å¤šç»´è¡¨æ ¼

    Args:
        papers: è®ºæ–‡åˆ—è¡¨
        cfg: é…ç½®å¯¹è±¡
        enable_notification: æ˜¯å¦å¯ç”¨é€šçŸ¥ï¼Œé»˜è®¤ä¸ºTrue
    """
    if not AUTOPAPER_AVAILABLE:
        print("âš ï¸ AutoPaperæ¨¡å—ä¸å¯ç”¨ï¼Œè·³è¿‡åŒæ­¥")
        return 0

    try:
        from dotenv import load_dotenv

        load_dotenv()

        # æ£€æŸ¥ç¯å¢ƒå˜é‡
        required_vars = ["FEISHU_APP_ID", "FEISHU_APP_SECRET", "FEISHU_BITABLE_APP_TOKEN"]
        missing_vars = [var for var in required_vars if not os.getenv(var)]

        if missing_vars:
            print(f"âŒ ç¼ºå°‘ç¯å¢ƒå˜é‡: {missing_vars}")
            return 0

        print("\nğŸ”— å¼€å§‹ä½¿ç”¨AutoPaperåŒæ­¥åˆ°é£ä¹¦å¤šç»´è¡¨æ ¼...")

        # åˆ›å»ºé…ç½®å’ŒåŒæ­¥ç®¡ç†å™¨
        config = FeishuConfig.from_env()
        sync_manager = SyncManager(config)

        # å¼ºåˆ¶åˆ·æ–°tokenç¡®ä¿è¿æ¥æœ‰æ•ˆ
        print("ğŸ”„ æ£€æŸ¥å¹¶åˆ·æ–°é£ä¹¦è®¿é—®token...")
        try:
            new_token = sync_manager.bitable_manager.connector.get_tenant_access_token()
            if new_token:
                print(f"âœ… Tokenåˆ·æ–°æˆåŠŸ: {new_token[:20]}...")
            else:
                print("âŒ Tokenåˆ·æ–°å¤±è´¥")
                return 0
        except Exception as e:
            print(f"âŒ Tokenåˆ·æ–°å¤±è´¥: {e}")
            return 0

        # å‡†å¤‡è®ºæ–‡æ•°æ®
        print(f"ğŸ” è°ƒè¯•: æ£€æŸ¥ä¼ å…¥çš„è®ºæ–‡æ•°æ®ç±»å‹å’Œç»“æ„...")
        print(f"   è®ºæ–‡æ•°é‡: {len(papers)}")
        if papers:
            first_paper = papers[0]
            print(f"   ç¬¬ä¸€ç¯‡è®ºæ–‡ç±»å‹: {type(first_paper)}")
            if isinstance(first_paper, dict):
                print(f"   ç¬¬ä¸€ç¯‡è®ºæ–‡å­—å…¸é”®: {list(first_paper.keys())}")
                print(f"   ç¬¬ä¸€ç¯‡è®ºæ–‡è¯„åˆ†: {first_paper.get('score', 'æœªæ‰¾åˆ°scoreå­—æ®µ')}")
            else:
                print(f"   ç¬¬ä¸€ç¯‡è®ºæ–‡å¯¹è±¡å±æ€§: {[attr for attr in dir(first_paper) if not attr.startswith('_')]}")

        feishu_papers = []
        for paper in papers:
            if isinstance(paper, dict):
                # ä¿®å¤è¯„åˆ†å­—æ®µæ˜ å°„
                paper_data = paper.copy()
                if "relevance_score" in paper_data and "score" not in paper_data:
                    paper_data["score"] = paper_data["relevance_score"]
                # ç¡®ä¿æœ‰scoreå­—æ®µ
                if "score" not in paper_data:
                    paper_data["score"] = 0.0
            else:
                # è½¬æ¢è®ºæ–‡å¯¹è±¡ä¸ºå­—å…¸æ ¼å¼
                paper_data = {
                    "title": getattr(paper, "title", ""),
                    "authors": getattr(paper, "authors", []),
                    "abstract": getattr(paper, "summary", ""),
                    "arxiv_id": getattr(paper, "id", "").split("/")[-1],
                    "published": getattr(paper, "published", None),
                    "categories": getattr(paper, "categories", []),
                    "pdf_url": getattr(paper, "pdf_url", ""),
                    "score": getattr(
                        paper, "score", getattr(paper, "final_score", getattr(paper, "relevance_score", 0.0))
                    ),
                    "matched_keywords": getattr(paper, "matched_keywords", []),
                }
            feishu_papers.append(paper_data)

        if not feishu_papers:
            print("â„¹ï¸ æ²¡æœ‰è®ºæ–‡éœ€è¦åŒæ­¥")
            return 0

        # è¿‡æ»¤ä½åˆ†è®ºæ–‡
        sync_threshold = cfg.get("feishu", {}).get("sync_threshold", 0.0)
        papers_to_sync = [p for p in feishu_papers if p.get("score", 0) >= sync_threshold]

        print(f"ğŸ“Š è®ºæ–‡è¯„åˆ†ç»Ÿè®¡:")
        scores = [p.get("score", 0) for p in feishu_papers]
        if scores:
            print(f"   æœ€é«˜åˆ†: {max(scores):.3f}")
            print(f"   æœ€ä½åˆ†: {min(scores):.3f}")
            print(f"   å¹³å‡åˆ†: {sum(scores)/len(scores):.3f}")
            print(f"   åŒæ­¥é˜ˆå€¼: {sync_threshold}")
            high_score_count = len([s for s in scores if s >= sync_threshold])
            print(f"   ç¬¦åˆé˜ˆå€¼çš„è®ºæ–‡: {high_score_count}/{len(scores)}")

        if not papers_to_sync:
            print("â„¹ï¸ æ²¡æœ‰ç¬¦åˆåŒæ­¥æ¡ä»¶çš„è®ºæ–‡")
            return 0

        print(f"ğŸ“Š å‡†å¤‡åŒæ­¥ {len(papers_to_sync)} ç¯‡è®ºæ–‡åˆ°é£ä¹¦...")

        # ä½¿ç”¨autopaperåŒæ­¥
        research_area = cfg.get("user_profile", {}).get("research_area", "unknown")

        try:
            print("ğŸš€ å¼€å§‹åŒæ­¥åˆ°é£ä¹¦...")
            result = sync_manager.sync_papers_to_feishu(
                papers=papers_to_sync,
                research_area=research_area,
                user_name=cfg.get("user_profile", {}).get("name", "ç ”ç©¶å‘˜"),
                sync_threshold=sync_threshold,
                enable_notification=enable_notification,
            )

            # æ­£ç¡®å¤„ç†è¿”å›å€¼
            synced_count = result.get("synced_count", 0) if isinstance(result, dict) else 0

            print(f"ğŸ‰ AutoPaperåŒæ­¥å®Œæˆï¼æˆåŠŸåŒæ­¥ {synced_count} ç¯‡è®ºæ–‡")
            return synced_count

        except Exception as e:
            print(f"âŒ AutoPaperåŒæ­¥å¤±è´¥: {e}")
            return 0

    except Exception as e:
        print(f"âŒ AutoPaperåŒæ­¥å¤±è´¥: {e}")
        traceback.print_exc()
        return 0


def load_keywords_from_config(cfg: DictConfig):
    """ä»é…ç½®ä¸­åŠ è½½å…³é”®è¯"""
    # æ–°çš„æ‰©å±•é…ç½®ç»“æ„ - ä¼˜å…ˆä½¿ç”¨å…³é”®è¯é…ç½®ä¸­çš„è®¾ç½®
    if hasattr(cfg, "keywords"):
        # ä¼ ç»Ÿç»“æ„æ”¯æŒ
        raw_interest_keywords = cfg.keywords.get("interest_keywords", [])
        raw_exclude_keywords = cfg.keywords.get("exclude_keywords", [])
    else:
        # ç›´æ¥ä»æ ¹çº§åˆ«è·å–
        raw_interest_keywords = cfg.get("interest_keywords", [])
        raw_exclude_keywords = cfg.get("exclude_keywords", [])

    # è½¬æ¢ä¸ºPythonåˆ—è¡¨ï¼ˆå¦‚æœæ˜¯DictConfigï¼‰
    if hasattr(raw_interest_keywords, "_content"):
        raw_interest_keywords = list(raw_interest_keywords)
    if hasattr(raw_exclude_keywords, "_content"):
        raw_exclude_keywords = list(raw_exclude_keywords)

    # è¿‡æ»¤æ‰æ³¨é‡Šè¡Œå’Œç©ºè¡Œï¼ˆç”¨äºå®é™…åŒ¹é…ï¼‰
    interest_keywords = _filter_keywords(raw_interest_keywords)
    exclude_keywords = _filter_keywords(raw_exclude_keywords)

    # åŠ è½½å¿…é¡»åŒ…å«å…³é”®è¯é…ç½®
    required_keywords_config = cfg.get("required_keywords", {})

    return interest_keywords, exclude_keywords, raw_interest_keywords, required_keywords_config


def _filter_keywords(keywords):
    """è¿‡æ»¤å…³é”®è¯åˆ—è¡¨ï¼Œç§»é™¤æ³¨é‡Šè¡Œå’Œç©ºè¡Œ"""
    if not keywords:
        return []

    filtered = []
    for keyword in keywords:
        # è·³è¿‡ç©ºå­—ç¬¦ä¸²
        if not keyword or not keyword.strip():
            continue

        # è·³è¿‡æ³¨é‡Šè¡Œï¼ˆä»¥ # å¼€å¤´ï¼‰
        if keyword.strip().startswith("#"):
            continue

        # ä¿ç•™æœ‰æ•ˆå…³é”®è¯
        filtered.append(keyword.strip())

    return filtered


def merge_configs(global_cfg: DictConfig, keyword_cfg: DictConfig) -> DictConfig:
    """åˆå¹¶å…¨å±€é…ç½®å’Œå…³é”®è¯é…ç½®ï¼Œå…³é”®è¯é…ç½®ä¼˜å…ˆ"""
    merged_cfg = OmegaConf.merge(global_cfg, keyword_cfg)

    # å¦‚æœå…³é”®è¯é…ç½®æœ‰search_configï¼Œåˆ™è¦†ç›–å…¨å±€searché…ç½®
    if hasattr(keyword_cfg, "search_config"):
        merged_cfg.search = OmegaConf.merge(merged_cfg.search, keyword_cfg.search_config)

    # å¦‚æœå…³é”®è¯é…ç½®æœ‰å…¶ä»–_configåç¼€çš„é…ç½®ï¼Œåˆ™è¦†ç›–å¯¹åº”çš„å…¨å±€é…ç½®
    config_mappings = {
        "intelligent_matching_config": "intelligent_matching",
        "download_config": "download",
        "display_config": "display",
        "output_config": "output",
    }

    for config_key, global_key in config_mappings.items():
        if hasattr(keyword_cfg, config_key):
            if not hasattr(merged_cfg, global_key):
                merged_cfg[global_key] = OmegaConf.create({})
            merged_cfg[global_key] = OmegaConf.merge(merged_cfg[global_key], keyword_cfg[config_key])

    return merged_cfg


def print_config_info(cfg: DictConfig):
    """æ‰“å°é…ç½®ä¿¡æ¯"""
    print("ğŸ“š AutoPaper CLI - ArXiv è®ºæ–‡é‡‡é›†å·¥å…·")
    print(f"ğŸ•’ {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}")
    print("=" * 70)

    # æ˜¾ç¤ºç”¨æˆ·ä¿¡æ¯ï¼ˆæ–°çš„æ‰©å±•é…ç½®ç»“æ„ï¼‰
    if hasattr(cfg, "user_profile"):
        print(f"ğŸ‘¤ ç”¨æˆ·: {cfg.user_profile.get('name', 'Unknown')}")
        print(f"ğŸ“ æè¿°: {cfg.user_profile.get('description', 'No description')}")
        print(f"ğŸ”¬ ç ”ç©¶é¢†åŸŸ: {cfg.user_profile.get('research_area', 'general')}")
    elif hasattr(cfg, "keywords") and hasattr(cfg.keywords, "description"):
        # å‘åå…¼å®¹ä¼ ç»Ÿç»“æ„
        keywords_name = cfg.defaults[0].keywords if hasattr(cfg, "defaults") else "unknown"
        print(f"ğŸ“‹ å½“å‰é…ç½®: {keywords_name}")
        print(f"ğŸ“ é…ç½®æè¿°: {cfg.keywords.description}")
    else:
        # æœ€åŸºç¡€çš„å…¼å®¹æ€§
        keywords_name = cfg.defaults[0].keywords if hasattr(cfg, "defaults") else "unknown"
        print(f"ğŸ“‹ å½“å‰é…ç½®: {keywords_name}")

    # æ˜¾ç¤ºå…³é”®è¯
    interest_keywords, exclude_keywords, raw_interest_keywords, required_keywords_config = load_keywords_from_config(
        cfg
    )

    if interest_keywords:
        print(f"\nğŸ¯ å…³æ³¨è¯æ¡ ({len(interest_keywords)}ä¸ª):")
        print(f"   {' > '.join(interest_keywords[:5])}{'...' if len(interest_keywords) > 5 else ''}")

    if exclude_keywords:
        print(f"ğŸš« æ’é™¤è¯æ¡ ({len(exclude_keywords)}ä¸ª):")
        print(f"   {', '.join(exclude_keywords[:5])}{'...' if len(exclude_keywords) > 5 else ''}")

    # æ˜¾ç¤ºå¿…é¡»å…³é”®è¯
    if required_keywords_config.get("enabled", False):
        required_keywords = required_keywords_config.get("keywords", [])
        fuzzy_match = required_keywords_config.get("fuzzy_match", True)
        threshold = required_keywords_config.get("similarity_threshold", 0.8)
        print(f"âœ… å¿…é¡»åŒ…å«å…³é”®è¯ ({len(required_keywords)}ä¸ª):")
        print(f"   {', '.join(required_keywords[:3])}{'...' if len(required_keywords) > 3 else ''}")
        print(f"   æ¨¡ç³ŠåŒ¹é…: {'å¯ç”¨' if fuzzy_match else 'ç¦ç”¨'}, é˜ˆå€¼: {threshold}")
    else:
        print(f"âœ… å¿…é¡»åŒ…å«å…³é”®è¯: æœªå¯ç”¨")

    print(f"âš™ï¸  æœç´¢å‚æ•°:")
    search_cfg = cfg.get("search", {})
    print(f"   å¤©æ•°: {search_cfg.get('days', 'N/A')}, æœ€å¤§ç»“æœ: {search_cfg.get('max_results', 'N/A')}")
    print(f"   é¢†åŸŸ: {search_cfg.get('field', 'N/A')}, æœ€å°è¯„åˆ†: {search_cfg.get('min_score', 'N/A')}")

    # æ˜¾ç¤ºæ™ºèƒ½åŒ¹é…é…ç½®
    intelligent_cfg = cfg.get("intelligent_matching", {})
    if intelligent_cfg.get("enabled", False):
        print(f"ğŸ§  æ™ºèƒ½åŒ¹é…: å¯ç”¨")
        weights = intelligent_cfg.get("score_weights", {})
        print(
            f"   è¯„åˆ†æƒé‡: åŸºç¡€({weights.get('base', 0)}) è¯­ä¹‰({weights.get('semantic', 0)}) æ–°é¢–æ€§({weights.get('novelty', 0)})"
        )
    else:
        print(f"ğŸ§  æ™ºèƒ½åŒ¹é…: å…³é—­")

    # æ˜¾ç¤ºä¸‹è½½é…ç½®
    download_cfg = cfg.get("download", {})
    if download_cfg.get("enabled", False):
        print(f"ğŸ“¥ PDFä¸‹è½½: å¯ç”¨ (æœ€å¤š{download_cfg.get('max_downloads', 0)}ç¯‡)")
    else:
        print(f"ğŸ“¥ PDFä¸‹è½½: å…³é—­")

    print("=" * 70)


@hydra.main(version_base=None, config_path="../config", config_name="default")
def main(cfg: DictConfig) -> None:
    """ä¸»å‡½æ•°"""
    try:
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ‰¹é‡å¤„ç†æ¨¡å¼
        try:
            hydra_cfg = HydraConfig.get()
            config_name = hydra_cfg.job.config_name

            if config_name == "all":
                process_all_configs()
                return
        except:
            pass  # å¦‚æœæ— æ³•è·å–é…ç½®åï¼Œç»§ç»­æ­£å¸¸å¤„ç†

        # æ£€æŸ¥æ˜¯å¦æ˜¯æ‰©å±•é…ç½®ç»“æ„ï¼Œå¦‚æœæ˜¯åˆ™è¿›è¡Œé…ç½®åˆå¹¶
        if hasattr(cfg, "search_config") or hasattr(cfg, "user_profile"):
            # åˆ›å»ºåŸºç¡€é…ç½®ç»“æ„
            base_cfg = OmegaConf.create(
                {
                    "search": {"days": 7, "max_results": 300, "max_display": 10, "min_score": 0.1, "field": "all"},
                    "download": {"enabled": False, "max_downloads": 10, "download_dir": "downloads"},
                    "intelligent_matching": {"enabled": False, "score_weights": {"base": 1.0, "semantic": 0.3}},
                    "display": {"show_ranking": True, "show_scores": True, "show_breakdown": False, "stats": True},
                    "output": {"save": True, "save_keywords": False, "include_scores": True, "format": "markdown"},
                }
            )
            final_cfg = merge_configs(base_cfg, cfg)
        else:
            final_cfg = cfg

        # åˆå§‹åŒ–ç»„ä»¶
        download_dir = final_cfg.get("download", {}).get("download_dir", "downloads")
        arxiv_api = ArxivAPI(download_dir=download_dir)
        paper_ranker = PaperRanker()
        displayer = PaperDisplayer()

        # æ‰“å°é…ç½®ä¿¡æ¯
        print_config_info(final_cfg)

        # åŠ è½½å…³é”®è¯
        interest_keywords, exclude_keywords, raw_interest_keywords, required_keywords_config = (
            load_keywords_from_config(final_cfg)
        )

        # è·å–è®ºæ–‡ - ä½¿ç”¨æ–°çš„å­—æ®µç±»å‹
        search_cfg = final_cfg.get("search", {})
        papers = arxiv_api.get_recent_papers(
            days=search_cfg.get("days", 7),
            max_results=search_cfg.get("max_results", 300),
            field_type=search_cfg.get("field", "all"),
        )

        if not papers:
            print("âŒ æœªè·å–åˆ°ä»»ä½•è®ºæ–‡")
            return

        # é¢†åŸŸç­›é€‰
        field_names = {"ai": "äººå·¥æ™ºèƒ½/æœºå™¨å­¦ä¹ ", "robotics": "æœºå™¨äººå­¦", "cv": "è®¡ç®—æœºè§†è§‰", "nlp": "è‡ªç„¶è¯­è¨€å¤„ç†"}
        field = search_cfg.get("field", "all")

        if field != "all":
            field_name = field_names.get(field, field)
            print(f"\nğŸ¯ ä¸“ä¸šé¢†åŸŸ: {field_name}")
        else:
            print(f"\nğŸŒ å…¨é¢†åŸŸæœç´¢")

        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        display_cfg = final_cfg.get("display", {})
        if display_cfg.get("stats", True):
            print(f"ğŸ“Š è·å–è®ºæ–‡: {len(papers)} ç¯‡")

        # æ™ºèƒ½æ’åºå¤„ç†
        if interest_keywords or exclude_keywords:
            intelligent_cfg = final_cfg.get("intelligent_matching", {})
            use_intelligent = intelligent_cfg.get("enabled", False)
            score_weights = dict(intelligent_cfg.get("score_weights", {})) if use_intelligent else None

            ranked_papers, excluded_papers, score_stats = paper_ranker.filter_and_rank_papers(
                papers,
                interest_keywords,
                exclude_keywords,
                search_cfg.get("min_score", 0.1),
                use_advanced_scoring=use_intelligent,
                score_weights=score_weights,
                raw_interest_keywords=raw_interest_keywords,
                required_keywords_config=required_keywords_config,
            )

            print(f"âœ… ç­›é€‰åè®ºæ–‡: {len(ranked_papers)} ç¯‡")
            print(f"âŒ æ’é™¤è®ºæ–‡: {len(excluded_papers)} ç¯‡")

            # æ˜¾ç¤ºè®ºæ–‡
            max_display = search_cfg.get("max_display", 10)
            if max_display > 0:
                displayer.display_papers(ranked_papers[:max_display], final_cfg)

            # åŒæ­¥åˆ°é£ä¹¦
            if ranked_papers and AUTOPAPER_AVAILABLE:
                sync_to_autopaper(ranked_papers, final_cfg)  # é»˜è®¤enable_notification=True

        else:
            print(f"â„¹ï¸ æœªé…ç½®å…³é”®è¯ï¼Œæ˜¾ç¤ºæ‰€æœ‰è®ºæ–‡")
            max_display = search_cfg.get("max_display", 10)
            if max_display > 0:
                displayer.display_papers(papers[:max_display], final_cfg)

        print(f"\nâœ… é‡‡é›†å®Œæˆï¼")

    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
