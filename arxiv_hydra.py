#!/usr/bin/env python3
"""
åŸºäº Hydra çš„ ArXiv è®ºæ–‡é‡‡é›†å·¥å…·
æ”¯æŒçµæ´»çš„é…ç½®ç®¡ç†å’Œä¸“ä¸šé¢†åŸŸå…³é”®è¯
"""

import sys
import os
from datetime import datetime
import hydra
from hydra.core.hydra_config import HydraConfig
from omegaconf import DictConfig, OmegaConf
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from arxiv_core import ArxivAPI, PaperRanker
from paper_display import PaperDisplayer

# é£ä¹¦é›†æˆå¯¼å…¥
try:
    from feishu_bitable_connector import FeishuBitableConnector
    from sync_to_feishu import sync_papers_to_feishu

    FEISHU_AVAILABLE = True
except ImportError:
    FEISHU_AVAILABLE = False
    print("âš ï¸ é£ä¹¦æ¨¡å—æœªæ‰¾åˆ°ï¼Œå°†è·³è¿‡é£ä¹¦åŒæ­¥åŠŸèƒ½")


def sync_to_feishu(papers, cfg: DictConfig):
    """åŒæ­¥è®ºæ–‡åˆ°é£ä¹¦å¤šç»´è¡¨æ ¼"""
    if not FEISHU_AVAILABLE:
        print("âš ï¸ é£ä¹¦æ¨¡å—ä¸å¯ç”¨ï¼Œè·³è¿‡åŒæ­¥")
        return False

    feishu_cfg = cfg.get('feishu', {})
    if not feishu_cfg.get('enabled', True):
        print("â„¹ï¸ é£ä¹¦åŒæ­¥å·²ç¦ç”¨")
        return False

    try:
        from dotenv import load_dotenv

        load_dotenv()

        # æ£€æŸ¥ç¯å¢ƒå˜é‡
        required_vars = ['FEISHU_APP_ID', 'FEISHU_APP_SECRET', 'FEISHU_BITABLE_APP_TOKEN', 'FEISHU_PAPERS_TABLE_ID']

        missing_vars = []
        for var in required_vars:
            if not os.getenv(var):
                missing_vars.append(var)

        # æ£€æŸ¥è®¿é—®ä»¤ç‰Œ
        has_user_token = bool(os.getenv('FEISHU_USER_ACCESS_TOKEN')) and 'xxxx' not in os.getenv(
            'FEISHU_USER_ACCESS_TOKEN', ''
        )
        has_tenant_token = bool(os.getenv('FEISHU_TENANT_ACCESS_TOKEN')) and 'xxxx' not in os.getenv(
            'FEISHU_TENANT_ACCESS_TOKEN', ''
        )

        if missing_vars or (not has_user_token and not has_tenant_token):
            print("âŒ é£ä¹¦é…ç½®ä¸å®Œæ•´ï¼Œè¯·å…ˆè¿è¡Œ setup_feishu.py é…ç½®")
            return False

        print("\nğŸ”— å¼€å§‹åŒæ­¥åˆ°é£ä¹¦å¤šç»´è¡¨æ ¼...")
        connector = FeishuBitableConnector()

        # å‡†å¤‡è®ºæ–‡æ•°æ®
        feishu_papers = []
        for paper in papers:
            # å¤„ç†ä¸åŒçš„è®ºæ–‡å¯¹è±¡æ ¼å¼
            if isinstance(paper, dict):
                # å­—å…¸æ ¼å¼çš„è®ºæ–‡å¯¹è±¡
                paper_data = {
                    "ArXiv ID": paper.get('arxiv_id', ''),
                    "æ ‡é¢˜": paper.get('title', ''),
                    "ä½œè€…": paper.get('authors_str', ''),
                    "æ‘˜è¦": (paper.get('summary', '')[:1000] if paper.get('summary') else ""),
                    "åˆ†ç±»": paper.get('categories_str', ''),
                    "å‘å¸ƒæ—¥æœŸ": paper.get('published_date').strftime("%Y-%m-%d") if paper.get('published_date') else "",
                    "æ›´æ–°æ—¥æœŸ": paper.get('updated_date').strftime("%Y-%m-%d") if paper.get('updated_date') else "",
                    "PDFé“¾æ¥": paper.get('pdf_url', ''),
                    "è®ºæ–‡é“¾æ¥": paper.get('paper_url', ''),
                }
            else:
                # å¯¹è±¡æ ¼å¼çš„è®ºæ–‡å¯¹è±¡
                paper_data = {
                    "ArXiv ID": getattr(paper, 'id', getattr(paper, 'arxiv_id', '')),
                    "æ ‡é¢˜": getattr(paper, 'title', ''),
                    "ä½œè€…": ", ".join(getattr(paper, 'authors', [])),
                    "æ‘˜è¦": (getattr(paper, 'summary', '')[:1000] if getattr(paper, 'summary') else ""),
                    "åˆ†ç±»": ", ".join(getattr(paper, 'categories', [])),
                    "å‘å¸ƒæ—¥æœŸ": (
                        getattr(paper, 'published', None).strftime("%Y-%m-%d")
                        if getattr(paper, 'published', None)
                        else ""
                    ),
                    "æ›´æ–°æ—¥æœŸ": (
                        getattr(paper, 'updated', None).strftime("%Y-%m-%d") if getattr(paper, 'updated', None) else ""
                    ),
                    "PDFé“¾æ¥": getattr(paper, 'pdf_url', ''),
                    "è®ºæ–‡é“¾æ¥": getattr(paper, 'entry_id', ''),
                }
            feishu_papers.append(paper_data)

        # æ‰¹é‡åŒæ­¥åˆ°é£ä¹¦
        sync_threshold = feishu_cfg.get('sync_threshold', 0.0)
        batch_size = feishu_cfg.get('batch_size', 20)

        # è¿‡æ»¤ä½åˆ†è®ºæ–‡ï¼ˆå¦‚æœæœ‰è¯„åˆ†ï¼‰
        papers_to_sync = []
        for i, paper_data in enumerate(feishu_papers):
            if hasattr(papers[i], 'score') and papers[i].score < sync_threshold:
                continue
            papers_to_sync.append(paper_data)

        if not papers_to_sync:
            print("â„¹ï¸ æ²¡æœ‰ç¬¦åˆåŒæ­¥æ¡ä»¶çš„è®ºæ–‡")
            return True

        print(f"ğŸ“Š å‡†å¤‡åŒæ­¥ {len(papers_to_sync)} ç¯‡è®ºæ–‡åˆ°é£ä¹¦...")

        # åˆ†æ‰¹åŒæ­¥
        synced_count = 0
        for i in range(0, len(papers_to_sync), batch_size):
            batch = papers_to_sync[i : i + batch_size]

            try:
                result = connector.batch_insert_paper_records(batch)
                if result and result.get('records'):
                    batch_synced = len(result.get('records', []))
                    synced_count += batch_synced
                    print(f"âœ… ç¬¬ {i//batch_size + 1} æ‰¹åŒæ­¥æˆåŠŸ: {batch_synced} ç¯‡")
                else:
                    print(f"âš ï¸ ç¬¬ {i//batch_size + 1} æ‰¹åŒæ­¥å¯èƒ½å¤±è´¥")
            except Exception as e:
                print(f"âŒ ç¬¬ {i//batch_size + 1} æ‰¹åŒæ­¥å¤±è´¥: {e}")
                continue

        print(f"ğŸ‰ é£ä¹¦åŒæ­¥å®Œæˆï¼æˆåŠŸåŒæ­¥ {synced_count} ç¯‡è®ºæ–‡")

        # åŒæ­¥å…³ç³»æ•°æ®ï¼ˆå¯é€‰ï¼‰
        research_area = cfg.get('user_profile', {}).get('research_area', 'unknown')
        if research_area and research_area != 'unknown':
            relations_table_id = os.getenv('FEISHU_RELATIONS_TABLE_ID')
            if relations_table_id:
                try:
                    relation_data = {
                        "è®ºæ–‡ID": "batch_" + datetime.now().strftime("%Y%m%d_%H%M%S"),
                        "é¢†åŸŸID": research_area,
                        "é¢†åŸŸåç§°": cfg.get('user_profile', {}).get('name', research_area),
                        "ç›¸å…³æ€§è¯„åˆ†": 1.0,
                        "åŒ¹é…å…³é”®è¯": ", ".join(cfg.get('interest_keywords', [])[:5]),
                    }

                    connector.insert_record(relations_table_id, relation_data)
                    print("âœ… é¢†åŸŸå…³ç³»æ•°æ®åŒæ­¥å®Œæˆ")
                except Exception as e:
                    print(f"âš ï¸ é¢†åŸŸå…³ç³»æ•°æ®åŒæ­¥å¤±è´¥: {e}")

        return True

    except Exception as e:
        print(f"âŒ é£ä¹¦åŒæ­¥å¤±è´¥: {e}")
        return False


def load_keywords_from_config(cfg: DictConfig):
    """ä»é…ç½®ä¸­åŠ è½½å…³é”®è¯"""
    # æ–°çš„æ‰©å±•é…ç½®ç»“æ„ - ä¼˜å…ˆä½¿ç”¨å…³é”®è¯é…ç½®ä¸­çš„è®¾ç½®
    if hasattr(cfg, 'keywords'):
        # ä¼ ç»Ÿç»“æ„æ”¯æŒ
        raw_interest_keywords = cfg.keywords.get('interest_keywords', [])
        raw_exclude_keywords = cfg.keywords.get('exclude_keywords', [])
    else:
        # ç›´æ¥ä»æ ¹çº§åˆ«è·å–
        raw_interest_keywords = cfg.get('interest_keywords', [])
        raw_exclude_keywords = cfg.get('exclude_keywords', [])

    # è½¬æ¢ä¸ºPythonåˆ—è¡¨ï¼ˆå¦‚æœæ˜¯DictConfigï¼‰
    if hasattr(raw_interest_keywords, '_content'):
        raw_interest_keywords = list(raw_interest_keywords)
    if hasattr(raw_exclude_keywords, '_content'):
        raw_exclude_keywords = list(raw_exclude_keywords)

    # è¿‡æ»¤æ‰æ³¨é‡Šè¡Œå’Œç©ºè¡Œï¼ˆç”¨äºå®é™…åŒ¹é…ï¼‰
    interest_keywords = _filter_keywords(raw_interest_keywords)
    exclude_keywords = _filter_keywords(raw_exclude_keywords)

    # åŠ è½½å¿…é¡»åŒ…å«å…³é”®è¯é…ç½®
    required_keywords_config = cfg.get('required_keywords', {})

    return interest_keywords, exclude_keywords, raw_interest_keywords, required_keywords_config


def _filter_keywords(keywords):
    """è¿‡æ»¤å…³é”®è¯åˆ—è¡¨ï¼Œç§»é™¤æ³¨é‡Šè¡Œå’Œç©ºè¡Œ"""
    if not keywords:
        return []

    filtered = []
    for keyword in keywords:
        # è·³è¿‡ç©ºå­—ç¬¦ä¸²
        if not keyword or not keyword.strip():
            continue

        # è·³è¿‡æ³¨é‡Šè¡Œï¼ˆä»¥ # å¼€å¤´ï¼‰
        if keyword.strip().startswith('#'):
            continue

        # ä¿ç•™æœ‰æ•ˆå…³é”®è¯
        filtered.append(keyword.strip())

    return filtered


def merge_configs(global_cfg: DictConfig, keyword_cfg: DictConfig) -> DictConfig:
    """åˆå¹¶å…¨å±€é…ç½®å’Œå…³é”®è¯é…ç½®ï¼Œå…³é”®è¯é…ç½®ä¼˜å…ˆ"""
    merged_cfg = OmegaConf.merge(global_cfg, keyword_cfg)

    # å¦‚æœå…³é”®è¯é…ç½®æœ‰search_configï¼Œåˆ™è¦†ç›–å…¨å±€searché…ç½®
    if hasattr(keyword_cfg, 'search_config'):
        merged_cfg.search = OmegaConf.merge(merged_cfg.search, keyword_cfg.search_config)

    # å¦‚æœå…³é”®è¯é…ç½®æœ‰å…¶ä»–_configåç¼€çš„é…ç½®ï¼Œåˆ™è¦†ç›–å¯¹åº”çš„å…¨å±€é…ç½®
    config_mappings = {
        'intelligent_matching_config': 'intelligent_matching',
        'download_config': 'download',
        'display_config': 'display',
        'output_config': 'output',
    }

    for config_key, global_key in config_mappings.items():
        if hasattr(keyword_cfg, config_key):
            if not hasattr(merged_cfg, global_key):
                merged_cfg[global_key] = {}
            merged_cfg[global_key] = OmegaConf.merge(merged_cfg[global_key], keyword_cfg[config_key])

    return merged_cfg


def print_config_info(cfg: DictConfig):
    """æ‰“å°é…ç½®ä¿¡æ¯"""
    print("ğŸ“š ArXiv è®ºæ–‡é‡‡é›†å·¥å…· - æ‰©å±•é…ç½®ç‰ˆ")
    print(f"ğŸ•’ {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}")
    print("=" * 70)

    # æ˜¾ç¤ºç”¨æˆ·ä¿¡æ¯ï¼ˆæ–°çš„æ‰©å±•é…ç½®ç»“æ„ï¼‰
    if hasattr(cfg, 'user_profile'):
        print(f"ğŸ‘¤ ç”¨æˆ·: {cfg.user_profile.get('name', 'Unknown')}")
        print(f"ğŸ“ æè¿°: {cfg.user_profile.get('description', 'No description')}")
        print(f"ğŸ”¬ ç ”ç©¶é¢†åŸŸ: {cfg.user_profile.get('research_area', 'general')}")
    elif hasattr(cfg, 'keywords') and hasattr(cfg.keywords, 'description'):
        # å‘åå…¼å®¹ä¼ ç»Ÿç»“æ„
        keywords_name = cfg.defaults[0].keywords if hasattr(cfg, 'defaults') else "unknown"
        print(f"ğŸ“‹ å½“å‰é…ç½®: {keywords_name}")
        print(f"ğŸ“ é…ç½®æè¿°: {cfg.keywords.description}")
    else:
        # æœ€åŸºç¡€çš„å…¼å®¹æ€§
        keywords_name = cfg.defaults[0].keywords if hasattr(cfg, 'defaults') else "unknown"
        print(f"ï¿½ å½“å‰é…ç½®: {keywords_name}")

    # æ˜¾ç¤ºå…³é”®è¯
    interest_keywords, exclude_keywords, raw_interest_keywords, required_keywords_config = load_keywords_from_config(
        cfg
    )

    if interest_keywords:
        print(f"\nğŸ¯ å…³æ³¨è¯æ¡ ({len(interest_keywords)}ä¸ª):")
        print(f"   {' > '.join(interest_keywords[:5])}{'...' if len(interest_keywords) > 5 else ''}")

    if exclude_keywords:
        print(f"ğŸš« æ’é™¤è¯æ¡ ({len(exclude_keywords)}ä¸ª):")
        print(f"   {', '.join(exclude_keywords[:5])}{'...' if len(exclude_keywords) > 5 else ''}")

    # æ˜¾ç¤ºå¿…é¡»å…³é”®è¯
    if required_keywords_config.get('enabled', False):
        required_keywords = required_keywords_config.get('keywords', [])
        fuzzy_match = required_keywords_config.get('fuzzy_match', True)
        threshold = required_keywords_config.get('similarity_threshold', 0.8)
        print(f"âœ… å¿…é¡»åŒ…å«å…³é”®è¯ ({len(required_keywords)}ä¸ª):")
        print(f"   {', '.join(required_keywords[:3])}{'...' if len(required_keywords) > 3 else ''}")
        print(f"   æ¨¡ç³ŠåŒ¹é…: {'å¯ç”¨' if fuzzy_match else 'ç¦ç”¨'}, é˜ˆå€¼: {threshold}")
    else:
        print(f"âœ… å¿…é¡»åŒ…å«å…³é”®è¯: æœªå¯ç”¨")

    print(f"âš™ï¸  æœç´¢å‚æ•°:")
    search_cfg = cfg.get('search', {})
    print(f"   å¤©æ•°: {search_cfg.get('days', 'N/A')}, æœ€å¤§ç»“æœ: {search_cfg.get('max_results', 'N/A')}")
    print(f"   é¢†åŸŸ: {search_cfg.get('field', 'N/A')}, æœ€å°è¯„åˆ†: {search_cfg.get('min_score', 'N/A')}")

    # æ˜¾ç¤ºæ™ºèƒ½åŒ¹é…é…ç½®
    intelligent_cfg = cfg.get('intelligent_matching', {})
    if intelligent_cfg.get('enabled', False):
        print(f"ğŸ§  æ™ºèƒ½åŒ¹é…: å¯ç”¨")
        weights = intelligent_cfg.get('score_weights', {})
        print(
            f"   è¯„åˆ†æƒé‡: åŸºç¡€({weights.get('base', 0)}) è¯­ä¹‰({weights.get('semantic', 0)}) æ–°é¢–æ€§({weights.get('novelty', 0)})"
        )
    else:
        print(f"ğŸ§  æ™ºèƒ½åŒ¹é…: å…³é—­")

    # æ˜¾ç¤ºä¸‹è½½é…ç½®
    download_cfg = cfg.get('download', {})
    if download_cfg.get('enabled', False):
        print(f"ğŸ“¥ PDFä¸‹è½½: å¯ç”¨ (æœ€å¤š{download_cfg.get('max_downloads', 0)}ç¯‡)")
    else:
        print(f"ğŸ“¥ PDFä¸‹è½½: å…³é—­")

    print("=" * 70)


@hydra.main(version_base=None, config_path="conf", config_name="default")
def main(cfg: DictConfig) -> None:
    """ä¸»å‡½æ•°"""

    # æ£€æŸ¥æ˜¯å¦æ˜¯æ‰©å±•é…ç½®ç»“æ„ï¼Œå¦‚æœæ˜¯åˆ™è¿›è¡Œé…ç½®åˆå¹¶
    if hasattr(cfg, 'search_config') or hasattr(cfg, 'user_profile'):
        # åˆ›å»ºåŸºç¡€é…ç½®ç»“æ„
        base_cfg = OmegaConf.create(
            {
                'search': {'days': 7, 'max_results': 300, 'max_display': 10, 'min_score': 0.1, 'field': 'all'},
                'download': {
                    'enabled': False,
                    'max_downloads': 10,
                    'download_dir': 'downloads',
                    'create_metadata': True,
                    'create_index': True,
                    'force_download': False,
                },
                'intelligent_matching': {
                    'enabled': False,
                    'score_weights': {'base': 1.0, 'semantic': 0.3, 'author': 0.2, 'novelty': 0.4, 'citation': 0.3},
                    'fuzzy_threshold': 0.8,
                    'time_decay_days': 30,
                },
                'display': {'show_ranking': True, 'show_scores': True, 'show_breakdown': False, 'stats': True},
                'output': {'save': True, 'save_keywords': False, 'include_scores': True, 'format': 'markdown'},
            }
        )
        final_cfg = merge_configs(base_cfg, cfg)
    else:
        # ä¼ ç»Ÿé…ç½®ç»“æ„ï¼Œç›´æ¥ä½¿ç”¨
        final_cfg = cfg

    # åˆå§‹åŒ–ç»„ä»¶
    download_dir = final_cfg.get('download', {}).get('download_dir', 'downloads')
    arxiv_api = ArxivAPI(download_dir=download_dir)
    paper_ranker = PaperRanker()
    displayer = PaperDisplayer()

    # æ‰“å°é…ç½®ä¿¡æ¯
    print_config_info(final_cfg)

    # åŠ è½½å…³é”®è¯
    interest_keywords, exclude_keywords, raw_interest_keywords, required_keywords_config = load_keywords_from_config(
        final_cfg
    )

    # è·å–è®ºæ–‡ - ä½¿ç”¨æ–°çš„å­—æ®µç±»å‹
    search_cfg = final_cfg.get('search', {})
    papers = arxiv_api.get_recent_papers(
        days=search_cfg.get('days', 7),
        max_results=search_cfg.get('max_results', 300),
        field_type=search_cfg.get('field', 'all'),
    )

    if not papers:
        print("âŒ æœªæ‰¾åˆ°ç›¸å…³è®ºæ–‡")
        return

    # é¢†åŸŸç­›é€‰
    field_names = {'ai': 'äººå·¥æ™ºèƒ½/æœºå™¨å­¦ä¹ ', 'robotics': 'æœºå™¨äººå­¦', 'cv': 'è®¡ç®—æœºè§†è§‰', 'nlp': 'è‡ªç„¶è¯­è¨€å¤„ç†'}
    field = search_cfg.get('field', 'all')

    if field != 'all':
        field_name = field_names.get(field, field)
        print(f"\nğŸ¯ {field_name} é¢†åŸŸç­›é€‰ç»“æœ: {len(papers)} ç¯‡")
    else:
        field_name = "å…¨éƒ¨"

    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    display_cfg = final_cfg.get('display', {})
    if display_cfg.get('stats', True):
        displayer.display_hot_categories(papers)

    # æ™ºèƒ½æ’åºå¤„ç†
    if interest_keywords or exclude_keywords:
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨æ™ºèƒ½åŒ¹é…
        intelligent_cfg = final_cfg.get('intelligent_matching', {})
        use_intelligent = intelligent_cfg.get('enabled', False)
        score_weights = None

        if use_intelligent:
            # è·å–æ™ºèƒ½åŒ¹é…é…ç½®
            score_weights = dict(intelligent_cfg.get('score_weights', {}))
            print(f"\nğŸ§  ä½¿ç”¨æ™ºèƒ½åŒ¹é…æ¨¡å¼")
        else:
            print(f"\nğŸ” ä½¿ç”¨åŸºç¡€åŒ¹é…æ¨¡å¼")

        ranked_papers, excluded_papers, score_stats = paper_ranker.filter_and_rank_papers(
            papers,
            interest_keywords,
            exclude_keywords,
            search_cfg.get('min_score', 0.1),
            use_advanced_scoring=use_intelligent,
            score_weights=score_weights,
            raw_interest_keywords=raw_interest_keywords,
            required_keywords_config=required_keywords_config,
        )

        if score_stats:
            if use_intelligent:
                displayer.display_advanced_ranking_stats(ranked_papers, score_stats)
            else:
                displayer.display_ranking_stats(score_stats, excluded_papers)

        if ranked_papers:
            # PDFä¸‹è½½å¤„ç†
            download_cfg = final_cfg.get('download', {})
            if download_cfg.get('enabled', False) and ranked_papers:
                print(f"\nğŸ“¥ å¼€å§‹ä¸‹è½½PDFæ–‡ä»¶...")
                download_stats = arxiv_api.batch_download_pdfs(
                    ranked_papers[: download_cfg.get('max_downloads', 10)],
                    max_downloads=download_cfg.get('max_downloads', 10),
                    create_index=download_cfg.get('create_index', True),
                )

                print(
                    f"ğŸ“Š ä¸‹è½½ç»Ÿè®¡: æˆåŠŸ {download_stats['downloaded']}, "
                    f"è·³è¿‡ {download_stats['skipped']}, å¤±è´¥ {download_stats['failed']}"
                )

                if download_stats['failed_papers']:
                    print("âŒ ä¸‹è½½å¤±è´¥çš„è®ºæ–‡:")
                    for failed in download_stats['failed_papers']:
                        print(f"   - {failed['title'][:50]}... ({failed['error']})")

            # åŒæ­¥åˆ°é£ä¹¦å¤šç»´è¡¨æ ¼
            if FEISHU_AVAILABLE:
                sync_papers_to_feishu(ranked_papers, final_cfg)

            display_cfg = final_cfg.get('display', {})
            if display_cfg.get('show_ranking', True):
                if use_intelligent:
                    show_breakdown = display_cfg.get('show_breakdown', False)
                    displayer.display_advanced_ranked_papers(
                        ranked_papers, search_cfg.get('max_display', 10), show_breakdown=show_breakdown
                    )
                else:
                    displayer.display_ranked_papers(
                        ranked_papers,
                        search_cfg.get('max_display', 10),
                        show_scores=display_cfg.get('show_scores', True),
                    )

            # ä¿å­˜æŠ¥å‘Š
            output_cfg = final_cfg.get('output', {})
            if output_cfg.get('save', True):
                # è·å–é…ç½®æ–‡ä»¶åå’Œç ”ç©¶é¢†åŸŸå
                try:
                    hydra_cfg = HydraConfig.get()
                    actual_config_name = hydra_cfg.job.config_name
                except:
                    actual_config_name = "unknown"

                # è·å–ç ”ç©¶é¢†åŸŸåæˆ–ç”¨æˆ·å
                research_area = ""
                if hasattr(final_cfg, 'user_profile'):
                    research_area = final_cfg.user_profile.get('research_area', '')
                elif hasattr(final_cfg, 'defaults'):
                    research_area = final_cfg.defaults[0].keywords if hasattr(final_cfg.defaults[0], 'keywords') else ''

                output_format = output_cfg.get('format', 'txt')

                if output_format == 'markdown':
                    displayer.save_papers_report_markdown(
                        ranked_papers,
                        field_name,
                        search_cfg.get('days', 7),
                        include_scores=output_cfg.get('include_scores', True),
                        config_name=actual_config_name,
                        research_area=research_area,
                    )
                else:
                    displayer.save_papers_report(
                        ranked_papers,
                        field_name,
                        search_cfg.get('days', 7),
                        include_scores=output_cfg.get('include_scores', True),
                        config_name=actual_config_name,
                        research_area=research_area,
                    )
        else:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ç›¸å…³è®ºæ–‡")
    else:
        # å¸¸è§„æ˜¾ç¤º
        displayer.display_papers_detailed(papers, search_cfg.get('max_display', 10))

        # åŒæ­¥åˆ°é£ä¹¦å¤šç»´è¡¨æ ¼ï¼ˆæ— å…³é”®è¯ç­›é€‰ï¼‰
        if FEISHU_AVAILABLE:
            sync_papers_to_feishu(papers, final_cfg)

        # PDFä¸‹è½½å¤„ç†ï¼ˆæ— å…³é”®è¯ç­›é€‰ï¼‰
        download_cfg = final_cfg.get('download', {})
        if download_cfg.get('enabled', False) and papers:
            print(f"\nğŸ“¥ å¼€å§‹ä¸‹è½½PDFæ–‡ä»¶...")
            download_stats = arxiv_api.batch_download_pdfs(
                papers[: download_cfg.get('max_downloads', 10)],
                max_downloads=download_cfg.get('max_downloads', 10),
                create_index=download_cfg.get('create_index', True),
            )

            print(
                f"ğŸ“Š ä¸‹è½½ç»Ÿè®¡: æˆåŠŸ {download_stats['downloaded']}, "
                f"è·³è¿‡ {download_stats['skipped']}, å¤±è´¥ {download_stats['failed']}"
            )

        if final_cfg.output.save:
            # è·å–é…ç½®æ–‡ä»¶åå’Œç ”ç©¶é¢†åŸŸå
            try:
                hydra_cfg = HydraConfig.get()
                actual_config_name = hydra_cfg.job.config_name
            except:
                actual_config_name = "unknown"

            # è·å–ç ”ç©¶é¢†åŸŸåæˆ–ç”¨æˆ·å
            research_area = ""
            if hasattr(final_cfg, 'user_profile'):
                research_area = final_cfg.user_profile.get('research_area', '')
            elif hasattr(final_cfg, 'defaults'):
                research_area = final_cfg.defaults[0].keywords if hasattr(final_cfg.defaults[0], 'keywords') else ''

            output_format = final_cfg.output.get('format', 'txt')

            if output_format == 'markdown':
                displayer.save_papers_report_markdown(
                    papers,
                    field_name,
                    final_cfg.search.days,
                    include_scores=False,
                    config_name=actual_config_name,
                    research_area=research_area,
                )
            else:
                displayer.save_papers_report(
                    papers,
                    field_name,
                    final_cfg.search.days,
                    include_scores=False,
                    config_name=actual_config_name,
                    research_area=research_area,
                )

    print(f"\nâœ… é‡‡é›†å®Œæˆï¼")


if __name__ == "__main__":
    main()
