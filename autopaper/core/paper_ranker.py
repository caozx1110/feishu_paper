import arxiv
import re
import json
import os
from datetime import datetime, timedelta
from typing import Dict, List, Any, Tuple, Optional
from collections import defaultdict
import math
import requests
from pathlib import Path
import hashlib

# ä¼˜å…ˆä½¿ç”¨ rapidfuzzï¼Œå¦‚æœä¸å¯ç”¨åˆ™å›é€€åˆ° difflib
try:
    from rapidfuzz import fuzz, process

    RAPIDFUZZ_AVAILABLE = True
except ImportError:
    import difflib

    RAPIDFUZZ_AVAILABLE = False
    print("âš ï¸  rapidfuzz ä¸å¯ç”¨ï¼Œä½¿ç”¨ difflib ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ")


class PaperRanker:
    """è®ºæ–‡æ™ºèƒ½æ’åºå’Œè¿‡æ»¤ç±»"""

    def __init__(self):
        """åˆå§‹åŒ–æ’åºå™¨"""
        # å…³é”®è¯æƒé‡åˆ†å±‚
        self.keyword_weights = {
            "core": 2.5,  # æ ¸å¿ƒå…³é”®è¯æƒé‡ï¼ˆğŸ¯æ ‡è®°çš„å…³é”®è¯ï¼‰
            "extended": 1.5,  # æ‰©å±•å…³é”®è¯æƒé‡ï¼ˆğŸ”§æ ‡è®°çš„å…³é”®è¯ï¼‰
            "related": 1.0,  # ç›¸å…³å…³é”®è¯æƒé‡
            "default": 1.0,  # é»˜è®¤æƒé‡
        }

        # åŒ¹é…ç¼“å­˜
        self._match_cache = {}
        self._max_cache_size = 1000

        # åŒä¹‰è¯è¯å…¸ - å¯ä»¥æ‰©å±•
        self.synonyms = {
            "robot": ["robotics", "robotic", "autonomous agent", "android", "humanoid"],
            "ai": ["artificial intelligence", "machine intelligence", "intelligent system"],
            "ml": ["machine learning", "statistical learning", "automated learning"],
            "dl": ["deep learning", "neural network", "neural net", "deep neural network"],
            "cv": ["computer vision", "visual perception", "image analysis", "visual recognition"],
            "nlp": ["natural language processing", "language processing", "text processing"],
            "llm": ["large language model", "language model", "generative model"],
            "vla": ["vision language action", "vision-language-action", "multimodal action"],
            "slam": ["simultaneous localization and mapping", "localization and mapping"],
            "rl": ["reinforcement learning", "reward learning", "policy learning"],
            "transformer": ["attention mechanism", "self-attention", "multi-head attention"],
        }

        # å¸¸è§ç¼©å†™æ‰©å±•
        self.abbreviations = {
            "ai": "artificial intelligence",
            "ml": "machine learning",
            "dl": "deep learning",
            "cv": "computer vision",
            "nlp": "natural language processing",
            "llm": "large language model",
            "vla": "vision language action",
            "slam": "simultaneous localization and mapping",
            "rl": "reinforcement learning",
            "gnn": "graph neural network",
            "cnn": "convolutional neural network",
            "rnn": "recurrent neural network",
            "lstm": "long short term memory",
            "bert": "bidirectional encoder representations from transformers",
            "gpt": "generative pre-trained transformer",
        }

        # æŠ€æœ¯é¢†åŸŸå…³é”®è¯æƒé‡
        self.domain_weights = {
            "cs.AI": 1.5,
            "cs.LG": 1.4,
            "cs.RO": 1.3,
            "cs.CV": 1.2,
            "cs.CL": 1.2,
        }

    def check_required_keywords(
        self, paper: Dict[str, Any], required_keywords_config: Dict[str, Any]
    ) -> Tuple[bool, List[str]]:
        """
        æ£€æŸ¥è®ºæ–‡æ˜¯å¦åŒ…å«å¿…é¡»çš„å…³é”®è¯ï¼ˆæ”¯æŒANDå’ŒORé€»è¾‘ç»„åˆï¼‰

        æ”¯æŒçš„æ ¼å¼:
        - "robot" - å•ä¸ªå…³é”®è¯
        - "A OR B" - Aæˆ–Bä»»é€‰å…¶ä¸€
        - ["mobile", "A OR B"] - å¿…é¡»åŒ…å«mobileä¸”å¿…é¡»åŒ…å«(Aæˆ–B)

        Args:
            paper: è®ºæ–‡ä¿¡æ¯å­—å…¸
            required_keywords_config: å¿…é¡»åŒ…å«å…³é”®è¯é…ç½®

        Returns:
            tuple: (æ˜¯å¦é€šè¿‡æ£€æŸ¥, å®é™…åŒ¹é…åˆ°çš„å…³é”®è¯åˆ—è¡¨)
        """
        if not required_keywords_config.get("enabled", False):
            return True, []

        required_keywords = required_keywords_config.get("keywords", [])
        if not required_keywords:
            return True, []

        # æå–è®ºæ–‡æ–‡æœ¬ä¿¡æ¯
        title = paper.get("title", "").lower()
        summary = paper.get("summary", "").lower()
        categories = paper.get("categories", [])
        categories_str = " ".join(categories).lower()
        authors = paper.get("authors_str", "").lower()

        # ç»„åˆæ‰€æœ‰æ–‡æœ¬ç”¨äºæœç´¢
        full_text = f"{title} {summary} {categories_str} {authors}"

        fuzzy_match = required_keywords_config.get("fuzzy_match", True)
        similarity_threshold = required_keywords_config.get("similarity_threshold", 0.8)

        all_matched_keywords = []

        # æ£€æŸ¥æ‰€æœ‰å…³é”®è¯é¡¹æ˜¯å¦éƒ½åŒ¹é…ï¼ˆANDé€»è¾‘ï¼šå…¨éƒ¨å¿…é¡»åŒ¹é…ï¼‰
        for keyword_item in required_keywords:
            matched_keywords = self._check_keyword_item_detailed(
                keyword_item, full_text, fuzzy_match, similarity_threshold
            )
            if matched_keywords:
                # æ·»åŠ å®é™…åŒ¹é…åˆ°çš„å…³é”®è¯
                all_matched_keywords.extend(matched_keywords)
            else:
                # å¦‚æœæœ‰ä»»ä½•ä¸€ä¸ªå…³é”®è¯é¡¹ä¸åŒ¹é…ï¼Œåˆ™ç›´æ¥è¿”å›False
                return False, []

        # æ‰€æœ‰å…³é”®è¯é¡¹éƒ½åŒ¹é…æ‰é€šè¿‡æ£€æŸ¥
        return True, all_matched_keywords

    def _check_keyword_item_detailed(
        self, keyword_item: str, full_text: str, fuzzy_match: bool, similarity_threshold: float
    ) -> List[str]:
        """
        æ£€æŸ¥å•ä¸ªå…³é”®è¯é¡¹å¹¶è¿”å›å…·ä½“åŒ¹é…çš„å…³é”®è¯

        Args:
            keyword_item: å…³é”®è¯é¡¹ï¼Œå¯èƒ½æ˜¯å•ä¸ªå…³é”®è¯æˆ–åŒ…å«ORçš„ç»„åˆ
            full_text: è®ºæ–‡å…¨æ–‡
            fuzzy_match: æ˜¯å¦å¯ç”¨æ¨¡ç³ŠåŒ¹é…
            similarity_threshold: æ¨¡ç³ŠåŒ¹é…é˜ˆå€¼

        Returns:
            List[str]: åŒ¹é…åˆ°çš„å…·ä½“å…³é”®è¯åˆ—è¡¨
        """
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ORé€»è¾‘
        if " or " in keyword_item.lower() or " OR " in keyword_item:
            return self._check_or_keyword_detailed(keyword_item, full_text, fuzzy_match, similarity_threshold)
        else:
            # å•ä¸ªå…³é”®è¯
            if self._check_single_keyword(keyword_item, full_text, fuzzy_match, similarity_threshold):
                return [keyword_item]
            else:
                return []

    def _check_or_keyword_detailed(
        self, or_keyword: str, full_text: str, fuzzy_match: bool, similarity_threshold: float
    ) -> List[str]:
        """
        æ£€æŸ¥ORé€»è¾‘å…³é”®è¯å¹¶è¿”å›æ‰€æœ‰åŒ¹é…çš„å…³é”®è¯

        Args:
            or_keyword: åŒ…å«ORçš„å…³é”®è¯å­—ç¬¦ä¸²ï¼Œå¦‚ "A OR B OR C"
            full_text: è®ºæ–‡å…¨æ–‡
            fuzzy_match: æ˜¯å¦å¯ç”¨æ¨¡ç³ŠåŒ¹é…
            similarity_threshold: æ¨¡ç³ŠåŒ¹é…é˜ˆå€¼

        Returns:
            List[str]: åŒ¹é…åˆ°çš„å…·ä½“å…³é”®è¯åˆ—è¡¨
        """
        # åˆ†å‰²ORå…³é”®è¯
        or_parts = []
        if " OR " in or_keyword:
            or_parts = [part.strip() for part in or_keyword.split(" OR ")]
        elif " or " in or_keyword:
            or_parts = [part.strip() for part in or_keyword.split(" or ")]

        if len(or_parts) < 2:
            return []

        matched_keywords = []
        # æ£€æŸ¥æ‰€æœ‰éƒ¨åˆ†ï¼Œæ”¶é›†æ‰€æœ‰åŒ¹é…çš„å…³é”®è¯
        for part in or_parts:
            if self._check_single_keyword(part, full_text, fuzzy_match, similarity_threshold):
                matched_keywords.append(part)

        return matched_keywords

    def _check_single_keyword(
        self, keyword: str, full_text: str, fuzzy_match: bool, similarity_threshold: float
    ) -> bool:
        """
        æ£€æŸ¥å•ä¸ªå…³é”®è¯æ˜¯å¦åŒ¹é…

        Args:
            keyword: å…³é”®è¯
            full_text: è®ºæ–‡å…¨æ–‡
            fuzzy_match: æ˜¯å¦å¯ç”¨æ¨¡ç³ŠåŒ¹é…
            similarity_threshold: æ¨¡ç³ŠåŒ¹é…é˜ˆå€¼

        Returns:
            bool: æ˜¯å¦åŒ¹é…
        """
        keyword_lower = keyword.lower()

        # ç²¾ç¡®åŒ¹é…
        if keyword_lower in full_text:
            return True

        # æ¨¡ç³ŠåŒ¹é…ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if fuzzy_match:
            # æ£€æŸ¥å…³é”®è¯å˜ä½“
            keyword_variants = self._generate_keyword_variants(keyword)

            for variant in keyword_variants:
                if variant.lower() in full_text:
                    return True

            # ä½¿ç”¨å­—ç¬¦ä¸²ç›¸ä¼¼åº¦åŒ¹é…
            if self._fuzzy_match_required_keyword(keyword_lower, full_text, similarity_threshold):
                return True

        return False

    def _generate_keyword_variants(self, keyword: str) -> List[str]:
        """
        ç”Ÿæˆå…³é”®è¯å˜ä½“

        Args:
            keyword: åŸå§‹å…³é”®è¯

        Returns:
            å…³é”®è¯å˜ä½“åˆ—è¡¨
        """
        variants = [keyword]
        keyword_lower = keyword.lower()

        # ä»åŒä¹‰è¯è¯å…¸è·å–å˜ä½“
        for syn_key, syn_list in self.synonyms.items():
            if syn_key in keyword_lower or keyword_lower in syn_key:
                variants.extend(syn_list)

        # ç”Ÿæˆå¸¸è§å˜ä½“
        # å¤æ•°å½¢å¼
        if not keyword.endswith("s"):
            variants.append(keyword + "s")
        if keyword.endswith("y"):
            variants.append(keyword[:-1] + "ies")

        # å½¢å®¹è¯å½¢å¼
        if keyword.endswith("e"):
            variants.append(keyword[:-1] + "ic")
        else:
            variants.append(keyword + "ic")

        # è¿å­—ç¬¦å’Œç©ºæ ¼å˜ä½“
        if " " in keyword:
            variants.append(keyword.replace(" ", "-"))
            variants.append(keyword.replace(" ", "_"))
            variants.append(keyword.replace(" ", ""))
        if "-" in keyword:
            variants.append(keyword.replace("-", " "))
            variants.append(keyword.replace("-", "_"))
            variants.append(keyword.replace("-", ""))

        # å»é™¤é‡å¤å¹¶è¿”å›
        return list(set(variants))

    def _fuzzy_match_required_keyword(self, keyword: str, text: str, threshold: float) -> bool:
        """
        å¯¹å¿…é¡»å…³é”®è¯è¿›è¡Œæ¨¡ç³ŠåŒ¹é…

        Args:
            keyword: å…³é”®è¯
            text: å¾…åŒ¹é…æ–‡æœ¬
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼

        Returns:
            æ˜¯å¦åŒ¹é…
        """
        try:
            from difflib import SequenceMatcher

            # åˆ†è¯å¤„ç†
            words = text.split()

            # æ£€æŸ¥ä¸å•ä¸ªè¯çš„ç›¸ä¼¼åº¦
            for word in words:
                if len(word) >= 3:  # åªæ£€æŸ¥é•¿åº¦å¤§äºç­‰äº3çš„è¯
                    similarity = SequenceMatcher(None, keyword, word).ratio()
                    if similarity >= threshold:
                        return True

            # æ£€æŸ¥ä¸è¯ç»„çš„ç›¸ä¼¼åº¦
            keyword_words = keyword.split()
            if len(keyword_words) > 1:
                for i in range(len(words) - len(keyword_words) + 1):
                    phrase = " ".join(words[i : i + len(keyword_words)])
                    similarity = SequenceMatcher(None, keyword, phrase).ratio()
                    if similarity >= threshold:
                        return True

            return False

        except ImportError:
            # å¦‚æœæ²¡æœ‰difflibï¼Œä½¿ç”¨ç®€å•çš„åŒ…å«æ£€æŸ¥
            return keyword in text

    def calculate_relevance_score(
        self,
        paper: Dict[str, Any],
        interest_keywords: List[str] = None,
        exclude_keywords: List[str] = None,
        raw_interest_keywords: List[str] = None,
    ) -> Tuple[float, bool, List[str], List[str]]:
        """
        è®¡ç®—è®ºæ–‡ä¸å…³æ³¨è¯æ¡çš„ç›¸å…³æ€§è¯„åˆ† (å¢å¼ºç‰ˆ)

        æ”¯æŒé€šé…ç¬¦åŒ¹é…ï¼š
        - "*" æˆ– ["*"] åŒ¹é…æ‰€æœ‰æ–‡ç« 
        - ä»¥ "regex:" å¼€å¤´çš„å…³é”®è¯ä¼šè¢«å½“ä½œæ­£åˆ™è¡¨è¾¾å¼å¤„ç†
        - æ”¯æŒåˆ†å±‚æƒé‡ç³»ç»Ÿï¼šğŸ¯æ ¸å¿ƒæ¦‚å¿µã€ğŸ”§æ‰©å±•æ¦‚å¿µ

        Args:
            paper: è®ºæ–‡ä¿¡æ¯å­—å…¸
            interest_keywords: å…³æ³¨è¯æ¡åˆ—è¡¨ï¼ˆå·²è¿‡æ»¤çš„ï¼‰ï¼Œè¶Šå‰é¢æƒé‡è¶Šé«˜
            exclude_keywords: æ’é™¤è¯æ¡åˆ—è¡¨
            raw_interest_keywords: åŸå§‹å…³é”®è¯åˆ—è¡¨ï¼ˆåŒ…å«æ³¨é‡Šè¡Œï¼Œç”¨äºæƒé‡åˆ†æï¼‰

        Returns:
            tuple: (relevance_score, is_excluded, matched_interests, matched_excludes)
        """
        if not interest_keywords:
            return 0.0, False, [], []

        # è§£æåˆ†å±‚æƒé‡ï¼ˆå¦‚æœæä¾›äº†åŸå§‹å…³é”®è¯åˆ—è¡¨ï¼‰
        keyword_categories = {}
        if raw_interest_keywords:
            keyword_categories = self._parse_keyword_weights(raw_interest_keywords)

        # æ£€æŸ¥é€šé…ç¬¦åŒ¹é…ï¼ˆåŒ¹é…æ‰€æœ‰æ–‡ç« ï¼‰
        if self._is_wildcard_match(interest_keywords):
            return 1.0, False, ["*"], []

        # æå–è®ºæ–‡æ–‡æœ¬ä¿¡æ¯
        title = paper.get("title", "").lower()
        summary = paper.get("summary", "").lower()
        categories = paper.get("categories", [])
        categories_str = " ".join(categories).lower()
        authors = paper.get("authors_str", "").lower()
        paper_date = paper.get("published_date", datetime.now())

        # ç»„åˆæ‰€æœ‰æ–‡æœ¬ç”¨äºæœç´¢
        full_text = f"{title} {summary} {categories_str} {authors}"

        # æ‰©å±•å…³é”®è¯
        expanded_interests = self._expand_keywords(interest_keywords)
        expanded_excludes = self._expand_keywords(exclude_keywords) if exclude_keywords else []

        # æ£€æŸ¥æ’é™¤è¯æ¡ (ä½¿ç”¨æ‰©å±•åçš„è¯æ¡)
        excluded = False
        matched_excludes = []
        if expanded_excludes:
            for exclude_term in expanded_excludes:
                if exclude_term.lower() in full_text:
                    excluded = True
                    matched_excludes.append(exclude_term)

                # æ¨¡ç³ŠåŒ¹é…æ£€æŸ¥
                fuzzy_score = self._fuzzy_match_score(exclude_term, full_text, threshold=0.9)
                if fuzzy_score > 0:
                    excluded = True
                    matched_excludes.append(f"{exclude_term}(æ¨¡ç³ŠåŒ¹é…)")

        # å¦‚æœè¢«æ’é™¤ï¼Œç›´æ¥è¿”å›
        if excluded:
            return -999.0, True, [], matched_excludes

        # è®¡ç®—å…³æ³¨è¯æ¡åŒ¹é…å¾—åˆ† (å¢å¼ºç‰ˆ)
        relevance_score = 0.0
        matched_interests = []

        # æ—¶é—´è¡°å‡æƒé‡
        time_weight = self._calculate_time_decay(paper_date)

        # é¢†åŸŸç›¸å…³æ€§æƒé‡
        domain_weight = self._calculate_domain_relevance(categories)

        # å…±ç°æ£€æµ‹
        cooccurrence_bonus = self._detect_cooccurrence(expanded_interests, full_text)

        for i, keyword in enumerate(interest_keywords):
            keyword_lower = keyword.lower()

            # åŸºç¡€æƒé‡ï¼šè¶Šé å‰çš„è¯æ¡æƒé‡è¶Šé«˜
            base_weight = len(interest_keywords) - i

            # å¢å¼ºçš„å…³é”®è¯åŒ¹é…ï¼ˆæ”¯æŒæ­£åˆ™è¡¨è¾¾å¼ï¼‰
            enhanced_matched, enhanced_score = self._enhance_keyword_matching(keyword, full_text)

            if enhanced_matched:
                matched_interests.append(keyword)

                # è·å–åˆ†å±‚æƒé‡
                tier_weight = self._get_keyword_weight(keyword, keyword_categories)

                final_score = (
                    enhanced_score * base_weight * tier_weight * time_weight * domain_weight * cooccurrence_bonus
                )
                relevance_score += final_score
                continue  # å¦‚æœå¢å¼ºåŒ¹é…æˆåŠŸï¼Œè·³è¿‡åç»­å¤„ç†

            # æ£€æŸ¥åŸå…³é”®è¯å’Œæ‰©å±•å…³é”®è¯
            all_variants = [keyword_lower] + [syn.lower() for syn in self.synonyms.get(keyword_lower, [])]

            keyword_score = 0.0
            keyword_matched = False

            for variant in all_variants:
                # ç²¾ç¡®åŒ¹é…
                position_weights = self._calculate_position_weight(variant, title, summary)
                exact_score = sum(position_weights.values())

                if exact_score > 0:
                    keyword_matched = True
                    keyword_score += exact_score

                # æ¨¡ç³ŠåŒ¹é…
                fuzzy_title_score = self._fuzzy_match_score(variant, title, threshold=0.8)
                fuzzy_summary_score = self._fuzzy_match_score(variant, summary, threshold=0.8)

                if fuzzy_title_score > 0:
                    keyword_matched = True
                    keyword_score += fuzzy_title_score * 2.0  # æ ‡é¢˜æ¨¡ç³ŠåŒ¹é…æƒé‡

                if fuzzy_summary_score > 0:
                    keyword_matched = True
                    keyword_score += fuzzy_summary_score * 1.0  # æ‘˜è¦æ¨¡ç³ŠåŒ¹é…æƒé‡

                # åˆ†ç±»åŒ¹é…
                category_matches = len(re.findall(r"\b" + re.escape(variant) + r"\b", categories_str))
                if category_matches > 0:
                    keyword_matched = True
                    keyword_score += category_matches * 1.5

            if keyword_matched:
                matched_interests.append(keyword)

                # è·å–åˆ†å±‚æƒé‡
                tier_weight = self._get_keyword_weight(keyword, keyword_categories)

                # ç»¼åˆè®¡ç®—æœ€ç»ˆåˆ†æ•°ï¼ˆæ·»åŠ åˆ†å±‚æƒé‡ï¼‰
                final_score = (
                    keyword_score * base_weight * tier_weight * time_weight * domain_weight * cooccurrence_bonus
                )
                relevance_score += final_score

        return relevance_score, excluded, matched_interests, matched_excludes

    def filter_and_rank_papers(
        self,
        papers: List[Dict[str, Any]],
        interest_keywords: List[str] = None,
        exclude_keywords: List[str] = None,
        min_score: float = 0.1,
        use_advanced_scoring: bool = False,
        score_weights: Dict[str, float] = None,
        raw_interest_keywords: List[str] = None,
        required_keywords_config: Dict[str, Any] = None,
    ) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]], Dict[str, Any]]:
        """
        æ ¹æ®å…³æ³¨è¯æ¡è¿‡æ»¤å’Œæ’åºè®ºæ–‡ (æ”¯æŒé«˜çº§è¯„åˆ†å’Œå¿…é¡»å…³é”®è¯)

        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            interest_keywords: å…³æ³¨è¯æ¡åˆ—è¡¨ (æŒ‰é‡è¦æ€§æ’åº)
            exclude_keywords: æ’é™¤è¯æ¡åˆ—è¡¨
            min_score: æœ€å°ç›¸å…³æ€§åˆ†æ•°é˜ˆå€¼
            use_advanced_scoring: æ˜¯å¦ä½¿ç”¨é«˜çº§æ™ºèƒ½è¯„åˆ†
            score_weights: è¯„åˆ†æƒé‡é…ç½®
            required_keywords_config: å¿…é¡»åŒ…å«å…³é”®è¯é…ç½®

        Returns:
            tuple: (ranked_papers, excluded_papers, score_stats)
        """
        if not papers:
            return [], [], {}

        # é»˜è®¤è¯„åˆ†æƒé‡
        if score_weights is None:
            score_weights = {"base": 1.0, "semantic": 0.3, "author": 0.2, "novelty": 0.4, "citation": 0.3}

        # è®¡ç®—æ¯ç¯‡è®ºæ–‡çš„ç›¸å…³æ€§åˆ†æ•°
        scored_papers = []
        excluded_papers = []

        for paper in papers:
            # é¦–å…ˆæ£€æŸ¥å¿…é¡»åŒ…å«å…³é”®è¯
            if required_keywords_config:
                required_passed, required_matches = self.check_required_keywords(paper, required_keywords_config)
                if not required_passed:
                    paper["exclude_reason"] = "æœªåŒ…å«å¿…é¡»å…³é”®è¯"
                    excluded_papers.append(paper)
                    continue
                else:
                    paper["required_keyword_matches"] = required_matches

            # å¦‚æœæ²¡æœ‰å…³æ³¨è¯æ¡ï¼Œåªè¿›è¡Œæ’é™¤è¿‡æ»¤
            if not interest_keywords:
                if exclude_keywords:
                    _, is_excluded, _, _ = self.calculate_relevance_score(
                        paper, [], exclude_keywords, raw_interest_keywords
                    )
                    if is_excluded:
                        excluded_papers.append(paper)
                    else:
                        scored_papers.append(paper)
                else:
                    scored_papers.append(paper)
                continue

            if use_advanced_scoring:
                # ä½¿ç”¨é«˜çº§è¯„åˆ†
                total_score, is_excluded, matched_interests, matched_excludes, score_breakdown = (
                    self.calculate_advanced_relevance_score(
                        paper, interest_keywords, exclude_keywords, True, True, raw_interest_keywords
                    )
                )

                # åº”ç”¨æƒé‡
                final_score = (
                    score_breakdown.get("base_score", 0) * score_weights["base"]
                    + score_breakdown.get("semantic_boost", 0) * score_weights["semantic"]
                    + score_breakdown.get("author_boost", 0) * score_weights["author"]
                    + score_breakdown.get("novelty_boost", 0) * score_weights["novelty"]
                    + score_breakdown.get("citation_potential", 0) * score_weights["citation"]
                )

                paper["score_breakdown"] = score_breakdown
                paper["final_score"] = final_score

            else:
                # ä½¿ç”¨åŸºç¡€è¯„åˆ†
                final_score, is_excluded, matched_interests, matched_excludes = self.calculate_relevance_score(
                    paper, interest_keywords, exclude_keywords, raw_interest_keywords
                )

            if is_excluded:
                paper["exclude_reason"] = matched_excludes
                excluded_papers.append(paper)
            else:
                paper["relevance_score"] = final_score
                paper["matched_interests"] = matched_interests
                paper["interest_match_count"] = len(matched_interests)

                if final_score >= min_score:
                    scored_papers.append(paper)

        # æŒ‰ç›¸å…³æ€§åˆ†æ•°é™åºæ’åº
        sort_key = "final_score" if use_advanced_scoring else "relevance_score"
        ranked_papers = sorted(scored_papers, key=lambda x: x.get(sort_key, 0), reverse=True)

        # ç»Ÿè®¡ä¿¡æ¯
        scores = [p.get(sort_key, 0) for p in ranked_papers]
        required_filtered = len([p for p in excluded_papers if p.get("exclude_reason") == "æœªåŒ…å«å¿…é¡»å…³é”®è¯"])

        score_stats = {
            "total_papers": len(papers),
            "ranked_papers": len(ranked_papers),
            "excluded_papers": len(excluded_papers),
            "required_filtered": required_filtered,
            "max_score": max(scores) if scores else 0,
            "min_score": min(scores) if scores else 0,
            "avg_score": sum(scores) / len(scores) if scores else 0,
            "use_advanced_scoring": use_advanced_scoring,
        }

        return ranked_papers, excluded_papers, score_stats

    def get_field_papers(self, papers: List[Dict[str, Any]], field_type: str) -> List[Dict[str, Any]]:
        """æ ¹æ®é¢†åŸŸç±»å‹è¿‡æ»¤è®ºæ–‡"""
        field_keywords = {
            "ai": {
                "keywords": [
                    "artificial intelligence",
                    "AI",
                    "machine intelligence",
                    "deep learning",
                    "neural network",
                ],
                "categories": ["cs.AI", "cs.LG", "stat.ML"],
            },
            "robotics": {
                "keywords": [
                    "robot",
                    "robotics",
                    "robotic",
                    "autonomous",
                    "navigation",
                    "manipulation",
                    "SLAM",
                    "motion planning",
                    "path planning",
                    "humanoid",
                    "quadruped",
                    "mobile robot",
                ],
                "categories": ["cs.RO"],
            },
            "cv": {
                "keywords": [
                    "computer vision",
                    "image processing",
                    "visual",
                    "object detection",
                    "image recognition",
                    "video analysis",
                ],
                "categories": ["cs.CV", "eess.IV"],
            },
            "nlp": {
                "keywords": [
                    "natural language",
                    "NLP",
                    "language model",
                    "text processing",
                    "machine translation",
                    "sentiment analysis",
                ],
                "categories": ["cs.CL"],
            },
        }

        if field_type not in field_keywords:
            return papers

        field_config = field_keywords[field_type]
        filtered_papers = []

        for paper in papers:
            # æ£€æŸ¥åˆ†ç±»åŒ¹é…
            if any(cat in paper.get("categories", []) for cat in field_config["categories"]):
                filtered_papers.append(paper)
                continue

            # æ£€æŸ¥å…³é”®è¯åŒ¹é…
            title_lower = paper["title"].lower()
            summary_lower = paper["summary"].lower()

            for keyword in field_config["keywords"]:
                if keyword.lower() in title_lower or keyword.lower() in summary_lower:
                    filtered_papers.append(paper)
                    break

        return filtered_papers

    def _expand_keywords(self, keywords: List[str]) -> List[str]:
        """æ‰©å±•å…³é”®è¯åˆ—è¡¨ï¼ŒåŒ…å«åŒä¹‰è¯å’Œç¼©å†™"""
        expanded = set(keywords)

        for keyword in keywords:
            keyword_lower = keyword.lower()

            # æ·»åŠ ç¼©å†™æ‰©å±•
            if keyword_lower in self.abbreviations:
                expanded.add(self.abbreviations[keyword_lower])

            # æ·»åŠ åŒä¹‰è¯
            if keyword_lower in self.synonyms:
                expanded.update(self.synonyms[keyword_lower])

            # åå‘æŸ¥æ‰¾ - å¦‚æœè¾“å…¥çš„æ˜¯å…¨ç§°ï¼Œä¹Ÿè¦åŒ…å«ç¼©å†™
            for abbr, full_term in self.abbreviations.items():
                if keyword_lower == full_term:
                    expanded.add(abbr)

        return list(expanded)

    def _fuzzy_match_score(self, keyword: str, text: str, threshold: float = 0.8) -> float:
        """è®¡ç®—æ¨¡ç³ŠåŒ¹é…åˆ†æ•° - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œä½¿ç”¨ rapidfuzz"""
        if RAPIDFUZZ_AVAILABLE:
            return self._rapidfuzz_match_score(keyword, text, threshold)
        else:
            return self._fallback_fuzzy_match(keyword, text, threshold)

    def _rapidfuzz_match_score(self, keyword: str, text: str, threshold: float = 0.8) -> float:
        """ä½¿ç”¨ rapidfuzz è¿›è¡Œå¿«é€Ÿæ¨¡ç³ŠåŒ¹é…"""
        keyword_lower = keyword.lower()

        # å¿«é€Ÿæ£€æŸ¥ç²¾ç¡®åŒ¹é…
        if keyword_lower in text.lower():
            return 1.0

        # åˆ†è¯å¹¶é™åˆ¶æ£€æŸ¥èŒƒå›´ä»¥æé«˜æ•ˆç‡
        words = re.findall(r"\b\w+\b", text.lower())
        if not words:
            return 0.0

        # é™åˆ¶æ£€æŸ¥çš„è¯æ•°ä»¥æé«˜æ•ˆç‡ï¼ˆrapidfuzz å¾ˆå¿«ï¼Œå¯ä»¥æ£€æŸ¥æ›´å¤šè¯ï¼‰
        check_words = words[:100] if len(words) > 100 else words

        # ä½¿ç”¨ rapidfuzz è¿›è¡Œå¿«é€Ÿæ¨¡ç³ŠåŒ¹é…
        best_match = process.extractOne(keyword_lower, check_words, scorer=fuzz.ratio, score_cutoff=threshold * 100)

        return best_match[1] / 100.0 if best_match else 0.0

    def _fallback_fuzzy_match(self, keyword: str, text: str, threshold: float = 0.8) -> float:
        """å¤‡ç”¨æ¨¡ç³ŠåŒ¹é…æ–¹æ³•ï¼ˆrapidfuzz ä¸å¯ç”¨æ—¶ä½¿ç”¨ difflibï¼‰"""
        if not RAPIDFUZZ_AVAILABLE:
            import difflib

        keyword_lower = keyword.lower()

        # å¿«é€Ÿæ£€æŸ¥ç²¾ç¡®åŒ¹é…
        if keyword_lower in text.lower():
            return 1.0

        words = re.findall(r"\b\w+\b", text.lower())
        if not words:
            return 0.0

        max_similarity = 0.0
        # é™åˆ¶æ£€æŸ¥çš„è¯æ•°ä»¥æé«˜æ•ˆç‡ï¼ˆdifflib è¾ƒæ…¢ï¼Œå‡å°‘æ£€æŸ¥è¯æ•°ï¼‰
        check_words = words[:30] if len(words) > 30 else words

        for word in check_words:
            # è·³è¿‡è¿‡çŸ­çš„è¯
            if len(word) < 3:
                continue

            if RAPIDFUZZ_AVAILABLE:
                # å¦‚æœ rapidfuzz å¯ç”¨ï¼Œä½¿ç”¨å®ƒ
                similarity = fuzz.ratio(keyword_lower, word) / 100.0
            else:
                # å¦åˆ™ä½¿ç”¨ difflib
                similarity = difflib.SequenceMatcher(None, keyword_lower, word).ratio()

            if similarity >= threshold:
                max_similarity = max(max_similarity, similarity)
                # å¦‚æœæ‰¾åˆ°éå¸¸å¥½çš„åŒ¹é…ï¼Œæå‰ç»ˆæ­¢
                if similarity > 0.95:
                    break

        return max_similarity

    def _calculate_time_decay(self, paper_date: datetime, decay_days: int = 30) -> float:
        """è®¡ç®—æ—¶é—´è¡°å‡æƒé‡ - è¾ƒæ–°çš„è®ºæ–‡æƒé‡æ›´é«˜"""
        # å¤„ç†æ—¶åŒºé—®é¢˜
        if paper_date.tzinfo is not None:
            paper_date = paper_date.replace(tzinfo=None)

        days_ago = (datetime.now() - paper_date).days
        if days_ago <= 0:
            return 1.0
        elif days_ago <= decay_days:
            # çº¿æ€§è¡°å‡
            return 1.0 - (days_ago / decay_days) * 0.3  # æœ€å¤šè¡°å‡30%
        else:
            return 0.7  # æœ€å°æƒé‡

    def _calculate_domain_relevance(self, categories: List[str]) -> float:
        """æ ¹æ®è®ºæ–‡åˆ†ç±»è®¡ç®—é¢†åŸŸç›¸å…³æ€§æƒé‡"""
        max_weight = 1.0
        for category in categories:
            if category in self.domain_weights:
                max_weight = max(max_weight, self.domain_weights[category])
        return max_weight

    def _detect_cooccurrence(self, keywords: List[str], text: str) -> float:
        """æ£€æµ‹å…³é”®è¯å…±ç°ï¼Œæå‡ç›¸å…³æ€§"""
        text_lower = text.lower()
        found_keywords = []

        for keyword in keywords:
            if keyword.lower() in text_lower:
                found_keywords.append(keyword)

        # å…±ç°å¥–åŠ± - åŒæ—¶å‡ºç°å¤šä¸ªå…³é”®è¯æ—¶ç»™äºˆé¢å¤–åˆ†æ•°
        cooccurrence_count = len(found_keywords)
        if cooccurrence_count >= 2:
            # å…±ç°å¥–åŠ±ç³»æ•°
            return 1.0 + (cooccurrence_count - 1) * 0.2
        return 1.0

    def _calculate_position_weight(self, keyword: str, title: str, summary: str) -> Dict[str, float]:
        """è®¡ç®—å…³é”®è¯åœ¨ä¸åŒä½ç½®çš„æƒé‡"""
        weights = {"title": 0.0, "summary_start": 0.0, "summary_mid": 0.0}

        keyword_lower = keyword.lower()
        title_lower = title.lower()
        summary_lower = summary.lower()

        # æ ‡é¢˜ä¸­çš„æƒé‡
        if keyword_lower in title_lower:
            title_words = title_lower.split()
            keyword_position = -1
            for i, word in enumerate(title_words):
                if keyword_lower in word:
                    keyword_position = i
                    break

            if keyword_position != -1:
                # æ ‡é¢˜å¼€å¤´çš„è¯æƒé‡æ›´é«˜
                position_factor = max(0.5, 1.0 - (keyword_position / len(title_words)) * 0.5)
                weights["title"] = 3.0 * position_factor

        # æ‘˜è¦ä¸­çš„æƒé‡ - åŒºåˆ†å‰åŠéƒ¨åˆ†å’ŒååŠéƒ¨åˆ†
        if keyword_lower in summary_lower:
            summary_length = len(summary_lower)
            keyword_pos = summary_lower.find(keyword_lower)

            if keyword_pos < summary_length * 0.3:  # å‰30%
                weights["summary_start"] = 2.5
            else:  # å…¶ä»–ä½ç½®
                weights["summary_mid"] = 1.5

        return weights

    def calculate_advanced_relevance_score(
        self,
        paper: Dict[str, Any],
        interest_keywords: List[str] = None,
        exclude_keywords: List[str] = None,
        use_semantic_boost: bool = True,
        use_author_analysis: bool = True,
        raw_interest_keywords: List[str] = None,
    ) -> Tuple[float, bool, List[str], List[str], Dict[str, Any]]:
        """
        é«˜çº§ç›¸å…³æ€§è¯„åˆ†è®¡ç®— (åŒ…å«æ›´å¤šæ™ºèƒ½ç‰¹æ€§)

        Args:
            paper: è®ºæ–‡ä¿¡æ¯å­—å…¸
            interest_keywords: å…³æ³¨è¯æ¡åˆ—è¡¨
            exclude_keywords: æ’é™¤è¯æ¡åˆ—è¡¨
            use_semantic_boost: æ˜¯å¦ä½¿ç”¨è¯­ä¹‰å¢å¼º
            use_author_analysis: æ˜¯å¦åˆ†æä½œè€…ä¿¡æ¯

        Returns:
            tuple: (relevance_score, is_excluded, matched_interests, matched_excludes, score_breakdown)
        """
        # åŸºç¡€è¯„åˆ†
        base_score, excluded, matched_interests, matched_excludes = self.calculate_relevance_score(
            paper, interest_keywords, exclude_keywords, raw_interest_keywords
        )

        if excluded:
            return base_score, excluded, matched_interests, matched_excludes, {}

        score_breakdown = {
            "base_score": base_score,
            "semantic_boost": 0.0,
            "author_boost": 0.0,
            "novelty_boost": 0.0,
            "citation_potential": 0.0,
        }

        total_score = base_score

        # è¯­ä¹‰å¢å¼ºåˆ†æ
        if use_semantic_boost and interest_keywords:
            semantic_boost = self._calculate_semantic_boost(paper, interest_keywords)
            score_breakdown["semantic_boost"] = semantic_boost
            total_score += semantic_boost

        # ä½œè€…åˆ†æå¢å¼º
        if use_author_analysis:
            author_boost = self._calculate_author_relevance(paper, interest_keywords)
            score_breakdown["author_boost"] = author_boost
            total_score += author_boost

        # æ–°é¢–æ€§åˆ†æ
        novelty_boost = self._calculate_novelty_score(paper)
        score_breakdown["novelty_boost"] = novelty_boost
        total_score += novelty_boost

        # å¼•ç”¨æ½œåŠ›é¢„æµ‹
        citation_potential = self._predict_citation_potential(paper)
        score_breakdown["citation_potential"] = citation_potential
        total_score += citation_potential

        return total_score, excluded, matched_interests, matched_excludes, score_breakdown

    def _calculate_semantic_boost(self, paper: Dict[str, Any], keywords: List[str]) -> float:
        """è®¡ç®—è¯­ä¹‰ç›¸å…³æ€§å¢å¼ºåˆ†æ•°"""
        title = paper.get("title", "").lower()
        summary = paper.get("summary", "").lower()

        # æŠ€æœ¯æœ¯è¯­å…±ç°åˆ†æ
        tech_terms = [
            "neural",
            "learning",
            "model",
            "algorithm",
            "method",
            "approach",
            "framework",
            "system",
            "network",
            "optimization",
            "training",
            "inference",
            "prediction",
            "classification",
            "regression",
        ]

        tech_term_count = sum(1 for term in tech_terms if term in title + " " + summary)

        # åŸºäºæŠ€æœ¯å¯†åº¦çš„è¯­ä¹‰å¢å¼º
        semantic_boost = min(tech_term_count * 0.1, 1.0)

        # å…³é”®è¯è¯­å¢ƒåˆ†æ
        context_boost = 0.0
        for keyword in keywords:
            keyword_lower = keyword.lower()

            # å¯»æ‰¾å…³é”®è¯é™„è¿‘çš„ç›¸å…³æœ¯è¯­
            for text in [title, summary]:
                sentences = re.split(r"[.!?]", text)
                for sentence in sentences:
                    if keyword_lower in sentence:
                        # åˆ†æå¥å­ä¸­çš„å…¶ä»–æŠ€æœ¯æœ¯è¯­
                        sentence_tech_terms = sum(1 for term in tech_terms if term in sentence)
                        context_boost += sentence_tech_terms * 0.05

        return semantic_boost + min(context_boost, 0.5)

    def _calculate_author_relevance(self, paper: Dict[str, Any], keywords: List[str]) -> float:
        """åŸºäºä½œè€…ä¿¡æ¯è®¡ç®—ç›¸å…³æ€§å¢å¼º"""
        authors = paper.get("authors", [])
        if not authors:
            return 0.0

        # ä½œè€…æ•°é‡å¯¹ç ”ç©¶è´¨é‡çš„å½±å“ (é€‚ä¸­çš„ä½œè€…æ•°é‡é€šå¸¸è¾ƒå¥½)
        author_count = len(authors)
        if 2 <= author_count <= 6:
            author_count_boost = 0.2
        elif author_count == 1:
            author_count_boost = 0.1  # å•ä½œè€…å¯èƒ½æ˜¯ç†è®ºæ€§å¼ºçš„å·¥ä½œ
        else:
            author_count_boost = 0.0  # ä½œè€…è¿‡å¤šå¯èƒ½è´¨é‡å‚å·®ä¸é½

        # åˆ†æä½œè€…å§“åä¸­çš„æœºæ„ä¿¡æ¯ (é€šè¿‡é‚®ç®±åŸŸåç­‰)
        institution_boost = 0.0
        # è¿™é‡Œå¯ä»¥æ‰©å±•ä¸ºä»ä½œè€…é™„åŠ ä¿¡æ¯ä¸­æå–æœºæ„ç›¸å…³æ€§

        return author_count_boost + institution_boost

    def _calculate_novelty_score(self, paper: Dict[str, Any]) -> float:
        """è®¡ç®—è®ºæ–‡æ–°é¢–æ€§åˆ†æ•°"""
        title = paper.get("title", "").lower()
        summary = paper.get("summary", "").lower()

        # æ–°é¢–æ€§æŒ‡ç¤ºè¯
        novelty_indicators = [
            "novel",
            "new",
            "first",
            "introduce",
            "propose",
            "present",
            "innovative",
            "breakthrough",
            "pioneer",
            "original",
            "unprecedented",
            "state-of-the-art",
            "sota",
            "outperform",
            "improve",
            "enhance",
            "advance",
            "superior",
            "better than",
        ]

        novelty_count = 0
        for indicator in novelty_indicators:
            novelty_count += len(re.findall(r"\b" + re.escape(indicator) + r"\b", title + " " + summary))

        # æ ‡é¢˜ä¸­çš„æ–°é¢–æ€§è¯æ±‡æƒé‡æ›´é«˜
        title_novelty = sum(1 for indicator in novelty_indicators if indicator in title)

        novelty_score = min((novelty_count * 0.1) + (title_novelty * 0.2), 1.0)
        return novelty_score

    def _predict_citation_potential(self, paper: Dict[str, Any]) -> float:
        """é¢„æµ‹è®ºæ–‡çš„å¼•ç”¨æ½œåŠ›"""
        title = paper.get("title", "").lower()
        summary = paper.get("summary", "").lower()
        categories = paper.get("categories", [])

        # é«˜å¼•ç”¨æ½œåŠ›æŒ‡æ ‡
        high_impact_terms = [
            "benchmark",
            "dataset",
            "survey",
            "review",
            "framework",
            "open source",
            "code available",
            "reproducible",
            "evaluation",
            "comparison",
            "analysis",
            "comprehensive",
            "extensive",
        ]

        impact_count = sum(1 for term in high_impact_terms if term in title + " " + summary)

        # çƒ­é—¨é¢†åŸŸåŠ æƒ
        hot_categories = ["cs.AI", "cs.LG", "cs.CV", "cs.CL", "cs.RO"]
        category_boost = 0.2 if any(cat in hot_categories for cat in categories) else 0.0

        # è®ºæ–‡é•¿åº¦é¢„æµ‹ (æ›´é•¿çš„æ‘˜è¦é€šå¸¸è¡¨ç¤ºæ›´å…¨é¢çš„å·¥ä½œ)
        length_boost = min(len(summary) / 1000, 0.3)  # æ‘˜è¦é•¿åº¦å½’ä¸€åŒ–

        citation_potential = min((impact_count * 0.15) + category_boost + length_boost, 1.0)
        return citation_potential

    def _is_wildcard_match(self, keywords: List[str]) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦ä¸ºé€šé…ç¬¦åŒ¹é…ï¼ˆåŒ¹é…æ‰€æœ‰æ–‡ç« ï¼‰
        """
        if not keywords:
            return False

        # æ£€æŸ¥æ˜¯å¦åŒ…å«é€šé…ç¬¦
        wildcard_patterns = ["*", "all", ".*", "å…¨éƒ¨", "æ‰€æœ‰"]
        for keyword in keywords:
            if keyword.lower().strip() in wildcard_patterns:
                return True

        # å¦‚æœåªæœ‰ä¸€ä¸ªå…³é”®è¯ä¸”ä¸ºç©ºæˆ–åªæœ‰ç©ºç™½å­—ç¬¦
        if len(keywords) == 1 and not keywords[0].strip():
            return True

        return False

    def _is_regex_keyword(self, keyword: str) -> bool:
        """
        æ£€æŸ¥å…³é”®è¯æ˜¯å¦ä¸ºæ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
        """
        return keyword.startswith("regex:") or keyword.startswith("re:")

    def _process_regex_keyword(self, keyword: str, text: str) -> bool:
        """
        å¤„ç†æ­£åˆ™è¡¨è¾¾å¼å…³é”®è¯åŒ¹é…
        """
        try:
            if keyword.startswith("regex:"):
                pattern = keyword[6:].strip()  # ç§»é™¤ "regex:" å‰ç¼€
            elif keyword.startswith("re:"):
                pattern = keyword[3:].strip()  # ç§»é™¤ "re:" å‰ç¼€
            else:
                return False

            # ç¼–è¯‘å¹¶åŒ¹é…æ­£åˆ™è¡¨è¾¾å¼
            regex = re.compile(pattern, re.IGNORECASE)
            return bool(regex.search(text))
        except re.error:
            # å¦‚æœæ­£åˆ™è¡¨è¾¾å¼æ— æ•ˆï¼Œå›é€€åˆ°æ™®é€šå­—ç¬¦ä¸²åŒ¹é…
            return keyword.lower() in text.lower()

    def _enhance_keyword_matching(self, keyword: str, text: str) -> Tuple[bool, float]:
        """
        å¢å¼ºçš„å…³é”®è¯åŒ¹é…ï¼Œæ”¯æŒæ­£åˆ™è¡¨è¾¾å¼å’Œé€šé…ç¬¦

        Returns:
            tuple: (æ˜¯å¦åŒ¹é…, åŒ¹é…åˆ†æ•°)
        """
        # æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…
        if self._is_regex_keyword(keyword):
            matched = self._process_regex_keyword(keyword, text)
            return matched, 1.0 if matched else 0.0

        # æ™®é€šå­—ç¬¦ä¸²åŒ¹é…
        if keyword.lower() in text.lower():
            return True, 1.0

        # æ¨¡ç³ŠåŒ¹é…
        fuzzy_score = self._fuzzy_match_score(keyword, text, threshold=0.8)
        if fuzzy_score > 0:
            return True, fuzzy_score

        return False, 0.0

    def _parse_keyword_weights(self, raw_keywords: List[str]) -> Dict[str, str]:
        """
        è§£æåŸå§‹å…³é”®è¯åˆ—è¡¨ï¼Œç¡®å®šæ¯ä¸ªå…³é”®è¯çš„æƒé‡ç±»åˆ«

        Args:
            raw_keywords: åŒ…å«æ³¨é‡Šè¡Œçš„åŸå§‹å…³é”®è¯åˆ—è¡¨

        Returns:
            å­—å…¸ï¼Œé”®ä¸ºå…³é”®è¯ï¼Œå€¼ä¸ºæƒé‡ç±»åˆ« ('core', 'extended', 'default')
        """
        keyword_categories = {}
        current_category = "default"

        for keyword in raw_keywords:
            keyword = keyword.strip()

            # è·³è¿‡ç©ºè¡Œ
            if not keyword:
                continue

            # æ£€æŸ¥æ³¨é‡Šè¡Œï¼Œç¡®å®šå½“å‰åˆ†ç±»
            if keyword.startswith("#"):
                if "ğŸ¯" in keyword or "æ ¸å¿ƒæ¦‚å¿µ" in keyword or "é«˜æƒé‡" in keyword:
                    current_category = "core"
                elif "ğŸ”§" in keyword or "æ‰©å±•æ¦‚å¿µ" in keyword or "ä¸­æƒé‡" in keyword:
                    current_category = "extended"
                elif "ğŸ“" in keyword or "ç›¸å…³æ¦‚å¿µ" in keyword or "æ ‡å‡†æƒé‡" in keyword:
                    current_category = "related"
                # æ³¨é‡Šè¡Œæœ¬èº«ä¸ä½œä¸ºå…³é”®è¯
                continue

            # ä¸ºéæ³¨é‡Šçš„å…³é”®è¯åˆ†é…ç±»åˆ«
            keyword_categories[keyword] = current_category

        return keyword_categories

    def _get_keyword_weight(self, keyword: str, keyword_categories: Dict[str, str]) -> float:
        """
        è·å–å…³é”®è¯çš„æƒé‡å€æ•°

        Args:
            keyword: å…³é”®è¯
            keyword_categories: å…³é”®è¯åˆ†ç±»å­—å…¸

        Returns:
            æƒé‡å€æ•°
        """
        category = keyword_categories.get(keyword, "default")
        return self.keyword_weights.get(category, self.keyword_weights["default"])
