#!/usr/bin/env python3
"""
ArXiv è®ºæ–‡åŒæ­¥ç³»ç»Ÿ - ç»Ÿä¸€å·¥å…·ç®¡ç†å™¨

è¿™ä¸ªè„šæœ¬æ•´åˆäº†æ‰€æœ‰ç®¡ç†åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- å®šæ—¶åŒæ­¥è„šæœ¬
- å¥åº·æ£€æŸ¥
- ç¯å¢ƒè®¾ç½®
- Docker ç®¡ç†
- ç³»ç»Ÿç»´æŠ¤

ä½¿ç”¨æ–¹æ³•:
    python tools/manager.py <command> [options]

æ”¯æŒçš„å‘½ä»¤:
    sync         - æ‰§è¡ŒåŒæ­¥
    schedule     - å®šæ—¶åŒæ­¥
    health       - å¥åº·æ£€æŸ¥
    setup        - ç¯å¢ƒè®¾ç½®
    docker       - Docker ç®¡ç†
    backup       - æ•°æ®å¤‡ä»½
    clean        - æ¸…ç†ç¯å¢ƒ
"""

import os
import sys
import argparse
import subprocess
import shutil
import tempfile
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Any
import yaml
import logging


class ArxivSyncManager:
    """ArXiv åŒæ­¥ç³»ç»Ÿç®¡ç†å™¨"""

    def __init__(self):
        self.project_root = Path(__file__).parent.parent
        self.setup_logging()

    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_dir = self.project_root / "logs"
        log_dir.mkdir(exist_ok=True)

        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.StreamHandler(sys.stdout),
                logging.FileHandler(log_dir / 'manager.log', encoding='utf-8'),
            ],
        )
        self.logger = logging.getLogger(__name__)

    def sync_all(self, dry_run: bool = False, days: int = None) -> bool:
        """æ‰§è¡Œæ‰¹é‡åŒæ­¥"""
        try:
            cmd = [sys.executable, str(self.project_root / "arxiv_hydra.py"), "--config-name=all"]

            self.logger.info("ğŸš€ å¼€å§‹æ‰§è¡Œæ‰¹é‡åŒæ­¥...")

            if dry_run:
                self.logger.info("ğŸ” æ¨¡æ‹Ÿè¿è¡Œæ¨¡å¼")
                return True

            result = subprocess.run(cmd, cwd=self.project_root, capture_output=False, text=True)

            if result.returncode == 0:
                self.logger.info("âœ… æ‰¹é‡åŒæ­¥æ‰§è¡ŒæˆåŠŸ")
                return True
            else:
                self.logger.error("âŒ æ‰¹é‡åŒæ­¥æ‰§è¡Œå¤±è´¥")
                return False

        except Exception as e:
            self.logger.error(f"âŒ åŒæ­¥è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e}")
            return False

    def schedule_sync(self, days: int = 1, dry_run: bool = False, backup: bool = True) -> bool:
        """å®šæ—¶åŒæ­¥ï¼ˆå¸¦é…ç½®ç®¡ç†ï¼‰"""
        try:
            from scripts.scheduled_sync import ScheduledSyncManager

            manager = ScheduledSyncManager(days=days, dry_run=dry_run, backup=backup)
            return manager.run()

        except ImportError:
            self.logger.warning("âš ï¸ scheduled_sync æ¨¡å—ä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€åŒ–åŒæ­¥")
            return self.sync_all(dry_run=dry_run)
        except Exception as e:
            self.logger.error(f"âŒ å®šæ—¶åŒæ­¥å¤±è´¥: {e}")
            return False

    def health_check(self) -> bool:
        """å¥åº·æ£€æŸ¥"""
        try:
            from scripts.health_check import main as health_main

            self.logger.info("ğŸ” å¼€å§‹å¥åº·æ£€æŸ¥...")
            health_main()
            return True

        except ImportError:
            return self._basic_health_check()
        except SystemExit as e:
            return e.code == 0
        except Exception as e:
            self.logger.error(f"âŒ å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return False

    def _basic_health_check(self) -> bool:
        """åŸºç¡€å¥åº·æ£€æŸ¥"""
        checks_passed = 0
        total_checks = 4

        # æ£€æŸ¥ Python ç¯å¢ƒ
        try:
            import yaml, requests, omegaconf

            self.logger.info("âœ… Python ç¯å¢ƒæ£€æŸ¥é€šè¿‡")
            checks_passed += 1
        except ImportError as e:
            self.logger.error(f"âŒ Python ä¾èµ–æ£€æŸ¥å¤±è´¥: {e}")

        # æ£€æŸ¥é…ç½®æ–‡ä»¶
        config_dir = self.project_root / "conf"
        if config_dir.exists() and list(config_dir.glob("sync*.yaml")):
            self.logger.info(f"âœ… æ‰¾åˆ°é…ç½®æ–‡ä»¶")
            checks_passed += 1
        else:
            self.logger.error("âŒ æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶")

        # æ£€æŸ¥ç¯å¢ƒå˜é‡
        required_vars = ['FEISHU_APP_ID', 'FEISHU_APP_SECRET', 'FEISHU_BITABLE_APP_TOKEN']
        missing_vars = [var for var in required_vars if not os.getenv(var)]

        if not missing_vars:
            self.logger.info("âœ… ç¯å¢ƒå˜é‡æ£€æŸ¥é€šè¿‡")
            checks_passed += 1
        else:
            self.logger.error(f"âŒ ç¼ºå°‘ç¯å¢ƒå˜é‡: {', '.join(missing_vars)}")

        # æ£€æŸ¥ç›®å½•ç»“æ„
        required_dirs = ['logs', 'backup', 'downloads']
        for dir_name in required_dirs:
            dir_path = self.project_root / dir_name
            dir_path.mkdir(exist_ok=True)

        self.logger.info("âœ… ç›®å½•ç»“æ„æ£€æŸ¥é€šè¿‡")
        checks_passed += 1

        success = checks_passed == total_checks
        if success:
            self.logger.info(f"ğŸ‰ å¥åº·æ£€æŸ¥é€šè¿‡ ({checks_passed}/{total_checks})")
        else:
            self.logger.error(f"âš ï¸ å¥åº·æ£€æŸ¥éƒ¨åˆ†å¤±è´¥ ({checks_passed}/{total_checks})")

        return success

    def setup_environment(self) -> bool:
        """ç¯å¢ƒè®¾ç½®"""
        try:
            self.logger.info("ğŸ”§ å¼€å§‹ç¯å¢ƒè®¾ç½®...")

            # åˆ›å»ºå¿…è¦ç›®å½•
            for dir_name in ['logs', 'backup', 'downloads']:
                dir_path = self.project_root / dir_name
                dir_path.mkdir(exist_ok=True)
                self.logger.info(f"ğŸ“ åˆ›å»ºç›®å½•: {dir_name}")

            # å¤åˆ¶ç¯å¢ƒå˜é‡æ¨¡æ¿
            env_template = self.project_root / ".env.template"
            env_file = self.project_root / ".env"

            if env_template.exists() and not env_file.exists():
                shutil.copy2(env_template, env_file)
                self.logger.info("ğŸ“„ å·²åˆ›å»º .env æ–‡ä»¶ï¼Œè¯·ç¼–è¾‘å…¶ä¸­çš„é…ç½®")
            elif env_file.exists():
                self.logger.info("ğŸ“„ .env æ–‡ä»¶å·²å­˜åœ¨")
            else:
                self.logger.warning("âš ï¸ æœªæ‰¾åˆ° .env.template æ–‡ä»¶")

            self.logger.info("âœ… ç¯å¢ƒè®¾ç½®å®Œæˆ")
            return True

        except Exception as e:
            self.logger.error(f"âŒ ç¯å¢ƒè®¾ç½®å¤±è´¥: {e}")
            return False

    def docker_manage(self, action: str) -> bool:
        """Docker ç®¡ç†"""
        docker_commands = {
            'up': ['docker-compose', 'up', '-d'],
            'down': ['docker-compose', 'down'],
            'build': ['docker-compose', 'build', '--no-cache'],
            'logs': ['docker-compose', 'logs', '-f'],
            'ps': ['docker-compose', 'ps'],
            'restart': ['docker-compose', 'restart'],
        }

        if action not in docker_commands:
            self.logger.error(f"âŒ ä¸æ”¯æŒçš„ Docker æ“ä½œ: {action}")
            return False

        try:
            cmd = docker_commands[action]
            self.logger.info(f"ğŸ³ æ‰§è¡Œ Docker å‘½ä»¤: {' '.join(cmd)}")

            result = subprocess.run(cmd, cwd=self.project_root, capture_output=(action != 'logs'))

            if result.returncode == 0:
                self.logger.info(f"âœ… Docker {action} æ‰§è¡ŒæˆåŠŸ")
                return True
            else:
                self.logger.error(f"âŒ Docker {action} æ‰§è¡Œå¤±è´¥")
                return False

        except FileNotFoundError:
            self.logger.error("âŒ æœªæ‰¾åˆ° docker-compose å‘½ä»¤")
            return False
        except Exception as e:
            self.logger.error(f"âŒ Docker æ“ä½œå¤±è´¥: {e}")
            return False

    def backup_data(self) -> bool:
        """å¤‡ä»½æ•°æ®"""
        try:
            backup_dir = self.project_root / "backups"
            backup_dir.mkdir(exist_ok=True)

            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            backup_file = backup_dir / f"arxiv_sync_backup_{timestamp}.tar.gz"

            # è¦å¤‡ä»½çš„ç›®å½•å’Œæ–‡ä»¶
            backup_items = ['logs', 'backup', 'downloads', '.env']
            existing_items = [item for item in backup_items if (self.project_root / item).exists()]

            if not existing_items:
                self.logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°éœ€è¦å¤‡ä»½çš„æ–‡ä»¶")
                return False

            # ä½¿ç”¨ tar å‘½ä»¤å¤‡ä»½ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            try:
                import tarfile

                with tarfile.open(backup_file, 'w:gz') as tar:
                    for item in existing_items:
                        item_path = self.project_root / item
                        tar.add(item_path, arcname=item)

                self.logger.info(f"âœ… å¤‡ä»½å®Œæˆ: {backup_file}")
                return True

            except Exception as e:
                self.logger.error(f"âŒ å¤‡ä»½å¤±è´¥: {e}")
                return False

        except Exception as e:
            self.logger.error(f"âŒ å¤‡ä»½è¿‡ç¨‹å¤±è´¥: {e}")
            return False

    def clean_environment(self, deep: bool = False) -> bool:
        """æ¸…ç†ç¯å¢ƒ"""
        try:
            self.logger.info("ğŸ§¹ å¼€å§‹æ¸…ç†ç¯å¢ƒ...")

            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            temp_patterns = ['*.pyc', '*.pyo', '*.pyd', '__pycache__']
            cleaned_count = 0

            for pattern in temp_patterns:
                for file_path in self.project_root.rglob(pattern):
                    try:
                        if file_path.is_file():
                            file_path.unlink()
                        elif file_path.is_dir():
                            shutil.rmtree(file_path)
                        cleaned_count += 1
                    except Exception:
                        pass

            # æ¸…ç†æ—§æ—¥å¿—æ–‡ä»¶ï¼ˆä¿ç•™æœ€è¿‘7å¤©ï¼‰
            logs_dir = self.project_root / "logs"
            if logs_dir.exists():
                cutoff_date = datetime.now() - timedelta(days=7)
                for log_file in logs_dir.glob("*.log"):
                    if log_file.stat().st_mtime < cutoff_date.timestamp():
                        try:
                            log_file.unlink()
                            cleaned_count += 1
                        except Exception:
                            pass

            if deep:
                # æ·±åº¦æ¸…ç†ï¼šåˆ é™¤æ‰€æœ‰ç”Ÿæˆçš„æ–‡ä»¶
                deep_clean_dirs = ['outputs', 'downloads', '__pycache__']
                for dir_name in deep_clean_dirs:
                    dir_path = self.project_root / dir_name
                    if dir_path.exists():
                        shutil.rmtree(dir_path)
                        cleaned_count += 1

            self.logger.info(f"âœ… æ¸…ç†å®Œæˆï¼Œæ¸…ç†äº† {cleaned_count} ä¸ªé¡¹ç›®")
            return True

        except Exception as e:
            self.logger.error(f"âŒ æ¸…ç†å¤±è´¥: {e}")
            return False


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='ArXiv è®ºæ–‡åŒæ­¥ç³»ç»Ÿç®¡ç†å·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  %(prog)s sync                    # æ‰§è¡Œæ‰¹é‡åŒæ­¥
  %(prog)s sync --dry-run          # æ¨¡æ‹ŸåŒæ­¥
  %(prog)s schedule --days 7       # 7å¤©å‘¨æœŸå®šæ—¶åŒæ­¥
  %(prog)s health                  # å¥åº·æ£€æŸ¥
  %(prog)s setup                   # ç¯å¢ƒè®¾ç½®
  %(prog)s docker up               # å¯åŠ¨ Docker æœåŠ¡
  %(prog)s backup                  # å¤‡ä»½æ•°æ®
  %(prog)s clean                   # æ¸…ç†ç¯å¢ƒ
        """,
    )

    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')

    # sync å‘½ä»¤
    sync_parser = subparsers.add_parser('sync', help='æ‰§è¡ŒåŒæ­¥')
    sync_parser.add_argument('--dry-run', action='store_true', help='æ¨¡æ‹Ÿè¿è¡Œ')

    # schedule å‘½ä»¤
    schedule_parser = subparsers.add_parser('schedule', help='å®šæ—¶åŒæ­¥')
    schedule_parser.add_argument('--days', type=int, default=1, help='åŒæ­¥å‘¨æœŸå¤©æ•°')
    schedule_parser.add_argument('--dry-run', action='store_true', help='æ¨¡æ‹Ÿè¿è¡Œ')
    schedule_parser.add_argument('--no-backup', action='store_true', help='ä¸å¤‡ä»½é…ç½®')

    # health å‘½ä»¤
    subparsers.add_parser('health', help='å¥åº·æ£€æŸ¥')

    # setup å‘½ä»¤
    subparsers.add_parser('setup', help='ç¯å¢ƒè®¾ç½®')

    # docker å‘½ä»¤
    docker_parser = subparsers.add_parser('docker', help='Docker ç®¡ç†')
    docker_parser.add_argument('action', choices=['up', 'down', 'build', 'logs', 'ps', 'restart'], help='Docker æ“ä½œ')

    # backup å‘½ä»¤
    subparsers.add_parser('backup', help='å¤‡ä»½æ•°æ®')

    # clean å‘½ä»¤
    clean_parser = subparsers.add_parser('clean', help='æ¸…ç†ç¯å¢ƒ')
    clean_parser.add_argument('--deep', action='store_true', help='æ·±åº¦æ¸…ç†')

    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        return

    manager = ArxivSyncManager()
    success = False

    try:
        if args.command == 'sync':
            success = manager.sync_all(dry_run=args.dry_run)
        elif args.command == 'schedule':
            success = manager.schedule_sync(days=args.days, dry_run=args.dry_run, backup=not args.no_backup)
        elif args.command == 'health':
            success = manager.health_check()
        elif args.command == 'setup':
            success = manager.setup_environment()
        elif args.command == 'docker':
            success = manager.docker_manage(args.action)
        elif args.command == 'backup':
            success = manager.backup_data()
        elif args.command == 'clean':
            success = manager.clean_environment(deep=args.deep)

    except KeyboardInterrupt:
        print("\nâš ï¸ æ“ä½œè¢«ç”¨æˆ·ä¸­æ–­")
        success = False
    except Exception as e:
        print(f"âŒ æ“ä½œå¤±è´¥: {e}")
        success = False

    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
