#!/usr/bin/env python3
"""
æ”¹è¿›çš„é£ä¹¦åŒæ­¥åŠŸèƒ½
æ”¯æŒï¼š
1. è®ºæ–‡ä¿¡æ¯ä¸­æ·»åŠ åŒ¹é…å…³é”®è¯å’Œç›¸å…³æ€§è¯„åˆ†
2. æ¯ä¸ªä¸»é¢˜/é¢†åŸŸå¯¹åº”ä¸€ä¸ªæ•°æ®è¡¨
3. é¿å…é‡å¤åŒæ­¥ï¼Œæ ¹æ®arxiv idè¿›è¡Œå»é‡
"""

import os
from datetime import datetime
from typing import Dict, List, Any, Optional
from dotenv import load_dotenv
from feishu_bitable_connector import FeishuBitableConnector


def sync_papers_to_feishu(papers, cfg, matched_keywords_map=None, score_map=None):
    """æ”¹è¿›çš„é£ä¹¦åŒæ­¥å‡½æ•°

    Args:
        papers: è®ºæ–‡åˆ—è¡¨
        cfg: é…ç½®å¯¹è±¡
        matched_keywords_map: è®ºæ–‡IDåˆ°åŒ¹é…å…³é”®è¯çš„æ˜ å°„
        score_map: è®ºæ–‡IDåˆ°è¯„åˆ†çš„æ˜ å°„
    """
    load_dotenv()

    # æ£€æŸ¥é£ä¹¦é…ç½®
    feishu_cfg = cfg.get('feishu', {})
    if not feishu_cfg.get('enabled', True):
        print("â„¹ï¸ é£ä¹¦åŒæ­¥å·²ç¦ç”¨")
        return False

    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    required_vars = ['FEISHU_APP_ID', 'FEISHU_APP_SECRET', 'FEISHU_BITABLE_APP_TOKEN']

    missing_vars = []
    for var in required_vars:
        if not os.getenv(var):
            missing_vars.append(var)

    # æ£€æŸ¥è®¿é—®ä»¤ç‰Œ
    has_user_token = bool(os.getenv('FEISHU_USER_ACCESS_TOKEN')) and 'xxxx' not in os.getenv(
        'FEISHU_USER_ACCESS_TOKEN', ''
    )
    has_tenant_token = bool(os.getenv('FEISHU_TENANT_ACCESS_TOKEN')) and 'xxxx' not in os.getenv(
        'FEISHU_TENANT_ACCESS_TOKEN', ''
    )

    if missing_vars or (not has_user_token and not has_tenant_token):
        print("âŒ é£ä¹¦é…ç½®ä¸å®Œæ•´ï¼Œè¯·å…ˆè¿è¡Œ setup_feishu.py é…ç½®")
        return False

    try:
        print("\nğŸ”— å¼€å§‹æ”¹è¿›ç‰ˆé£ä¹¦åŒæ­¥...")
        connector = FeishuBitableConnector()

        # è·å–ç”¨æˆ·é…ç½®ä¿¡æ¯
        user_profile = cfg.get('user_profile', {})
        research_area = user_profile.get('research_area', 'general')
        user_name = user_profile.get('name', 'ç ”ç©¶å‘˜')

        # æ ¹æ®ç ”ç©¶é¢†åŸŸåˆ›å»ºè¡¨æ ¼åç§°
        table_display_name = f"{user_name.replace('ç ”ç©¶å‘˜', '')}è®ºæ–‡è¡¨"

        print(f"ğŸ“Š ä¸ºç ”ç©¶é¢†åŸŸ '{research_area}' å¤„ç†ä¸“ç”¨æ•°æ®è¡¨...")

        # æŸ¥æ‰¾æˆ–åˆ›å»ºæ•°æ®è¡¨
        target_table_id = connector.find_table_by_name(table_display_name)

        if not target_table_id:
            print(f"ğŸ†• åˆ›å»ºæ–°æ•°æ®è¡¨: {table_display_name}")
            table_result = connector.create_domain_papers_table(table_display_name, research_area)
            if table_result:
                target_table_id = table_result.get('table_id')
                print(f"âœ… æ•°æ®è¡¨åˆ›å»ºæˆåŠŸï¼ŒID: {target_table_id}")
            else:
                print("âŒ æ•°æ®è¡¨åˆ›å»ºå¤±è´¥")
                return False
        else:
            print(f"âœ… æ‰¾åˆ°ç°æœ‰æ•°æ®è¡¨: {table_display_name} (ID: {target_table_id})")

        # è·å–ç°æœ‰è®°å½•ï¼Œé¿å…é‡å¤
        print("ğŸ” æ£€æŸ¥ç°æœ‰è®°å½•ï¼Œé¿å…é‡å¤...")
        existing_records = connector.get_all_records(target_table_id)
        existing_arxiv_ids = set()

        for record in existing_records:
            fields = record.get('fields', {})
            arxiv_id = fields.get('ArXiv ID', '')
            if arxiv_id:
                existing_arxiv_ids.add(arxiv_id)

        print(f"ğŸ“‹ å‘ç° {len(existing_arxiv_ids)} æ¡ç°æœ‰è®°å½•")

        # å‡†å¤‡æ–°çš„è®ºæ–‡æ•°æ®
        new_papers_data = []
        new_papers_count = 0

        for i, paper in enumerate(papers):
            # æå–è®ºæ–‡åŸºæœ¬ä¿¡æ¯
            if isinstance(paper, dict):
                arxiv_id = paper.get('arxiv_id', '')
                title = paper.get('title', '')
                authors_str = paper.get('authors_str', '')
                summary = paper.get('summary', '')
                categories_str = paper.get('categories_str', '')
                published_date = paper.get('published_date')
                updated_date = paper.get('updated_date')
                pdf_url = paper.get('pdf_url', '')
                paper_url = paper.get('paper_url', '')
            else:
                arxiv_id = getattr(paper, 'arxiv_id', getattr(paper, 'id', ''))
                title = getattr(paper, 'title', '')
                authors_str = ", ".join(getattr(paper, 'authors', []))
                summary = getattr(paper, 'summary', '')
                categories_str = ", ".join(getattr(paper, 'categories', []))
                published_date = getattr(paper, 'published_date', None)
                updated_date = getattr(paper, 'updated_date', None)
                pdf_url = getattr(paper, 'pdf_url', '')
                paper_url = getattr(paper, 'paper_url', getattr(paper, 'entry_id', ''))

            # è·³è¿‡å·²å­˜åœ¨çš„è®°å½•
            if arxiv_id in existing_arxiv_ids:
                continue

            # è·å–åŒ¹é…å…³é”®è¯å’Œè¯„åˆ†
            matched_keywords = []
            relevance_score = 0.0

            # ä»è®ºæ–‡å¯¹è±¡ä¸­è·å–åŒ¹é…ä¿¡æ¯
            if isinstance(paper, dict):
                # ä»rankingç³»ç»Ÿä¸­è·å–åŒ¹é…ä¿¡æ¯
                if 'matched_interests' in paper:
                    matched_keywords = paper['matched_interests']

                # è·å–è¯„åˆ†
                if 'final_score' in paper:
                    relevance_score = paper['final_score']
                elif 'relevance_score' in paper:
                    relevance_score = paper['relevance_score']
                elif 'score' in paper:
                    relevance_score = paper['score']
            else:
                # å¯¹è±¡æ ¼å¼
                if hasattr(paper, 'matched_interests'):
                    matched_keywords = paper.matched_interests
                elif hasattr(paper, 'matched_keywords'):
                    matched_keywords = paper.matched_keywords

                if hasattr(paper, 'final_score'):
                    relevance_score = paper.final_score
                elif hasattr(paper, 'relevance_score'):
                    relevance_score = paper.relevance_score
                elif hasattr(paper, 'score'):
                    relevance_score = paper.score

            # ä»å¤–éƒ¨æ˜ å°„ä¸­è·å–ä¿¡æ¯ï¼ˆå¦‚æœæä¾›çš„è¯ï¼‰
            if matched_keywords_map and arxiv_id in matched_keywords_map:
                matched_keywords = matched_keywords_map[arxiv_id]

            if score_map and arxiv_id in score_map:
                relevance_score = score_map[arxiv_id]

            # å¤„ç†ä½œè€…ã€åˆ†ç±»å’Œå…³é”®è¯ä¸ºå¤šé€‰é¡¹æ ¼å¼
            authors_list = []
            categories_list = []

            if isinstance(paper, dict):
                # å¤„ç†ä½œè€…
                authors = paper.get('authors', [])
                if isinstance(authors, list):
                    authors_list = [author.strip() for author in authors if author and author.strip()]
                else:
                    authors_list = [author.strip() for author in str(authors).split(',') if author and author.strip()]

                # å¤„ç†åˆ†ç±»
                categories = paper.get('categories', [])
                if isinstance(categories, list):
                    categories_list = [cat.strip() for cat in categories if cat and cat.strip()]
                else:
                    categories_list = [cat.strip() for cat in str(categories).split(',') if cat and cat.strip()]
            else:
                # å¯¹è±¡æ ¼å¼
                authors = getattr(paper, 'authors', [])
                if isinstance(authors, list):
                    authors_list = [author.strip() for author in authors if author and author.strip()]
                else:
                    authors_list = [author.strip() for author in str(authors).split(',') if author and author.strip()]

                categories = getattr(paper, 'categories', [])
                if isinstance(categories, list):
                    categories_list = [cat.strip() for cat in categories if cat and cat.strip()]
                else:
                    categories_list = [cat.strip() for cat in str(categories).split(',') if cat and cat.strip()]

            # å¤„ç†åŒ¹é…å…³é”®è¯ä¸ºå¤šé€‰é¡¹æ ¼å¼
            matched_keywords_list = []
            if matched_keywords:
                if isinstance(matched_keywords, list):
                    matched_keywords_list = [kw.strip() for kw in matched_keywords if kw and kw.strip()]
                else:
                    matched_keywords_list = [kw.strip() for kw in str(matched_keywords).split(',') if kw and kw.strip()]

            # å¤„ç†å¿…é¡»å…³é”®è¯åŒ¹é…ä¸ºå¤šé€‰é¡¹æ ¼å¼
            required_keywords_list = []
            if isinstance(paper, dict) and 'required_keyword_matches' in paper:
                required_matches = paper['required_keyword_matches']
                if required_matches:
                    if isinstance(required_matches, list):
                        required_keywords_list = [kw.strip() for kw in required_matches if kw and kw.strip()]
                    else:
                        required_keywords_list = [
                            kw.strip() for kw in str(required_matches).split(',') if kw and kw.strip()
                        ]
            elif hasattr(paper, 'required_keyword_matches'):
                required_matches = paper.required_keyword_matches
                if required_matches:
                    if isinstance(required_matches, list):
                        required_keywords_list = [kw.strip() for kw in required_matches if kw and kw.strip()]
                    else:
                        required_keywords_list = [
                            kw.strip() for kw in str(required_matches).split(',') if kw and kw.strip()
                        ]

            # å¤„ç†ç ”ç©¶é¢†åŸŸä¸ºå¤šé€‰é¡¹æ ¼å¼
            research_area_list = []
            if research_area:
                if isinstance(research_area, list):
                    research_area_list = [area.strip() for area in research_area if area and area.strip()]
                else:
                    research_area_list = [research_area.strip()] if research_area.strip() else []

            # é™åˆ¶æ•°é‡ä»¥é¿å…å­—æ®µè¿‡é•¿
            authors_list = authors_list[:10]  # æœ€å¤š10ä¸ªä½œè€…
            categories_list = categories_list[:5]  # æœ€å¤š5ä¸ªåˆ†ç±»
            matched_keywords_list = matched_keywords_list[:10]  # æœ€å¤š10ä¸ªå…³é”®è¯
            required_keywords_list = required_keywords_list[:5]  # æœ€å¤š5ä¸ªå¿…é¡»å…³é”®è¯
            research_area_list = research_area_list[:3]  # æœ€å¤š3ä¸ªç ”ç©¶é¢†åŸŸ

            # æ„å»ºè®ºæ–‡æ•°æ®
            paper_data = {
                "ArXiv ID": arxiv_id,
                "æ ‡é¢˜": title,
                "ä½œè€…": authors_list,  # å¤šé€‰é¡¹å­—æ®µ
                "æ‘˜è¦": summary[:1000] if summary else "",  # é™åˆ¶é•¿åº¦
                "åˆ†ç±»": categories_list,  # å¤šé€‰é¡¹å­—æ®µ
                "åŒ¹é…å…³é”®è¯": matched_keywords_list,  # å¤šé€‰é¡¹å­—æ®µ
                "ç›¸å…³æ€§è¯„åˆ†": round(relevance_score, 2),
                "ç ”ç©¶é¢†åŸŸ": research_area_list,  # å¤šé€‰é¡¹å­—æ®µ
                "PDFé“¾æ¥": {"text": "PDF", "link": pdf_url} if pdf_url else None,  # è¶…é“¾æ¥æ ¼å¼
                "è®ºæ–‡é“¾æ¥": {"text": "ArXiv", "link": paper_url} if paper_url else None,  # è¶…é“¾æ¥æ ¼å¼
                "å¿…é¡»å…³é”®è¯åŒ¹é…": required_keywords_list,  # å¤šé€‰é¡¹å­—æ®µ
                "å‘å¸ƒæ—¥æœŸ": int(published_date.timestamp() * 1000) if published_date else None,  # æ—¶é—´æˆ³æ ¼å¼
                "æ›´æ–°æ—¥æœŸ": int(updated_date.timestamp() * 1000) if updated_date else None,  # æ—¶é—´æˆ³æ ¼å¼
            }

            new_papers_data.append(paper_data)
            new_papers_count += 1

        if not new_papers_data:
            print("â„¹ï¸ æ²¡æœ‰æ–°çš„è®ºæ–‡éœ€è¦åŒæ­¥")
            return True

        # è¿‡æ»¤ä½åˆ†è®ºæ–‡
        sync_threshold = feishu_cfg.get('sync_threshold', 0.0)
        papers_to_sync = []

        for paper_data in new_papers_data:
            if paper_data.get('ç›¸å…³æ€§è¯„åˆ†', 0) < sync_threshold:
                continue
            papers_to_sync.append(paper_data)

        if not papers_to_sync:
            print(f"â„¹ï¸ æ²¡æœ‰ç¬¦åˆåŒæ­¥æ¡ä»¶çš„è®ºæ–‡ï¼ˆé˜ˆå€¼: {sync_threshold}ï¼‰")
            return True

        # æ‰¹é‡åŒæ­¥
        batch_size = feishu_cfg.get('batch_size', 20)
        print(f"ğŸ“Š å‡†å¤‡åŒæ­¥ {len(papers_to_sync)} ç¯‡æ–°è®ºæ–‡åˆ° '{table_display_name}'...")

        synced_count = 0
        for i in range(0, len(papers_to_sync), batch_size):
            batch = papers_to_sync[i : i + batch_size]

            try:
                result = connector.batch_insert_records(target_table_id, batch)
                if result and result.get('records'):
                    batch_synced = len(result.get('records', []))
                    synced_count += batch_synced
                    print(f"âœ… ç¬¬ {i//batch_size + 1} æ‰¹åŒæ­¥æˆåŠŸ: {batch_synced} ç¯‡")
                else:
                    print(f"âš ï¸ ç¬¬ {i//batch_size + 1} æ‰¹åŒæ­¥å¯èƒ½å¤±è´¥")
            except Exception as e:
                print(f"âŒ ç¬¬ {i//batch_size + 1} æ‰¹åŒæ­¥å¤±è´¥: {e}")
                continue

        # ç®¡ç†è§†å›¾ï¼ˆå¦‚æœé…ç½®äº†çš„è¯ï¼‰
        view_config = feishu_cfg.get('views', {})
        if view_config.get('enabled', False):
            print("ğŸ¯ ç®¡ç†è¡¨æ ¼è§†å›¾...")
            view_configs = view_config.get('create_views', [])
            auto_cleanup = view_config.get('auto_cleanup', True)

            if view_configs:
                view_result = connector.manage_table_views(target_table_id, view_configs, auto_cleanup)
                print(f"ğŸ“Š è§†å›¾ç®¡ç†ç»“æœ:")
                print(f"   - åˆ›å»º: {view_result['created']} ä¸ª")
                print(f"   - å·²å­˜åœ¨: {view_result['existing']} ä¸ª")
                print(f"   - åˆ é™¤: {view_result['deleted']} ä¸ª")

                if view_result['errors']:
                    print(f"   - é”™è¯¯: {len(view_result['errors'])} ä¸ª")
                    for error in view_result['errors']:
                        print(f"     â€¢ {error}")

        print(f"ğŸ‰ é£ä¹¦åŒæ­¥å®Œæˆï¼")
        print(f"   - è¡¨æ ¼åç§°: {table_display_name}")
        print(f"   - ç ”ç©¶é¢†åŸŸ: {research_area}")
        print(f"   - æ–°å¢è®ºæ–‡: {synced_count} ç¯‡")
        print(f"   - æ€»è®°å½•æ•°: {len(existing_arxiv_ids) + synced_count} ç¯‡")

        return True

    except Exception as e:
        print(f"âŒ é£ä¹¦åŒæ­¥å¤±è´¥: {e}")
        return False
