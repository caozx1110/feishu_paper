#!/usr/bin/env python3
"""
ArXiv API æ ¸å¿ƒæ¨¡å—
ç»Ÿä¸€ç®¡ç†ArXivæ•°æ®äº¤äº’å’Œè®ºæ–‡æ™ºèƒ½æ’åºåŠŸèƒ½
ä½¿ç”¨å®˜æ–¹arxivåº“è¿›è¡Œè¯·æ±‚ï¼Œæ”¯æŒPDFä¸‹è½½
"""

import arxiv
import re
import json
import os
from datetime import datetime, timedelta
from typing import Dict, List, Any, Tuple, Optional
from collections import defaultdict
import math
import requests
from pathlib import Path
import hashlib

# ä¼˜å…ˆä½¿ç”¨ rapidfuzzï¼Œå¦‚æœä¸å¯ç”¨åˆ™å›é€€åˆ° difflib
try:
    from rapidfuzz import fuzz, process

    RAPIDFUZZ_AVAILABLE = True
except ImportError:
    import difflib

    RAPIDFUZZ_AVAILABLE = False
    print("âš ï¸  rapidfuzz ä¸å¯ç”¨ï¼Œä½¿ç”¨ difflib ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ")


class ArxivAPI:
    """ArXiv API äº¤äº’ç±» - ä½¿ç”¨å®˜æ–¹arxivåº“"""

    def __init__(self, timeout: int = 30, download_dir: str = "downloads"):
        """åˆå§‹åŒ–ArXiv APIå®¢æˆ·ç«¯"""
        self.timeout = timeout
        self.download_dir = Path(download_dir)
        self.download_dir.mkdir(exist_ok=True)

        # é…ç½®arxivå®¢æˆ·ç«¯ï¼ˆä½¿ç”¨æ­£ç¡®çš„æ–¹å¼ï¼‰
        self.client = arxiv.Client(page_size=100, delay_seconds=3.0, num_retries=3)

    def search_papers(
        self,
        query: str = None,
        categories: List[str] = None,
        max_results: int = 100,
        sort_by: arxiv.SortCriterion = arxiv.SortCriterion.SubmittedDate,
        sort_order: arxiv.SortOrder = arxiv.SortOrder.Descending,
        date_from: datetime = None,
        date_to: datetime = None,
    ) -> List[Dict[str, Any]]:
        """
        æœç´¢ArXivè®ºæ–‡

        Args:
            query: æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²
            categories: åˆ†ç±»åˆ—è¡¨ï¼Œå¦‚ ['cs.AI', 'cs.RO']
            max_results: æœ€å¤§ç»“æœæ•°
            sort_by: æ’åºå­—æ®µ
            sort_order: æ’åºé¡ºåº
            date_from: å¼€å§‹æ—¥æœŸ
            date_to: ç»“æŸæ—¥æœŸ

        Returns:
            è®ºæ–‡ä¿¡æ¯åˆ—è¡¨
        """
        try:
            # æ„å»ºæœç´¢æŸ¥è¯¢
            search_query = self._build_search_query(query, categories, date_from, date_to)

            print(f"ğŸ” æœç´¢æŸ¥è¯¢: {search_query}")

            # åˆ›å»ºæœç´¢å¯¹è±¡
            search = arxiv.Search(query=search_query, max_results=max_results, sort_by=sort_by, sort_order=sort_order)

            papers = []
            for result in self.client.results(search):
                paper_info = self._parse_arxiv_result(result)
                if paper_info:
                    papers.append(paper_info)

            print(f"âœ… æˆåŠŸè·å– {len(papers)} ç¯‡è®ºæ–‡")
            return papers

        except Exception as e:
            print(f"âŒ æœç´¢è®ºæ–‡æ—¶å‡ºé”™: {e}")
            return []

    def get_recent_papers(
        self, days: int = 7, max_results: int = 300, categories: List[str] = None, field_type: str = "all"
    ) -> List[Dict[str, Any]]:
        """
        è·å–æœ€è¿‘Nå¤©çš„è®ºæ–‡

        Args:
            days: å¤©æ•°èŒƒå›´
            max_results: æœ€å¤§ç»“æœæ•°
            categories: åˆ†ç±»åˆ—è¡¨
            field_type: é¢†åŸŸç±»å‹ (all, ai, robotics, cv, nlp)

        Returns:
            è®ºæ–‡ä¿¡æ¯åˆ—è¡¨
        """
        # æ ¹æ®field_typeè®¾ç½®é»˜è®¤åˆ†ç±»
        if categories is None:
            categories = self._get_field_categories(field_type)

        # è®¡ç®—æ—¥æœŸèŒƒå›´
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days)

        return self.search_papers(
            categories=categories, max_results=max_results, date_from=start_date, date_to=end_date
        )

    def download_pdf(
        self, paper: Dict[str, Any], force_download: bool = False, create_metadata: bool = True
    ) -> Tuple[bool, str]:
        """
        ä¸‹è½½è®ºæ–‡PDF

        Args:
            paper: è®ºæ–‡ä¿¡æ¯å­—å…¸
            force_download: æ˜¯å¦å¼ºåˆ¶é‡æ–°ä¸‹è½½
            create_metadata: æ˜¯å¦åˆ›å»ºå…ƒæ•°æ®æ–‡ä»¶

        Returns:
            tuple: (æ˜¯å¦æˆåŠŸ, æ–‡ä»¶è·¯å¾„æˆ–é”™è¯¯ä¿¡æ¯)
        """
        try:
            arxiv_id = paper.get('arxiv_id', '')
            if not arxiv_id:
                return False, "è®ºæ–‡IDæ— æ•ˆ"

            # ç”Ÿæˆæ–‡ä»¶å
            safe_title = self._sanitize_filename(paper.get('title', ''))[:100]
            pdf_filename = f"{arxiv_id}_{safe_title}.pdf"
            pdf_path = self.download_dir / pdf_filename

            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            if pdf_path.exists() and not force_download:
                return True, str(pdf_path)

            print(f"ğŸ“¥ ä¸‹è½½è®ºæ–‡: {paper.get('title', '')[:50]}...")

            # è·å–arxivç»“æœå¯¹è±¡
            search = arxiv.Search(id_list=[arxiv_id])
            result = next(self.client.results(search), None)

            if not result:
                return False, "æ— æ³•æ‰¾åˆ°è®ºæ–‡"

            # ä¸‹è½½PDF
            result.download_pdf(dirpath=str(self.download_dir), filename=pdf_filename)

            # åˆ›å»ºå…ƒæ•°æ®æ–‡ä»¶
            if create_metadata:
                self._create_paper_metadata(paper, pdf_path.with_suffix('.md'))

            print(f"âœ… ä¸‹è½½å®Œæˆ: {pdf_filename}")
            return True, str(pdf_path)

        except Exception as e:
            error_msg = f"ä¸‹è½½å¤±è´¥: {e}"
            print(f"âŒ {error_msg}")
            return False, error_msg

    def batch_download_pdfs(
        self, papers: List[Dict[str, Any]], max_downloads: int = 10, create_index: bool = True
    ) -> Dict[str, Any]:
        """
        æ‰¹é‡ä¸‹è½½PDF

        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            max_downloads: æœ€å¤§ä¸‹è½½æ•°é‡
            create_index: æ˜¯å¦åˆ›å»ºç´¢å¼•æ–‡ä»¶

        Returns:
            ä¸‹è½½ç»Ÿè®¡ä¿¡æ¯
        """
        stats = {'total': len(papers), 'downloaded': 0, 'skipped': 0, 'failed': 0, 'failed_papers': []}

        download_count = 0
        downloaded_papers = []

        for paper in papers:
            if download_count >= max_downloads:
                break

            success, result = self.download_pdf(paper)
            if success:
                stats['downloaded'] += 1
                downloaded_papers.append({**paper, 'pdf_path': result})
            else:
                if "å·²å­˜åœ¨" in result:
                    stats['skipped'] += 1
                else:
                    stats['failed'] += 1
                    stats['failed_papers'].append({'title': paper.get('title', ''), 'error': result})

            download_count += 1

        # åˆ›å»ºä¸‹è½½ç´¢å¼•
        if create_index and downloaded_papers:
            self._create_download_index(downloaded_papers)

        return stats

    def _build_search_query(
        self, query: str = None, categories: List[str] = None, date_from: datetime = None, date_to: datetime = None
    ) -> str:
        """æ„å»ºæœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²"""
        query_parts = []

        # æ·»åŠ è‡ªå®šä¹‰æŸ¥è¯¢
        if query:
            query_parts.append(f"({query})")

        # æ·»åŠ åˆ†ç±»æŸ¥è¯¢
        if categories:
            category_query = " OR ".join([f"cat:{cat}" for cat in categories])
            query_parts.append(f"({category_query})")

        # æ·»åŠ æ—¥æœŸèŒƒå›´æŸ¥è¯¢
        if date_from or date_to:
            date_parts = []

            if date_from:
                # å°†datetimeè½¬æ¢ä¸ºYYYYMMDDæ ¼å¼
                date_from_str = date_from.strftime("%Y%m%d")
                date_parts.append(f"submittedDate:[{date_from_str}0000")

            if date_to:
                # å°†datetimeè½¬æ¢ä¸ºYYYYMMDDæ ¼å¼
                date_to_str = date_to.strftime("%Y%m%d")
                if not date_from:
                    date_parts.append(f"submittedDate:[19910801 TO {date_to_str}2359]")
                else:
                    # é—­åˆä¹‹å‰çš„èŒƒå›´
                    date_parts[0] = f"submittedDate:[{date_from_str}0000 TO {date_to_str}2359]"
            elif date_from:
                # åªæœ‰å¼€å§‹æ—¥æœŸï¼Œåˆ°å½“å‰æ—¥æœŸ
                current_date = datetime.now().strftime("%Y%m%d")
                date_parts[0] = f"submittedDate:[{date_from_str}0000 TO {current_date}2359]"

            if date_parts:
                query_parts.extend(date_parts)

        # åˆå¹¶æŸ¥è¯¢éƒ¨åˆ†
        search_query = " AND ".join(query_parts) if query_parts else "all:*"

        return search_query

    def _parse_arxiv_result(self, result: arxiv.Result) -> Optional[Dict[str, Any]]:
        """è§£æarxiv.Resultå¯¹è±¡ä¸ºè®ºæ–‡ä¿¡æ¯å­—å…¸"""
        try:
            return {
                'title': result.title.strip(),
                'authors': [author.name for author in result.authors],
                'authors_str': ', '.join([author.name for author in result.authors]),
                'summary': result.summary.strip(),
                'published_date': result.published,
                'updated_date': result.updated if result.updated else result.published,
                'paper_url': result.entry_id,
                'pdf_url': result.pdf_url,
                'categories': [cat for cat in result.categories],
                'categories_str': ', '.join(result.categories),
                'arxiv_id': result.entry_id.split('/')[-1],
                'primary_category': result.primary_category,
                'comment': result.comment if result.comment else "",
                'journal_ref': result.journal_ref if result.journal_ref else "",
                'doi': result.doi if result.doi else "",
            }
        except Exception as e:
            print(f"è§£æè®ºæ–‡ä¿¡æ¯æ—¶å‡ºé”™: {e}")
            return None

    def _get_field_categories(self, field_type) -> List[str]:
        """
        æ ¹æ®é¢†åŸŸç±»å‹è·å–åˆ†ç±»åˆ—è¡¨ï¼Œæ”¯æŒäº¤é›†/å¹¶é›†æ“ä½œ

        æ”¯æŒçš„æ ¼å¼ï¼š
        - å•ä¸ªå­—ç¬¦ä¸²: "ai", "robotics"
        - å¹¶é›†æ“ä½œ: "ai+robotics" æˆ– ["ai", "robotics"]
        - äº¤é›†æ“ä½œ: "ai&robotics" (ç›®å‰ArXiv APIä¸ç›´æ¥æ”¯æŒï¼Œè¿”å›å¹¶é›†)
        - ç›´æ¥åˆ†ç±»: "cs.AI" æˆ– ["cs.AI", "cs.LG"]
        """
        # åŸºç¡€é¢†åŸŸæ˜ å°„
        field_mappings = {
            'ai': ['cs.AI', 'cs.LG', 'stat.ML'],
            'robotics': ['cs.RO'],
            'cv': ['cs.CV', 'eess.IV'],
            'nlp': ['cs.CL'],
            'physics': ['physics.comp-ph', 'cond-mat', 'quant-ph'],
            'math': ['math.OC', 'math.ST', 'math.NA'],
            'stat': ['stat.ML', 'stat.ME', 'stat.AP'],
            'eess': ['eess.IV', 'eess.SP', 'eess.AS'],
            'q-bio': ['q-bio.QM', 'q-bio.GN', 'q-bio.MN'],
            'all': ['cs.AI', 'cs.LG', 'cs.RO', 'cs.CV', 'cs.CL', 'cs.CR', 'cs.DC', 'cs.DS', 'cs.HC', 'cs.IR'],
        }

        # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œå¤„ç†åˆ—è¡¨ä¸­çš„æ¯ä¸ªå…ƒç´ 
        if isinstance(field_type, list):
            all_categories = []
            for field in field_type:
                all_categories.extend(self._parse_single_field(field, field_mappings))
            return list(set(all_categories))  # å»é‡

        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œæ£€æŸ¥æ˜¯å¦åŒ…å«è¿ç®—ç¬¦
        if isinstance(field_type, str):
            return self._parse_field_string(field_type, field_mappings)

        # é»˜è®¤è¿”å›all
        return field_mappings['all']

    def _parse_field_string(self, field_str: str, field_mappings: dict) -> List[str]:
        """è§£æå­—æ®µå­—ç¬¦ä¸²ï¼Œæ”¯æŒè¿ç®—ç¬¦"""
        field_str = field_str.strip()

        # æ£€æŸ¥å¹¶é›†è¿ç®—ç¬¦ (+, |, or)
        if '+' in field_str or '|' in field_str or ' or ' in field_str.lower():
            # åˆ†å‰²å¹¶å¤„ç†å„ä¸ªéƒ¨åˆ†
            separators = ['+', '|', ' or ', ' OR ']
            parts = [field_str]

            for sep in separators:
                new_parts = []
                for part in parts:
                    new_parts.extend(part.split(sep))
                parts = new_parts

            all_categories = []
            for part in parts:
                part = part.strip()
                if part:
                    all_categories.extend(self._parse_single_field(part, field_mappings))

            return list(set(all_categories))  # å»é‡

        # æ£€æŸ¥äº¤é›†è¿ç®—ç¬¦ (&, and) - æ³¨æ„ï¼šArXiv APIä¸ç›´æ¥æ”¯æŒäº¤é›†
        elif '&' in field_str or ' and ' in field_str.lower():
            print("âš ï¸  æ³¨æ„ï¼šArXiv APIä¸ç›´æ¥æ”¯æŒåˆ†ç±»äº¤é›†æŸ¥è¯¢ï¼Œå°†è½¬æ¢ä¸ºå¹¶é›†æŸ¥è¯¢")
            # å°†äº¤é›†è½¬æ¢ä¸ºå¹¶é›†å¤„ç†
            separators = ['&', ' and ', ' AND ']
            parts = [field_str]

            for sep in separators:
                new_parts = []
                for part in parts:
                    new_parts.extend(part.split(sep))
                parts = new_parts

            all_categories = []
            for part in parts:
                part = part.strip()
                if part:
                    all_categories.extend(self._parse_single_field(part, field_mappings))

            return list(set(all_categories))

        # å•ä¸ªå­—æ®µ
        else:
            return self._parse_single_field(field_str, field_mappings)

    def _parse_single_field(self, field: str, field_mappings: dict) -> List[str]:
        """è§£æå•ä¸ªå­—æ®µ"""
        field = field.strip()

        # ç›´æ¥æ˜¯ArXivåˆ†ç±»
        if field.startswith(('cs.', 'stat.', 'math.', 'physics.', 'eess.', 'q-bio.', 'quant-ph', 'cond-mat')):
            return [field]

        # æŸ¥æ‰¾é¢„å®šä¹‰æ˜ å°„
        if field.lower() in field_mappings:
            return field_mappings[field.lower()]

        # å¦‚æœä¸åœ¨æ˜ å°„ä¸­ï¼Œå‡è®¾æ˜¯è‡ªå®šä¹‰åˆ†ç±»
        print(f"âš ï¸  æœªè¯†åˆ«çš„é¢†åŸŸç±»å‹: {field}ï¼Œå°†å°è¯•ä½œä¸ºArXivåˆ†ç±»ä½¿ç”¨")
        return [field]

    def _sanitize_filename(self, filename: str) -> str:
        """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤ä¸åˆæ³•å­—ç¬¦"""
        # ç§»é™¤æˆ–æ›¿æ¢ä¸åˆæ³•å­—ç¬¦
        invalid_chars = r'[<>:"/\\|?*]'
        sanitized = re.sub(invalid_chars, '_', filename)
        # ç§»é™¤å¤šä½™ç©ºæ ¼å’Œæ ‡ç‚¹
        sanitized = re.sub(r'\s+', '_', sanitized)
        sanitized = re.sub(r'[^\w\-_.]', '', sanitized)
        return sanitized.strip('_')

    def _create_paper_metadata(self, paper: Dict[str, Any], md_path: Path) -> None:
        """åˆ›å»ºè®ºæ–‡å…ƒæ•°æ®Markdownæ–‡ä»¶"""
        try:
            content = f"""# {paper.get('title', 'Unknown Title')}

## åŸºæœ¬ä¿¡æ¯
- **ArXiv ID**: {paper.get('arxiv_id', 'N/A')}
- **å‘å¸ƒæ—¥æœŸ**: {paper.get('published_date', 'N/A')}
- **ä¸»åˆ†ç±»**: {paper.get('primary_category', 'N/A')}
- **æ‰€æœ‰åˆ†ç±»**: {paper.get('categories_str', 'N/A')}

## ä½œè€…
{paper.get('authors_str', 'Unknown Authors')}

## æ‘˜è¦
{paper.get('summary', 'No summary available')}

## é“¾æ¥
- **è®ºæ–‡é¡µé¢**: {paper.get('paper_url', 'N/A')}
- **PDFä¸‹è½½**: {paper.get('pdf_url', 'N/A')}

## å…¶ä»–ä¿¡æ¯
- **æœŸåˆŠå¼•ç”¨**: {paper.get('journal_ref', 'N/A')}
- **DOI**: {paper.get('doi', 'N/A')}
- **å¤‡æ³¨**: {paper.get('comment', 'N/A')}

---
*ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""

            with open(md_path, 'w', encoding='utf-8') as f:
                f.write(content)

        except Exception as e:
            print(f"åˆ›å»ºå…ƒæ•°æ®æ–‡ä»¶å¤±è´¥: {e}")

    def _create_download_index(self, papers: List[Dict[str, Any]]) -> None:
        """åˆ›å»ºä¸‹è½½ç´¢å¼•æ–‡ä»¶"""
        try:
            index_path = self.download_dir / "README.md"

            content = f"""# ArXiv è®ºæ–‡ä¸‹è½½ç´¢å¼•

> ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
> æ€»è®¡è®ºæ–‡æ•°: {len(papers)}

## è®ºæ–‡åˆ—è¡¨

"""

            for i, paper in enumerate(papers, 1):
                arxiv_id = paper.get('arxiv_id', 'N/A')
                title = paper.get('title', 'Unknown Title')
                authors = paper.get('authors_str', 'Unknown Authors')
                categories = paper.get('categories_str', 'N/A')
                published = paper.get('published_date', 'N/A')

                content += f"""### {i}. {title}

- **ArXiv ID**: {arxiv_id}
- **ä½œè€…**: {authors}
- **åˆ†ç±»**: {categories}
- **å‘å¸ƒ**: {published}
- **é“¾æ¥**: [{arxiv_id}]({paper.get('paper_url', '#')}) | [PDF]({paper.get('pdf_url', '#')})

---

"""

            with open(index_path, 'w', encoding='utf-8') as f:
                f.write(content)

            print(f"ğŸ“ åˆ›å»ºä¸‹è½½ç´¢å¼•: {index_path}")

        except Exception as e:
            print(f"åˆ›å»ºç´¢å¼•æ–‡ä»¶å¤±è´¥: {e}")


class PaperRanker:
    """è®ºæ–‡æ™ºèƒ½æ’åºå’Œè¿‡æ»¤ç±»"""

    def __init__(self):
        """åˆå§‹åŒ–æ’åºå™¨"""
        # å…³é”®è¯æƒé‡åˆ†å±‚
        self.keyword_weights = {
            'core': 2.5,  # æ ¸å¿ƒå…³é”®è¯æƒé‡ï¼ˆğŸ¯æ ‡è®°çš„å…³é”®è¯ï¼‰
            'extended': 1.5,  # æ‰©å±•å…³é”®è¯æƒé‡ï¼ˆğŸ”§æ ‡è®°çš„å…³é”®è¯ï¼‰
            'related': 1.0,  # ç›¸å…³å…³é”®è¯æƒé‡
            'default': 1.0,  # é»˜è®¤æƒé‡
        }

        # åŒ¹é…ç¼“å­˜
        self._match_cache = {}
        self._max_cache_size = 1000

        # åŒä¹‰è¯è¯å…¸ - å¯ä»¥æ‰©å±•
        self.synonyms = {
            'robot': ['robotics', 'robotic', 'autonomous agent', 'android', 'humanoid'],
            'ai': ['artificial intelligence', 'machine intelligence', 'intelligent system'],
            'ml': ['machine learning', 'statistical learning', 'automated learning'],
            'dl': ['deep learning', 'neural network', 'neural net', 'deep neural network'],
            'cv': ['computer vision', 'visual perception', 'image analysis', 'visual recognition'],
            'nlp': ['natural language processing', 'language processing', 'text processing'],
            'llm': ['large language model', 'language model', 'generative model'],
            'vla': ['vision language action', 'vision-language-action', 'multimodal action'],
            'slam': ['simultaneous localization and mapping', 'localization and mapping'],
            'rl': ['reinforcement learning', 'reward learning', 'policy learning'],
            'transformer': ['attention mechanism', 'self-attention', 'multi-head attention'],
        }

        # å¸¸è§ç¼©å†™æ‰©å±•
        self.abbreviations = {
            'ai': 'artificial intelligence',
            'ml': 'machine learning',
            'dl': 'deep learning',
            'cv': 'computer vision',
            'nlp': 'natural language processing',
            'llm': 'large language model',
            'vla': 'vision language action',
            'slam': 'simultaneous localization and mapping',
            'rl': 'reinforcement learning',
            'gnn': 'graph neural network',
            'cnn': 'convolutional neural network',
            'rnn': 'recurrent neural network',
            'lstm': 'long short term memory',
            'bert': 'bidirectional encoder representations from transformers',
            'gpt': 'generative pre-trained transformer',
        }

        # æŠ€æœ¯é¢†åŸŸå…³é”®è¯æƒé‡
        self.domain_weights = {
            'cs.AI': 1.5,
            'cs.LG': 1.4,
            'cs.RO': 1.3,
            'cs.CV': 1.2,
            'cs.CL': 1.2,
        }

    def calculate_relevance_score(
        self,
        paper: Dict[str, Any],
        interest_keywords: List[str] = None,
        exclude_keywords: List[str] = None,
        raw_interest_keywords: List[str] = None,
    ) -> Tuple[float, bool, List[str], List[str]]:
        """
        è®¡ç®—è®ºæ–‡ä¸å…³æ³¨è¯æ¡çš„ç›¸å…³æ€§è¯„åˆ† (å¢å¼ºç‰ˆ)

        æ”¯æŒé€šé…ç¬¦åŒ¹é…ï¼š
        - "*" æˆ– ["*"] åŒ¹é…æ‰€æœ‰æ–‡ç« 
        - ä»¥ "regex:" å¼€å¤´çš„å…³é”®è¯ä¼šè¢«å½“ä½œæ­£åˆ™è¡¨è¾¾å¼å¤„ç†
        - æ”¯æŒåˆ†å±‚æƒé‡ç³»ç»Ÿï¼šğŸ¯æ ¸å¿ƒæ¦‚å¿µã€ğŸ”§æ‰©å±•æ¦‚å¿µ

        Args:
            paper: è®ºæ–‡ä¿¡æ¯å­—å…¸
            interest_keywords: å…³æ³¨è¯æ¡åˆ—è¡¨ï¼ˆå·²è¿‡æ»¤çš„ï¼‰ï¼Œè¶Šå‰é¢æƒé‡è¶Šé«˜
            exclude_keywords: æ’é™¤è¯æ¡åˆ—è¡¨
            raw_interest_keywords: åŸå§‹å…³é”®è¯åˆ—è¡¨ï¼ˆåŒ…å«æ³¨é‡Šè¡Œï¼Œç”¨äºæƒé‡åˆ†æï¼‰

        Returns:
            tuple: (relevance_score, is_excluded, matched_interests, matched_excludes)
        """
        if not interest_keywords:
            return 0.0, False, [], []

        # è§£æåˆ†å±‚æƒé‡ï¼ˆå¦‚æœæä¾›äº†åŸå§‹å…³é”®è¯åˆ—è¡¨ï¼‰
        keyword_categories = {}
        if raw_interest_keywords:
            keyword_categories = self._parse_keyword_weights(raw_interest_keywords)

        # æ£€æŸ¥é€šé…ç¬¦åŒ¹é…ï¼ˆåŒ¹é…æ‰€æœ‰æ–‡ç« ï¼‰
        if self._is_wildcard_match(interest_keywords):
            return 1.0, False, ["*"], []

        # æå–è®ºæ–‡æ–‡æœ¬ä¿¡æ¯
        title = paper.get('title', '').lower()
        summary = paper.get('summary', '').lower()
        categories = paper.get('categories', [])
        categories_str = ' '.join(categories).lower()
        authors = paper.get('authors_str', '').lower()
        paper_date = paper.get('published_date', datetime.now())

        # ç»„åˆæ‰€æœ‰æ–‡æœ¬ç”¨äºæœç´¢
        full_text = f"{title} {summary} {categories_str} {authors}"

        # æ‰©å±•å…³é”®è¯
        expanded_interests = self._expand_keywords(interest_keywords)
        expanded_excludes = self._expand_keywords(exclude_keywords) if exclude_keywords else []

        # æ£€æŸ¥æ’é™¤è¯æ¡ (ä½¿ç”¨æ‰©å±•åçš„è¯æ¡)
        excluded = False
        matched_excludes = []
        if expanded_excludes:
            for exclude_term in expanded_excludes:
                if exclude_term.lower() in full_text:
                    excluded = True
                    matched_excludes.append(exclude_term)

                # æ¨¡ç³ŠåŒ¹é…æ£€æŸ¥
                fuzzy_score = self._fuzzy_match_score(exclude_term, full_text, threshold=0.9)
                if fuzzy_score > 0:
                    excluded = True
                    matched_excludes.append(f"{exclude_term}(æ¨¡ç³ŠåŒ¹é…)")

        # å¦‚æœè¢«æ’é™¤ï¼Œç›´æ¥è¿”å›
        if excluded:
            return -999.0, True, [], matched_excludes

        # è®¡ç®—å…³æ³¨è¯æ¡åŒ¹é…å¾—åˆ† (å¢å¼ºç‰ˆ)
        relevance_score = 0.0
        matched_interests = []

        # æ—¶é—´è¡°å‡æƒé‡
        time_weight = self._calculate_time_decay(paper_date)

        # é¢†åŸŸç›¸å…³æ€§æƒé‡
        domain_weight = self._calculate_domain_relevance(categories)

        # å…±ç°æ£€æµ‹
        cooccurrence_bonus = self._detect_cooccurrence(expanded_interests, full_text)

        for i, keyword in enumerate(interest_keywords):
            keyword_lower = keyword.lower()

            # åŸºç¡€æƒé‡ï¼šè¶Šé å‰çš„è¯æ¡æƒé‡è¶Šé«˜
            base_weight = len(interest_keywords) - i

            # å¢å¼ºçš„å…³é”®è¯åŒ¹é…ï¼ˆæ”¯æŒæ­£åˆ™è¡¨è¾¾å¼ï¼‰
            enhanced_matched, enhanced_score = self._enhance_keyword_matching(keyword, full_text)

            if enhanced_matched:
                matched_interests.append(keyword)

                # è·å–åˆ†å±‚æƒé‡
                tier_weight = self._get_keyword_weight(keyword, keyword_categories)

                final_score = (
                    enhanced_score * base_weight * tier_weight * time_weight * domain_weight * cooccurrence_bonus
                )
                relevance_score += final_score
                continue  # å¦‚æœå¢å¼ºåŒ¹é…æˆåŠŸï¼Œè·³è¿‡åç»­å¤„ç†

            # æ£€æŸ¥åŸå…³é”®è¯å’Œæ‰©å±•å…³é”®è¯
            all_variants = [keyword_lower] + [syn.lower() for syn in self.synonyms.get(keyword_lower, [])]

            keyword_score = 0.0
            keyword_matched = False

            for variant in all_variants:
                # ç²¾ç¡®åŒ¹é…
                position_weights = self._calculate_position_weight(variant, title, summary)
                exact_score = sum(position_weights.values())

                if exact_score > 0:
                    keyword_matched = True
                    keyword_score += exact_score

                # æ¨¡ç³ŠåŒ¹é…
                fuzzy_title_score = self._fuzzy_match_score(variant, title, threshold=0.8)
                fuzzy_summary_score = self._fuzzy_match_score(variant, summary, threshold=0.8)

                if fuzzy_title_score > 0:
                    keyword_matched = True
                    keyword_score += fuzzy_title_score * 2.0  # æ ‡é¢˜æ¨¡ç³ŠåŒ¹é…æƒé‡

                if fuzzy_summary_score > 0:
                    keyword_matched = True
                    keyword_score += fuzzy_summary_score * 1.0  # æ‘˜è¦æ¨¡ç³ŠåŒ¹é…æƒé‡

                # åˆ†ç±»åŒ¹é…
                category_matches = len(re.findall(r'\b' + re.escape(variant) + r'\b', categories_str))
                if category_matches > 0:
                    keyword_matched = True
                    keyword_score += category_matches * 1.5

            if keyword_matched:
                matched_interests.append(keyword)

                # è·å–åˆ†å±‚æƒé‡
                tier_weight = self._get_keyword_weight(keyword, keyword_categories)

                # ç»¼åˆè®¡ç®—æœ€ç»ˆåˆ†æ•°ï¼ˆæ·»åŠ åˆ†å±‚æƒé‡ï¼‰
                final_score = (
                    keyword_score * base_weight * tier_weight * time_weight * domain_weight * cooccurrence_bonus
                )
                relevance_score += final_score

        return relevance_score, excluded, matched_interests, matched_excludes

    def filter_and_rank_papers(
        self,
        papers: List[Dict[str, Any]],
        interest_keywords: List[str] = None,
        exclude_keywords: List[str] = None,
        min_score: float = 0.1,
        use_advanced_scoring: bool = False,
        score_weights: Dict[str, float] = None,
        raw_interest_keywords: List[str] = None,
    ) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]], Dict[str, Any]]:
        """
        æ ¹æ®å…³æ³¨è¯æ¡è¿‡æ»¤å’Œæ’åºè®ºæ–‡ (æ”¯æŒé«˜çº§è¯„åˆ†)

        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            interest_keywords: å…³æ³¨è¯æ¡åˆ—è¡¨ (æŒ‰é‡è¦æ€§æ’åº)
            exclude_keywords: æ’é™¤è¯æ¡åˆ—è¡¨
            min_score: æœ€å°ç›¸å…³æ€§åˆ†æ•°é˜ˆå€¼
            use_advanced_scoring: æ˜¯å¦ä½¿ç”¨é«˜çº§æ™ºèƒ½è¯„åˆ†
            score_weights: è¯„åˆ†æƒé‡é…ç½®

        Returns:
            tuple: (ranked_papers, excluded_papers, score_stats)
        """
        if not papers:
            return [], [], {}

        # é»˜è®¤è¯„åˆ†æƒé‡
        if score_weights is None:
            score_weights = {'base': 1.0, 'semantic': 0.3, 'author': 0.2, 'novelty': 0.4, 'citation': 0.3}

        if not interest_keywords:
            # å¦‚æœæ²¡æœ‰å…³æ³¨è¯æ¡ï¼Œåªè¿›è¡Œæ’é™¤è¿‡æ»¤
            if exclude_keywords:
                filtered_papers = []
                excluded_papers = []
                for paper in papers:
                    _, is_excluded, _, _ = self.calculate_relevance_score(
                        paper, [], exclude_keywords, raw_interest_keywords
                    )
                    if is_excluded:
                        excluded_papers.append(paper)
                    else:
                        filtered_papers.append(paper)
                return filtered_papers, excluded_papers, {}
            else:
                return papers, [], {}

        # è®¡ç®—æ¯ç¯‡è®ºæ–‡çš„ç›¸å…³æ€§åˆ†æ•°
        scored_papers = []
        excluded_papers = []

        for paper in papers:
            if use_advanced_scoring:
                # ä½¿ç”¨é«˜çº§è¯„åˆ†
                total_score, is_excluded, matched_interests, matched_excludes, score_breakdown = (
                    self.calculate_advanced_relevance_score(
                        paper, interest_keywords, exclude_keywords, True, True, raw_interest_keywords
                    )
                )

                # åº”ç”¨æƒé‡
                final_score = (
                    score_breakdown.get('base_score', 0) * score_weights['base']
                    + score_breakdown.get('semantic_boost', 0) * score_weights['semantic']
                    + score_breakdown.get('author_boost', 0) * score_weights['author']
                    + score_breakdown.get('novelty_boost', 0) * score_weights['novelty']
                    + score_breakdown.get('citation_potential', 0) * score_weights['citation']
                )

                paper['score_breakdown'] = score_breakdown
                paper['final_score'] = final_score

            else:
                # ä½¿ç”¨åŸºç¡€è¯„åˆ†
                final_score, is_excluded, matched_interests, matched_excludes = self.calculate_relevance_score(
                    paper, interest_keywords, exclude_keywords, raw_interest_keywords
                )

            if is_excluded:
                paper['exclude_reason'] = matched_excludes
                excluded_papers.append(paper)
            else:
                paper['relevance_score'] = final_score
                paper['matched_interests'] = matched_interests
                paper['interest_match_count'] = len(matched_interests)

                if final_score >= min_score:
                    scored_papers.append(paper)

        # æŒ‰ç›¸å…³æ€§åˆ†æ•°é™åºæ’åº
        sort_key = 'final_score' if use_advanced_scoring else 'relevance_score'
        ranked_papers = sorted(scored_papers, key=lambda x: x[sort_key], reverse=True)

        # ç»Ÿè®¡ä¿¡æ¯
        scores = [p[sort_key] for p in ranked_papers]
        score_stats = {
            'total_papers': len(papers),
            'ranked_papers': len(ranked_papers),
            'excluded_papers': len(excluded_papers),
            'max_score': max(scores) if scores else 0,
            'min_score': min(scores) if scores else 0,
            'avg_score': sum(scores) / len(scores) if scores else 0,
            'use_advanced_scoring': use_advanced_scoring,
        }

        return ranked_papers, excluded_papers, score_stats

    def get_field_papers(self, papers: List[Dict[str, Any]], field_type: str) -> List[Dict[str, Any]]:
        """æ ¹æ®é¢†åŸŸç±»å‹è¿‡æ»¤è®ºæ–‡"""
        field_keywords = {
            'ai': {
                'keywords': [
                    'artificial intelligence',
                    'AI',
                    'machine intelligence',
                    'deep learning',
                    'neural network',
                ],
                'categories': ['cs.AI', 'cs.LG', 'stat.ML'],
            },
            'robotics': {
                'keywords': [
                    'robot',
                    'robotics',
                    'robotic',
                    'autonomous',
                    'navigation',
                    'manipulation',
                    'SLAM',
                    'motion planning',
                    'path planning',
                    'humanoid',
                    'quadruped',
                    'mobile robot',
                ],
                'categories': ['cs.RO'],
            },
            'cv': {
                'keywords': [
                    'computer vision',
                    'image processing',
                    'visual',
                    'object detection',
                    'image recognition',
                    'video analysis',
                ],
                'categories': ['cs.CV', 'eess.IV'],
            },
            'nlp': {
                'keywords': [
                    'natural language',
                    'NLP',
                    'language model',
                    'text processing',
                    'machine translation',
                    'sentiment analysis',
                ],
                'categories': ['cs.CL'],
            },
        }

        if field_type not in field_keywords:
            return papers

        field_config = field_keywords[field_type]
        filtered_papers = []

        for paper in papers:
            # æ£€æŸ¥åˆ†ç±»åŒ¹é…
            if any(cat in paper.get('categories', []) for cat in field_config['categories']):
                filtered_papers.append(paper)
                continue

            # æ£€æŸ¥å…³é”®è¯åŒ¹é…
            title_lower = paper['title'].lower()
            summary_lower = paper['summary'].lower()

            for keyword in field_config['keywords']:
                if keyword.lower() in title_lower or keyword.lower() in summary_lower:
                    filtered_papers.append(paper)
                    break

        return filtered_papers

    def _expand_keywords(self, keywords: List[str]) -> List[str]:
        """æ‰©å±•å…³é”®è¯åˆ—è¡¨ï¼ŒåŒ…å«åŒä¹‰è¯å’Œç¼©å†™"""
        expanded = set(keywords)

        for keyword in keywords:
            keyword_lower = keyword.lower()

            # æ·»åŠ ç¼©å†™æ‰©å±•
            if keyword_lower in self.abbreviations:
                expanded.add(self.abbreviations[keyword_lower])

            # æ·»åŠ åŒä¹‰è¯
            if keyword_lower in self.synonyms:
                expanded.update(self.synonyms[keyword_lower])

            # åå‘æŸ¥æ‰¾ - å¦‚æœè¾“å…¥çš„æ˜¯å…¨ç§°ï¼Œä¹Ÿè¦åŒ…å«ç¼©å†™
            for abbr, full_term in self.abbreviations.items():
                if keyword_lower == full_term:
                    expanded.add(abbr)

        return list(expanded)

    def _fuzzy_match_score(self, keyword: str, text: str, threshold: float = 0.8) -> float:
        """è®¡ç®—æ¨¡ç³ŠåŒ¹é…åˆ†æ•° - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œä½¿ç”¨ rapidfuzz"""
        if RAPIDFUZZ_AVAILABLE:
            return self._rapidfuzz_match_score(keyword, text, threshold)
        else:
            return self._fallback_fuzzy_match(keyword, text, threshold)

    def _rapidfuzz_match_score(self, keyword: str, text: str, threshold: float = 0.8) -> float:
        """ä½¿ç”¨ rapidfuzz è¿›è¡Œå¿«é€Ÿæ¨¡ç³ŠåŒ¹é…"""
        keyword_lower = keyword.lower()

        # å¿«é€Ÿæ£€æŸ¥ç²¾ç¡®åŒ¹é…
        if keyword_lower in text.lower():
            return 1.0

        # åˆ†è¯å¹¶é™åˆ¶æ£€æŸ¥èŒƒå›´ä»¥æé«˜æ•ˆç‡
        words = re.findall(r'\b\w+\b', text.lower())
        if not words:
            return 0.0

        # é™åˆ¶æ£€æŸ¥çš„è¯æ•°ä»¥æé«˜æ•ˆç‡ï¼ˆrapidfuzz å¾ˆå¿«ï¼Œå¯ä»¥æ£€æŸ¥æ›´å¤šè¯ï¼‰
        check_words = words[:100] if len(words) > 100 else words

        # ä½¿ç”¨ rapidfuzz è¿›è¡Œå¿«é€Ÿæ¨¡ç³ŠåŒ¹é…
        best_match = process.extractOne(keyword_lower, check_words, scorer=fuzz.ratio, score_cutoff=threshold * 100)

        return best_match[1] / 100.0 if best_match else 0.0

    def _fallback_fuzzy_match(self, keyword: str, text: str, threshold: float = 0.8) -> float:
        """å¤‡ç”¨æ¨¡ç³ŠåŒ¹é…æ–¹æ³•ï¼ˆrapidfuzz ä¸å¯ç”¨æ—¶ä½¿ç”¨ difflibï¼‰"""
        if not RAPIDFUZZ_AVAILABLE:
            import difflib

        keyword_lower = keyword.lower()

        # å¿«é€Ÿæ£€æŸ¥ç²¾ç¡®åŒ¹é…
        if keyword_lower in text.lower():
            return 1.0

        words = re.findall(r'\b\w+\b', text.lower())
        if not words:
            return 0.0

        max_similarity = 0.0
        # é™åˆ¶æ£€æŸ¥çš„è¯æ•°ä»¥æé«˜æ•ˆç‡ï¼ˆdifflib è¾ƒæ…¢ï¼Œå‡å°‘æ£€æŸ¥è¯æ•°ï¼‰
        check_words = words[:30] if len(words) > 30 else words

        for word in check_words:
            # è·³è¿‡è¿‡çŸ­çš„è¯
            if len(word) < 3:
                continue

            if RAPIDFUZZ_AVAILABLE:
                # å¦‚æœ rapidfuzz å¯ç”¨ï¼Œä½¿ç”¨å®ƒ
                similarity = fuzz.ratio(keyword_lower, word) / 100.0
            else:
                # å¦åˆ™ä½¿ç”¨ difflib
                similarity = difflib.SequenceMatcher(None, keyword_lower, word).ratio()

            if similarity >= threshold:
                max_similarity = max(max_similarity, similarity)
                # å¦‚æœæ‰¾åˆ°éå¸¸å¥½çš„åŒ¹é…ï¼Œæå‰ç»ˆæ­¢
                if similarity > 0.95:
                    break

        return max_similarity

    def _calculate_time_decay(self, paper_date: datetime, decay_days: int = 30) -> float:
        """è®¡ç®—æ—¶é—´è¡°å‡æƒé‡ - è¾ƒæ–°çš„è®ºæ–‡æƒé‡æ›´é«˜"""
        # å¤„ç†æ—¶åŒºé—®é¢˜
        if paper_date.tzinfo is not None:
            paper_date = paper_date.replace(tzinfo=None)

        days_ago = (datetime.now() - paper_date).days
        if days_ago <= 0:
            return 1.0
        elif days_ago <= decay_days:
            # çº¿æ€§è¡°å‡
            return 1.0 - (days_ago / decay_days) * 0.3  # æœ€å¤šè¡°å‡30%
        else:
            return 0.7  # æœ€å°æƒé‡

    def _calculate_domain_relevance(self, categories: List[str]) -> float:
        """æ ¹æ®è®ºæ–‡åˆ†ç±»è®¡ç®—é¢†åŸŸç›¸å…³æ€§æƒé‡"""
        max_weight = 1.0
        for category in categories:
            if category in self.domain_weights:
                max_weight = max(max_weight, self.domain_weights[category])
        return max_weight

    def _detect_cooccurrence(self, keywords: List[str], text: str) -> float:
        """æ£€æµ‹å…³é”®è¯å…±ç°ï¼Œæå‡ç›¸å…³æ€§"""
        text_lower = text.lower()
        found_keywords = []

        for keyword in keywords:
            if keyword.lower() in text_lower:
                found_keywords.append(keyword)

        # å…±ç°å¥–åŠ± - åŒæ—¶å‡ºç°å¤šä¸ªå…³é”®è¯æ—¶ç»™äºˆé¢å¤–åˆ†æ•°
        cooccurrence_count = len(found_keywords)
        if cooccurrence_count >= 2:
            # å…±ç°å¥–åŠ±ç³»æ•°
            return 1.0 + (cooccurrence_count - 1) * 0.2
        return 1.0

    def _calculate_position_weight(self, keyword: str, title: str, summary: str) -> Dict[str, float]:
        """è®¡ç®—å…³é”®è¯åœ¨ä¸åŒä½ç½®çš„æƒé‡"""
        weights = {'title': 0.0, 'summary_start': 0.0, 'summary_mid': 0.0}

        keyword_lower = keyword.lower()
        title_lower = title.lower()
        summary_lower = summary.lower()

        # æ ‡é¢˜ä¸­çš„æƒé‡
        if keyword_lower in title_lower:
            title_words = title_lower.split()
            keyword_position = -1
            for i, word in enumerate(title_words):
                if keyword_lower in word:
                    keyword_position = i
                    break

            if keyword_position != -1:
                # æ ‡é¢˜å¼€å¤´çš„è¯æƒé‡æ›´é«˜
                position_factor = max(0.5, 1.0 - (keyword_position / len(title_words)) * 0.5)
                weights['title'] = 3.0 * position_factor

        # æ‘˜è¦ä¸­çš„æƒé‡ - åŒºåˆ†å‰åŠéƒ¨åˆ†å’ŒååŠéƒ¨åˆ†
        if keyword_lower in summary_lower:
            summary_length = len(summary_lower)
            keyword_pos = summary_lower.find(keyword_lower)

            if keyword_pos < summary_length * 0.3:  # å‰30%
                weights['summary_start'] = 2.5
            else:  # å…¶ä»–ä½ç½®
                weights['summary_mid'] = 1.5

        return weights

    def calculate_advanced_relevance_score(
        self,
        paper: Dict[str, Any],
        interest_keywords: List[str] = None,
        exclude_keywords: List[str] = None,
        use_semantic_boost: bool = True,
        use_author_analysis: bool = True,
        raw_interest_keywords: List[str] = None,
    ) -> Tuple[float, bool, List[str], List[str], Dict[str, Any]]:
        """
        é«˜çº§ç›¸å…³æ€§è¯„åˆ†è®¡ç®— (åŒ…å«æ›´å¤šæ™ºèƒ½ç‰¹æ€§)

        Args:
            paper: è®ºæ–‡ä¿¡æ¯å­—å…¸
            interest_keywords: å…³æ³¨è¯æ¡åˆ—è¡¨
            exclude_keywords: æ’é™¤è¯æ¡åˆ—è¡¨
            use_semantic_boost: æ˜¯å¦ä½¿ç”¨è¯­ä¹‰å¢å¼º
            use_author_analysis: æ˜¯å¦åˆ†æä½œè€…ä¿¡æ¯

        Returns:
            tuple: (relevance_score, is_excluded, matched_interests, matched_excludes, score_breakdown)
        """
        # åŸºç¡€è¯„åˆ†
        base_score, excluded, matched_interests, matched_excludes = self.calculate_relevance_score(
            paper, interest_keywords, exclude_keywords, raw_interest_keywords
        )

        if excluded:
            return base_score, excluded, matched_interests, matched_excludes, {}

        score_breakdown = {
            'base_score': base_score,
            'semantic_boost': 0.0,
            'author_boost': 0.0,
            'novelty_boost': 0.0,
            'citation_potential': 0.0,
        }

        total_score = base_score

        # è¯­ä¹‰å¢å¼ºåˆ†æ
        if use_semantic_boost and interest_keywords:
            semantic_boost = self._calculate_semantic_boost(paper, interest_keywords)
            score_breakdown['semantic_boost'] = semantic_boost
            total_score += semantic_boost

        # ä½œè€…åˆ†æå¢å¼º
        if use_author_analysis:
            author_boost = self._calculate_author_relevance(paper, interest_keywords)
            score_breakdown['author_boost'] = author_boost
            total_score += author_boost

        # æ–°é¢–æ€§åˆ†æ
        novelty_boost = self._calculate_novelty_score(paper)
        score_breakdown['novelty_boost'] = novelty_boost
        total_score += novelty_boost

        # å¼•ç”¨æ½œåŠ›é¢„æµ‹
        citation_potential = self._predict_citation_potential(paper)
        score_breakdown['citation_potential'] = citation_potential
        total_score += citation_potential

        return total_score, excluded, matched_interests, matched_excludes, score_breakdown

    def _calculate_semantic_boost(self, paper: Dict[str, Any], keywords: List[str]) -> float:
        """è®¡ç®—è¯­ä¹‰ç›¸å…³æ€§å¢å¼ºåˆ†æ•°"""
        title = paper.get('title', '').lower()
        summary = paper.get('summary', '').lower()

        # æŠ€æœ¯æœ¯è¯­å…±ç°åˆ†æ
        tech_terms = [
            'neural',
            'learning',
            'model',
            'algorithm',
            'method',
            'approach',
            'framework',
            'system',
            'network',
            'optimization',
            'training',
            'inference',
            'prediction',
            'classification',
            'regression',
        ]

        tech_term_count = sum(1 for term in tech_terms if term in title + ' ' + summary)

        # åŸºäºæŠ€æœ¯å¯†åº¦çš„è¯­ä¹‰å¢å¼º
        semantic_boost = min(tech_term_count * 0.1, 1.0)

        # å…³é”®è¯è¯­å¢ƒåˆ†æ
        context_boost = 0.0
        for keyword in keywords:
            keyword_lower = keyword.lower()

            # å¯»æ‰¾å…³é”®è¯é™„è¿‘çš„ç›¸å…³æœ¯è¯­
            for text in [title, summary]:
                sentences = re.split(r'[.!?]', text)
                for sentence in sentences:
                    if keyword_lower in sentence:
                        # åˆ†æå¥å­ä¸­çš„å…¶ä»–æŠ€æœ¯æœ¯è¯­
                        sentence_tech_terms = sum(1 for term in tech_terms if term in sentence)
                        context_boost += sentence_tech_terms * 0.05

        return semantic_boost + min(context_boost, 0.5)

    def _calculate_author_relevance(self, paper: Dict[str, Any], keywords: List[str]) -> float:
        """åŸºäºä½œè€…ä¿¡æ¯è®¡ç®—ç›¸å…³æ€§å¢å¼º"""
        authors = paper.get('authors', [])
        if not authors:
            return 0.0

        # ä½œè€…æ•°é‡å¯¹ç ”ç©¶è´¨é‡çš„å½±å“ (é€‚ä¸­çš„ä½œè€…æ•°é‡é€šå¸¸è¾ƒå¥½)
        author_count = len(authors)
        if 2 <= author_count <= 6:
            author_count_boost = 0.2
        elif author_count == 1:
            author_count_boost = 0.1  # å•ä½œè€…å¯èƒ½æ˜¯ç†è®ºæ€§å¼ºçš„å·¥ä½œ
        else:
            author_count_boost = 0.0  # ä½œè€…è¿‡å¤šå¯èƒ½è´¨é‡å‚å·®ä¸é½

        # åˆ†æä½œè€…å§“åä¸­çš„æœºæ„ä¿¡æ¯ (é€šè¿‡é‚®ç®±åŸŸåç­‰)
        institution_boost = 0.0
        # è¿™é‡Œå¯ä»¥æ‰©å±•ä¸ºä»ä½œè€…é™„åŠ ä¿¡æ¯ä¸­æå–æœºæ„ç›¸å…³æ€§

        return author_count_boost + institution_boost

    def _calculate_novelty_score(self, paper: Dict[str, Any]) -> float:
        """è®¡ç®—è®ºæ–‡æ–°é¢–æ€§åˆ†æ•°"""
        title = paper.get('title', '').lower()
        summary = paper.get('summary', '').lower()

        # æ–°é¢–æ€§æŒ‡ç¤ºè¯
        novelty_indicators = [
            'novel',
            'new',
            'first',
            'introduce',
            'propose',
            'present',
            'innovative',
            'breakthrough',
            'pioneer',
            'original',
            'unprecedented',
            'state-of-the-art',
            'sota',
            'outperform',
            'improve',
            'enhance',
            'advance',
            'superior',
            'better than',
        ]

        novelty_count = 0
        for indicator in novelty_indicators:
            novelty_count += len(re.findall(r'\b' + re.escape(indicator) + r'\b', title + ' ' + summary))

        # æ ‡é¢˜ä¸­çš„æ–°é¢–æ€§è¯æ±‡æƒé‡æ›´é«˜
        title_novelty = sum(1 for indicator in novelty_indicators if indicator in title)

        novelty_score = min((novelty_count * 0.1) + (title_novelty * 0.2), 1.0)
        return novelty_score

    def _predict_citation_potential(self, paper: Dict[str, Any]) -> float:
        """é¢„æµ‹è®ºæ–‡çš„å¼•ç”¨æ½œåŠ›"""
        title = paper.get('title', '').lower()
        summary = paper.get('summary', '').lower()
        categories = paper.get('categories', [])

        # é«˜å¼•ç”¨æ½œåŠ›æŒ‡æ ‡
        high_impact_terms = [
            'benchmark',
            'dataset',
            'survey',
            'review',
            'framework',
            'open source',
            'code available',
            'reproducible',
            'evaluation',
            'comparison',
            'analysis',
            'comprehensive',
            'extensive',
        ]

        impact_count = sum(1 for term in high_impact_terms if term in title + ' ' + summary)

        # çƒ­é—¨é¢†åŸŸåŠ æƒ
        hot_categories = ['cs.AI', 'cs.LG', 'cs.CV', 'cs.CL', 'cs.RO']
        category_boost = 0.2 if any(cat in hot_categories for cat in categories) else 0.0

        # è®ºæ–‡é•¿åº¦é¢„æµ‹ (æ›´é•¿çš„æ‘˜è¦é€šå¸¸è¡¨ç¤ºæ›´å…¨é¢çš„å·¥ä½œ)
        length_boost = min(len(summary) / 1000, 0.3)  # æ‘˜è¦é•¿åº¦å½’ä¸€åŒ–

        citation_potential = min((impact_count * 0.15) + category_boost + length_boost, 1.0)
        return citation_potential

    def _is_wildcard_match(self, keywords: List[str]) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦ä¸ºé€šé…ç¬¦åŒ¹é…ï¼ˆåŒ¹é…æ‰€æœ‰æ–‡ç« ï¼‰
        """
        if not keywords:
            return False

        # æ£€æŸ¥æ˜¯å¦åŒ…å«é€šé…ç¬¦
        wildcard_patterns = ["*", "all", ".*", "å…¨éƒ¨", "æ‰€æœ‰"]
        for keyword in keywords:
            if keyword.lower().strip() in wildcard_patterns:
                return True

        # å¦‚æœåªæœ‰ä¸€ä¸ªå…³é”®è¯ä¸”ä¸ºç©ºæˆ–åªæœ‰ç©ºç™½å­—ç¬¦
        if len(keywords) == 1 and not keywords[0].strip():
            return True

        return False

    def _is_regex_keyword(self, keyword: str) -> bool:
        """
        æ£€æŸ¥å…³é”®è¯æ˜¯å¦ä¸ºæ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
        """
        return keyword.startswith("regex:") or keyword.startswith("re:")

    def _process_regex_keyword(self, keyword: str, text: str) -> bool:
        """
        å¤„ç†æ­£åˆ™è¡¨è¾¾å¼å…³é”®è¯åŒ¹é…
        """
        try:
            if keyword.startswith("regex:"):
                pattern = keyword[6:].strip()  # ç§»é™¤ "regex:" å‰ç¼€
            elif keyword.startswith("re:"):
                pattern = keyword[3:].strip()  # ç§»é™¤ "re:" å‰ç¼€
            else:
                return False

            # ç¼–è¯‘å¹¶åŒ¹é…æ­£åˆ™è¡¨è¾¾å¼
            regex = re.compile(pattern, re.IGNORECASE)
            return bool(regex.search(text))
        except re.error:
            # å¦‚æœæ­£åˆ™è¡¨è¾¾å¼æ— æ•ˆï¼Œå›é€€åˆ°æ™®é€šå­—ç¬¦ä¸²åŒ¹é…
            return keyword.lower() in text.lower()

    def _enhance_keyword_matching(self, keyword: str, text: str) -> Tuple[bool, float]:
        """
        å¢å¼ºçš„å…³é”®è¯åŒ¹é…ï¼Œæ”¯æŒæ­£åˆ™è¡¨è¾¾å¼å’Œé€šé…ç¬¦

        Returns:
            tuple: (æ˜¯å¦åŒ¹é…, åŒ¹é…åˆ†æ•°)
        """
        # æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…
        if self._is_regex_keyword(keyword):
            matched = self._process_regex_keyword(keyword, text)
            return matched, 1.0 if matched else 0.0

        # æ™®é€šå­—ç¬¦ä¸²åŒ¹é…
        if keyword.lower() in text.lower():
            return True, 1.0

        # æ¨¡ç³ŠåŒ¹é…
        fuzzy_score = self._fuzzy_match_score(keyword, text, threshold=0.8)
        if fuzzy_score > 0:
            return True, fuzzy_score

        return False, 0.0

    def _parse_keyword_weights(self, raw_keywords: List[str]) -> Dict[str, str]:
        """
        è§£æåŸå§‹å…³é”®è¯åˆ—è¡¨ï¼Œç¡®å®šæ¯ä¸ªå…³é”®è¯çš„æƒé‡ç±»åˆ«

        Args:
            raw_keywords: åŒ…å«æ³¨é‡Šè¡Œçš„åŸå§‹å…³é”®è¯åˆ—è¡¨

        Returns:
            å­—å…¸ï¼Œé”®ä¸ºå…³é”®è¯ï¼Œå€¼ä¸ºæƒé‡ç±»åˆ« ('core', 'extended', 'default')
        """
        keyword_categories = {}
        current_category = 'default'

        for keyword in raw_keywords:
            keyword = keyword.strip()

            # è·³è¿‡ç©ºè¡Œ
            if not keyword:
                continue

            # æ£€æŸ¥æ³¨é‡Šè¡Œï¼Œç¡®å®šå½“å‰åˆ†ç±»
            if keyword.startswith('#'):
                if 'ğŸ¯' in keyword or 'æ ¸å¿ƒæ¦‚å¿µ' in keyword or 'é«˜æƒé‡' in keyword:
                    current_category = 'core'
                elif 'ğŸ”§' in keyword or 'æ‰©å±•æ¦‚å¿µ' in keyword or 'ä¸­æƒé‡' in keyword:
                    current_category = 'extended'
                elif 'ğŸ“' in keyword or 'ç›¸å…³æ¦‚å¿µ' in keyword or 'æ ‡å‡†æƒé‡' in keyword:
                    current_category = 'related'
                # æ³¨é‡Šè¡Œæœ¬èº«ä¸ä½œä¸ºå…³é”®è¯
                continue

            # ä¸ºéæ³¨é‡Šçš„å…³é”®è¯åˆ†é…ç±»åˆ«
            keyword_categories[keyword] = current_category

        return keyword_categories

    def _get_keyword_weight(self, keyword: str, keyword_categories: Dict[str, str]) -> float:
        """
        è·å–å…³é”®è¯çš„æƒé‡å€æ•°

        Args:
            keyword: å…³é”®è¯
            keyword_categories: å…³é”®è¯åˆ†ç±»å­—å…¸

        Returns:
            æƒé‡å€æ•°
        """
        category = keyword_categories.get(keyword, 'default')
        return self.keyword_weights.get(category, self.keyword_weights['default'])
